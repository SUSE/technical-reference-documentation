
ifdef::iK3s[]

The figure below illustrates the high-level architecture of {pn_K3s}:

image::{an_K3s}_architecture.png[title="Architecture Overview - {pn_K3s}", scaledwidth=80%]

Container Runtime::
* Containerd & runc
* Kine as a datastore shim that allows `etcd` to be replaced with other databases

Networking::
* Flannel for CNI
* Kube-router for network policy

Services::
* CoreDNS
* Metrics Server
* Traefik for ingress
* Klipper-lb as an embedded service load balancer provider
* Local-path-provisioner for provisioning volumes using local storage

Workloads::
Helm-controller to allow for CRD-driven deployment of helm manifests

Host utilities::
iptables/nftables, ebtables, ethtool, and socat

Once set up, users can interact with {pn_K3s} via

* kubectl
** directly on the {pn_K3s} host or
** remotely, leveraging the KUBECONFIG file of the {pn_K3s} cluster's deployment (`/etc/rancher/k3s/k3s.yaml`)
* manual or automatic, manifest or Helm Chart based, workload deployments

// ifdef::RC,RI[]
// FixMe - For the best performance and security, the recommended deployment is a dedicated Kubernetes cluster for the {pn_Rancher} management server. Running user workloads on this cluster is not advised. After deploying {pn_Rancher}, one can then create or import clusters for orchestrated workloads.
// endif::RC,RI[]

ifdef::GS[]

To aid in planning, training or assessing functionality like in a <<g-poc>> deployment, {pn_K3s} can be installed on a single node running a Linux operating system as described later in this document.

ifdef::BP[]
TIP: To improve <<g-availability>>, the {pn_K3s) solution can leverage <<g-scaling>> to nodes with different roles (server, agent) and with a shared datastore (embedded `etcd` or external `etd`, MariaDB, MySQL, PostgreSQL) from the single node to a <<g-production>> installation on a multi-node, high-availability Kubernetes cluster.
endif::BP[]
endif::GS[]

endif::iK3s[]
