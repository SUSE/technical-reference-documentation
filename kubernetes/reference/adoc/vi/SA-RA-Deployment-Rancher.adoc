
ifdef::focusRancher[]

=== {pn_Rancher}

// leverage multiple prep sections
ifndef::layerSLEMicro,layerSLES[include::./SA-RA-Deployment-OS-prep.adoc[]]
Preparation(s)::
+
. For the respective node's firewall service, either
* enable and configure the necessary inbound link:{pn_Rancher_PortURL}[ports] or
* stop and completely disable the firewall service.
ifdef::layerK3s,layerRKE1,layerRKE2[]
+
. Determine the desired link:{pn_Rancher_TLSURL}[SSL configuration] for TLS termination
* Rancher-generated TLS certificate
NOTE: This is the easiest way of installing {pn_Rancher} with self-signed certificates.
* Letâ€™s Encrypt
* Bring your own certificate
endif::layerK3s,layerRKE1,layerRKE2[]
ifndef::layerK3s[]
+
. Obtain a link:{kubectl_DownloadURL}[kubectl] binary matching the respective Kubernetes version for this {pn_Rancher} implementation.
endif::layerK3s[]
+
. Obtain a link:{helm_DownloadURL}[Helm] binary matching the respective Kubernetes version for this {pn_Rancher} implementation.
+
NOTE: Enable the respective kubeconfig setting for kubectl
ifdef::layerK3s[, {pn_K3s} - /etc/rancher/k3s/k3s.yml,]
ifdef::layerRKE2[, {pn_RKE2} - /etc/rancher/rke2/rke2.yaml,]
to be leveraged by helm command.

Deployment Process::
While logged into the node, as root or with sudo privileges, install {pn_Rancher}:
+
ifdef::GS[]
. Run the following installation command
+
----
sudo docker run \
	--detach \
	--restart=unless-stopped \
	--publish 80:80 --publish 443:443 \
	--privileged rancher/rancher
----
+
ifdef::BP[]
TIP: This process deploys an auto-generated, self-signed security certificate for the {pn_Rancher} service. Other <<g-security>> certificate authority options are described in the "Install/Upgrade {portfolioName} on a Kubernetes Cluster" in the {pn_Rancher} link:{pn_Rancher_DocURL}[product documentation].
endif::BP[]
+
. When the previous command completes, from a client system connect via a Web browser to the {pn_Rancher} node via *_IPAddress_* or *_HostName_* and accept the service certificate (if deemed valid).
** Enter a new admin password
+
IMPORTANT: On the second configuration page, also ensure the "Server URL" is set to the *_IPAddress_* or *_HostName_* of this deployed {pn_Rancher} node.
endif::GS[]
ifdef::RC,RI[]
. Install cert-manager
* Set the following variable with the desired version of cert-manager
+
----
CERT_MANAGER_VERSION=""
----
+
NOTE: At this time, the most current, supported version of cert-manager is {pn_Rancher_certVer}
+
* Create the cert-manager CRDs and apply the Helm Chart resource manifest
+
----
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/${CERT_MANAGER_VERSION}/cert-manager.crds.yaml

# Add the Jetstack Helm repository
helm repo add jetstack https://charts.jetstack.io

# Update your local Helm chart repository cache
helm repo update

# Install the cert-manager Helm chart
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version ${CERT_MANAGER_VERSION}
----
+
** Check the progress of the installation, looking for all pods to be in running status:
+
----
kubectl get pods --namespace cert-manager
----
+
. Add the {pn_Rancher} helm chart repository:
+
----
helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
----
+
. Create a namespace for {pn_Rancher}
+
----
kubectl create namespace cattle-system
----
+
. Prepare to use the Helm Chart for {pn_Rancher}:
* Set the following variable to the host name of the {pn_Rancher} server instance
+
----
HOSTNAME=""
----
+
NOTE: This host name should be resolvable to an IP address of the
ifdef::layerK3s[{pn_K3s}]
ifdef::layerRKE1[{pn_RKE1}]
ifdef::layerRKE2[{pn_RKE2}]
host, or a load balancer/proxy server that supports this installation of {pn_Rancher}.
+
* Set the following variable to the number of deployed
ifdef::layerK3s[{pn_K3s}]
ifdef::layerRKE1[{pn_RKE1}]
ifdef::layerRKE2[{pn_RKE2}]
nodes planned to host the {pn_Rancher} service
+
----
REPLICAS=""
----
+
* Set the following variable to the desired version of {pn_Rancher} server instance
+
----
RANCHER_VERSION=""
----
+
* Install the {pn_Rancher} Helm Chart
+
----
helm install rancher rancher-stable/rancher \
  --namespace cattle-system \
  --set hostname=${HOSTNAME} \
  --set replicas=${REPLICAS} \
  --version=${RANCHER_VERSION}
----
+
** Monitor the progress of the installation:
+
----
kubectl -n cattle-system rollout status deploy/rancher
----

ifdef::layerRKE1[]

NOTE: Installing Rancher requires a client system (i.e. admin workstation) that has been configured with Helm version 3 and kubectl, as well as access to the kubeconfig file for the RKE cluster. 

.Perform the following steps to install {pn_Rancher}:
* Export the variable KUBECONFIG to the absolute pathname of the kube_config_cluster.yml file. I.e. `export KUBECONFIG=~/rke-cluster/kube_config_cluster.yml`

NOTE: The easiest way of installing {pn_Rancher} is with self-signed certificates. See https://rancher.com/docs/rancher/v2.x/en/installation/install-rancher-on-k8s/#6-install-rancher-with-helm-and-your-chosen-certificate-option for other options.

. Create the Helm Chart custom resource for cert-manager:

* Install the cert-manager custom resource definitions:
----
kubectl create namespace cert-manager

kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.4/cert-manager.crds.yaml
----

.Install cert-manager:

* Set the following variable with the desired version of cert-manager
+
----
CERT_MANAGER_VERSION=""
----
+
NOTE: At the time of writing, the most current, supported version of cert-manager is v1.0.4
+
// ** for example, `CERT_MANAGER_VERSION="v1.0.4"`
* Add the jetstack repo and install the cert-manager Helm chart
----
helm repo add jetstack https://charts.jetstack.io

helm repo update

helm install \
  cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --version ${CERT_MANAGER_VERSION}
----

.Install {pn_Rancher}  

* Set the following variable to the FQDN of the {pn_Rancher} server instance, or load balancer that is serving {pn_Rancher}

----
RANCHER_HOSTNAME=""
----

* Add the {pn_Rancher} Helm repository and install the {pn_Rancher} Helm chart: 
----
kubectl create namespace cattle-system

helm repo add rancher-stable https://releases.rancher.com/server-charts/stable

helm repo update

helm install \
  rancher rancher-stable/rancher \
  --namespace cattle-system \
  --set hostname=${RANCHER_HOSTNAME}
----

endif::layerRKE1[]

. (Optional) Create an SSH tunnel to access {pn_Rancher}: 
+
NOTE: This optional step is useful in cases where NAT routers and/or firewalls prevent the client Web browser from reaching the exposed {pn_Rancher} server IP address and/or port. This step requires that a Linux host is accessible through SSH from the client system and that the Linux host can reach the exposed {pn_Rancher} service. The {pn_Rancher} host name should be resolvable to the appropriate IP address by the local workstation.
+
* Create an SSH tunnel through the Linux host to the IP address of the {pn_Rancher} server on the NodePort, as noted in Step 3:
+
----
ssh -N -D 8080 user@Linux-host
----
+
* On the local workstation Web browser, change the SOCKS Host settings to "127.0.0.1" and port "8080"
+
NOTE: This will route all traffic from this Web browser through the remote Linux host. Be sure to close the tunnel and revert the SOCKS Host settings when you're done.
+
. Connect to the {pn_Rancher} Web UI
* On a client system, use a Web browser to connect to the {pn_Rancher} service, via https
* Provide a new Admin password
+
IMPORTANT: On the second configuration page, ensure the "Rancher Server URL" is set to the host name specified when installing the {pn_Rancher} Helm Chart and the port is 443.

//-
Deployment Consideration(s)::
To further optimize deployment factors, leverage the following practices
ifdef::FCTR+Availability[]
* <<g-availability>>
** In instances where a load balancer is used to access a {pn_K3s} cluster, deploying two additional {pn_K3s} cluster nodes, for a total of three, will automatically make {pn_Rancher} highly available.
endif::FCTR+Availability[]
ifdef::FCTR+Security[]
* <<g-security>>
** The basic deployment steps described above are for deploying {pn_Rancher} with automatically generated, self-signed security certificates. Other options are to have {pn_Rancher} create public certificates via Let's Encrypt associated with with a publicly resolvable host name for the {pn_Rancher} server, or to provide preconfigured, private certificates.
endif::FCTR+Security[]
ifdef::FCTR+Integrity[]
* <<g-integrity>>
** This deployment of {pn_Rancher} uses the {pn_K3s} etcd key/value store to persist its data and configuration, which offers several advantages. With a multi-node cluster and this resiliency through replication, having to provide highly-available storage isn't needed. In addition, backing up the {pn_K3s} etcd store protects the cluster as well as the installation of {pn_Rancher} and permits restoration of a given state.
endif::FCTR+Integrity[]

endif::RC,RI[]

// Next Steps::
After this successful deployment of the {pn_Rancher} solution, review the link:{pn_Rancher_DocURL}[product documentation] for details on how downstream Kubernetes clusters can be:

* deployed (refer to sub-section "Setting up Kubernetes Clusters in {portfolioName}") or
* imported (refer to sub-section "Importing Existing Clusters"), then 
* managed (refer to sub-section "Cluster Administration") and
* accessed (refer to sub-section "Cluster Access") to address orchestration of workload, maintaining security and many more functions are readily available.

endif::focusRancher[]

