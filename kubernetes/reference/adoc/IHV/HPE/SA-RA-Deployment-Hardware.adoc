
The base, starting configuration can reside all within a single {vn_HPE} {familyHPE-S} Frame. Based upon the relatively small resource requirements for a
ifdef::focusRancher[{pn_Rancher}]
ifdef::focusK3s[{pn_K3s}]
ifdef::focusRKE1[{pn_RKE1}]
ifdef::focusRKE2[{pn_RKE2}]
deployment, a viable approach is to deploy as a virtual machine (VM) on the target nodes, on top of an existing hypervisor, like KVM. For a physical host, there are tools that can be used during the setup of the server, see below.

//-
Preparation(s)::
ifdef::IHV-HPE-Apollo,IHV-HPE-Edgeline,IHV-HPE-ProLiant,IHV-HPE-Synergy[]
The {an_HPE} link:{vn_HPE_BMCURL}[{vn_HPE_BMC}] [{an_HPE_BMC}] is designed for secure local and remote server management and helps IT administrators deploy, update and monitor {an_HPE} servers anywhere, anytime.
. Upgrade your basic {an_HPE_BMC} license for additional functionality, such as graphical remote console and virtual media access to allow the remote usage of software image files (ISO files), which can be used for installing operating systems or updating servers.
** (Optional) - link:{vn_HPE_BMCFederationURL}[{an_HPE_BMC} Federation] enables you to manage multiple servers from one system using the {an_HPE_BMC} web interface.
endif::IHV-HPE-Apollo,IHV-HPE-Edgeline,IHV-HPE-ProLiant,IHV-HPE-Synergy[]
ifdef::IHV-HPE-Synergy[]
.  For nodes situated in an {an_HPE} {familyHPE-S} enclosure, like {an_HPE} {familyHPE-S} {modelHPE-SY480} used in the deployment:
** Setup the necessary items in the link:{vn_HPE_ComposerTechURL}[{vn_HPE} {vn_HPE_ComposerTech}] interface, including:
*** Settings -> Addresses and Identifiers (Subnets and Address Ranges)
*** Networks -> Create (associate subnets and designate bandwidths)
*** Network Sets -> Create (aggregate all the necessary Networks)
*** Logical Interconnects -> Edit (include the respective Network Sets)
*** Logical Interconnect Groups -> Edit (include the respective Network Sets)
*** Server Profile Templates -> Create (or use existing hypervisor templates)
*** OS Deployment mode -> could be configured to boot from PXE, local storage, shared storage
*** Firmware (upgrade to the latest and strive for consistency across node types)
*** Manage Connections (assign the Network Set to be bonded across NICs)
*** Local Storage (create the internal RAID1 set and request additional drives for the respective roles)
*** Manage Boot/BIOS/{an_HPE_BMC} Settings
*** Server Profile -> Create (assign the role template to the target model)
** Add Servers and Assign Server Roles
*** Use the Discover function from {vn_HPE} {vn_HPE_ComposerTech} to see all of the available nodes that can be assigned to to their respective roles:
*** Then drag and drop the nodes into the roles and ensure there is no missing configuration information, by reviewing and editing each node's server details
*** Manage Settings - setup DNS/NTP, designate Disk Models/NIC Mappings/Interface Model/Networks
*** Manage Subnet and Netmask - edit Management Network information, ensuring a match exists to those setup in {vn_HPE} {vn_HPE_ComposerTech}
endif::IHV-HPE-Synergy[]

//-
Deployment Process::
On the respective compute module node, determine if a hypervisor is already available for the solution's virtual machines.

. If this will be the first use of this node, an option is to deploy a KVM hypervisor, based upon {pn_SLES} by following the link:{pn_SLES_VirtDocURL}[Virtualization Guide].
** Given the simplicity of the deployment, the operating system and hypervisor can be installed with the {pn_SLES} ISO media and the {vn_HPE} {vn_HPE_BMC} virtual media and virtual console methodology.
. Then for the solution VM, use the hypervisor user interface to allocate the necessary CPU, memory, disk and networking as noted in the link:{pn_Rancher} {pn_Rancher_HWReqURL}[hardware requirements].

//-
Deployment Consideration(s)::
To further optimize deployment factors, leverage the following practices:

ifdef::IHV-HPE-Synergy[]
ifdef::FCTR+Automation[]
* <<g-automation>>
** For {an_HPE} {familyHPE-S} servers, you can simplify multiple compute module setups and configurations, leveraging the {vn_HPE} {vn_HPE_ComposerTech} link:{vn_HPE_OVTerraformURL}[SDK for Terraform Provider].
** For nodes running KVM, you can leverage either link:{pn_SLES_VirtDocURL}[virt-install] or link:{pn_SLES_LibvirtTerraformURL}[Terraform Libvirt Provider] to quickly and efficiently automate the deployment of multiple virtual machines.
endif::FCTR+Automation[]
endif::IHV-HPE-Synergy[]
ifdef::FCTR+Availability[]
* <<g-availability>>
** While the initial deployment only requires a single VM, as noted in later deployment sections, having multiple VMs provides resiliency to accomplish high availability. To reduce single points of failure, it would be beneficial to have the multi-VM deployments spread across multiple hypervisor nodes. So consideration of consistent hypervisor and compute module configurations, with the needed resources for the VMs will yield a robust, reliable production implementation.
endif::FCTR+Availability[]
