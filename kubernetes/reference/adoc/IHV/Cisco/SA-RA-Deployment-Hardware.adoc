
ifdef::IHV-Cisco-C240-SD[]

The base, starting configuration can reside all within a single {an_Cisco} {an_CiscoUCS} server. Based upon the relatively small resource requirements for a
ifdef::focusRancher[{pn_Rancher}] 
ifdef::focusK3s[{pn_K3s}]
ifdef::focusRKE1[{pn_RKE1}]
ifdef::focusRKE2[{pn_RKE2}]
deployment, a viable approach is to deploy as a virtual machine (VM) on the target nodes, on top of an existing hypervisor, like KVM.

//-
Preparation(s)::
For a physical host, that is racked, cabled and powered up, like link:{modelCiscoUCS-C240SDURL}[{an_Cisco} {an_CiscoUCS} {modelCiscoUCS-C240SD}] used in the deployment:
+
. If using {an_Cisco} {an_CiscoUCS} {pn_Cisco_BMC} ({an_Cisco_BMC}):
** Provide a DHCP Server for an IP address to the {an_Cisco} {an_CiscoUCS} {pn_Cisco_BMC} or use a monitor, keyboard, and mouse for initial {an_Cisco_BMC} configuration
. Log into the interface as admin
** On left menu click on `Storage -> Cisco 12G Modular Raid Controller`
*** Create virtual drive from unused physical drives, for example pick two drives for the operating system and click on `>>` button.  Under virtual drive properties enter `boot` as the name and click on `Create Virtual Drive`, then `OK`.
** On the left menu click on `Networking -> Adapter Card MLOM`
*** Click on the `vNICs` tab, and the factory default configuration comes with two vNICs defined with one vNIC assigned to port 0 and one vNIC assigned to port 1. Both vNICs are configured to allow any kind of traffic, with or without a VLAN tag. VLAN IDs must be managed on the operating system level. 
+
TIP: A great feature of the {vn_Cisco} VIC card is the possibility to define multiple virtual network adapters presented to the operating system, which are configured best for specific use. Like, admin traffic should be configured with MTU 1500 to be compatible with all communication partners, whereas the network for storage intensive traffic should be configured with MTU 9000 for best throughput. For high-availability, the two network devices per traffic type will be combined in a bond on the operating system layer.
+
. These new settings become active with the next power cycle of the server. At the top right side of the window click on `Host Power -> Power Off`, in the pop-up windows click on `OK`.
. On the top menu item list, select `Launch vKVM`
** Select the `Virtual Media` tab and activate `Virtual Devices` found in `Virtual Media` tab
** Click the `Virtual Media` tab to select `Map CD/DVD`
** In the `Virtual Media - CD/DVD` window, browse to respective operating system media, open and use the image for a system boot.

//-
Deployment Process::
On the respective compute module node, determine if a hypervisor is already available for the solution`s virtual machines.

. If this will be the first use of this node, an option is to deploy a KVM hypervisor, based upon {pn_SLES} by following the {pn_SLES_VirtDocURL}[Virtualization Guide].
** Given the simplicity of the deployment, the operating system and hypervisor can be installed with the {pn_SLES} ISO media and the {an_Cisco} {an_Cisco_BMC} virtual media and virtual console methodology.
. Then for the solution VM, use the hypervisor user interface to allocate the necessary CPU, memory, disk and networking as noted in the {pn_Rancher} {pn_Rancher_HWReqURL}[hardware requirements].

//-
Deployment Consideration(s)::
To further optimize deployment factors, leverage the following practices:

ifdef::FCTR+Automation[]
* <<g-automation>>
** To monitor and operate a Cisco UCS server from Intersight, the first step is to claim the device. The following procedure provides the steps to claim the Cisco UCS C240 server manually in Intersight.
*** Logon to {pn_Cisco_Intersight} web interface and navigate to `Admin > Targets`
*** On the top right corner of the window click on `Claim a New Target`
*** In the next window, select `Compute / Fabric -> {vn_Cisco} {an_CiscoUCS} Server (Standalone)`, click on `Start`
*** In another tab of the web browser, logon to the C{pn_Cisco_BMC} portal of the {an_Cisco} {an_CiscoUCS} {modelCiscoUCS-C240SD} and navigate to `Admin -> Device Connector`
*** Back in {pn_Cisco_Intersight}, enter the Device ID and Claim Code from the server and click on Claim. The server is now listed in {pn_Cisco_Intersight} under `Targets` and under `Servers`
*** Enable `Tunneld vKVM` and click on `Save`. Tunneld vKVM allows {pn_Cisco_Intersight} to open the vKVM window in case the client has no direct network access to the server on the local lan or via VPN.
*** Navigate to `Operate -> Servers ->` name of the new server to see the details and Actions available for this system.
*** The available actions are based on the {pn_Cisco_Intersight} license level available for this server and the privileges of the used user account.
+
NOTE: Please have  a look at link:{pn_Cisco_IntersightLicenseURL}[{pn_Cisco_Intersight} Licensing] to get an overview of the functions available with the different license tiers.
+
*** Now you can remotely manage the server and leverage existing or setup specific deployment profiles for the use case, plus perform the operating system installation.
+
TIP: An even more advanced infrastructure-as-code approach with {pn_Cisco_Intersight} can use link:{pn_Cisco_IntersightTFURL}[Terraform].  
endif::FCTR+Automation[]
ifdef::FCTR+Availability[]
* <<g-availability>>
** While the initial deployment only requires a single VM, as noted in later deployment sections, having multiple VMs provides resiliency to accomplish high availability. To reduce single points of failure, it would be beneficial to have the multi-VM deployments spread across multiple hypervisor nodes. So consideration of consistent hypervisor and compute module configurations, with the needed resources for the {pn_Rancher} VMs will yield a robust, reliable production implementation.
endif::FCTR+Availability[]
// ifdef::FCTR+Scaling[]
// * <<g-scaling>>
// ** FixMe - While the initial deployment only requires a single VM, as noted in later deployment sections, having multiple VMs provides resiliency to accomplish high availability. To reduce single points of failure, it would be beneficial to have the multi-VM deployments spread across multiple hypervisor nodes. So consideration of consistent hypervisor and compute module configurations, with the needed resources for the {pn_Rancher} VMs will yield a robust, reliable production implementation.
// endif::FCTR+Scaling[]

endif::IHV-Cisco-C240-SD[]
