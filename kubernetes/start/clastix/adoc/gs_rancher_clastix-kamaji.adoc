:docinfo:
include::./common_docinfo_vars.adoc[]
include::./gs_rancher_clastix-kamaji-vars.adoc[]
[#art-{article-id}]


= {title}: {subtitle}



== Introduction

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a motivation for the document in 2-4 sentences to identify:
//   - what the document is about
//   - why it may be of interest to the reader
//   - what products are being highlighted
// Include an approved SUSE | Partner logo lock-up if possible
// Include any additional, relevant details
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Efficiency, security, and cost savings are top concerns for most businesses.
This becomes particularly important for large enterprises, who need to share Kubernetes infrastructure and resources between multiple teams, departments, or business units.
These are also concerns for managed services providers, who offer Kubernetes-as-a-Service or leverage Kubernetes to deliver other services.

Multi-tenancy enables different users or tenants to securely share Kubernetes resources, simplifying administration and reducing costs.
Multi-tenancy is recognized as key to achieving these goals.
Yet Kubernetes does not have first-class concepts of end users or tenants.

image::logo-lockup_suse+clastix_hor_lightbackground.svg[SUSE | CLASTIX, scaledwidth="75%", align="center"]

By integrating {comp2-full} into your {comp1-full} environment, you gain:

* A highly scalable and high-density Kubernetes control plane infrastructure.
* Reduced operational overhead, yielding faster deployment, configuration, upgrades, and maintenance.
* Consistent configurations across multiple tenants.
* Distributed architectures across clouds, edge, and data center.
* Hard multi-tenancy with strong security and isolation.





=== Scope

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Specify what this guide covers in no more than 2 sentences.
//   E.g., "You will learn how to ..."
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Learn how to deploy {comp2-full} into an existing Kubernetes cluster managed by {comp1-full}.


=== Audience

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify for whom this document is intended, perhaps with:
//   - topics of interests (e.g., machine learning, security, etc.)
//   - job roles (e.g., developer, administrator, platform architect, etc.)
//   - required skills
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Systems architects, platform engineers, administrators, and others seeking efficient operation of Kubernetes infrastructure through multi-tenancy and secure workload isolation will find this guide relevant.

A basic understanding of Kubernetes and cluster management with {comp1-full} is needed.



== Prerequisites

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify minimum requirements (prerequisites) the reader
// would need in order to follow the steps of this guide.
// - Use an unordered list.
// - Use proper product names.
// - Identify product versions.
// - Link to existing resources whenever possible.
// - Keep this section brief but elaborate as needed.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

For this guide, you need the following:

* {comp1-website}[{comp1-full}]
//
+
You can follow this guide with {comp1-website}[{comp1}] {comp1-version1} or later.
See {comp1-docs}[Rancher Deployment Quick Start Guides] for setting up your Rancher environment.

* A Kubernetes cluster to be the Kamaji Admin Cluster and managed by {comp1}.
+
--
Any CNCF-certified Kubernetes cluster can be used, including {rke2-website}[RKE2] and {k3s-website}[K3s].
Follow the instructions of your chosen Kubernetes distribution for proper setup of the Kamaji Admin Cluster.

For this guide, in addition to the cluster's control plane nodes, you need at least 3 worker nodes.
Each worker node should have the following minimum specifications:

* 2 vCPUs

* 2 GB of RAM

* 16 GB storage

* Swap disabled

* Full network connectivity between all machines


Your Kamaji Admin Cluster also must provide the following services:

* A Container Storage Interface (CSI) module installed with a defined {comp1-docs}pages-for-subheaders/create-kubernetes-persistent-storage[Storage Class] for the tenant datastores.
For example, you can use {longhorn-website}[Rancher Longhorn] or any other persistent storage system.
The {lpp-website}[Rancher Local Path Provisioner] is also an option.

* Support for {k8s-docs}concepts/services-networking/service/#loadbalancer[Load Balancer] service types, such as {metallb-website}[MetalLB] or one provided by your cloud provider.
The addresses provided by the load balancer must be accessible by the worker nodes of the tenant clusters as well as by tenant users.

* {k8s-docs}concepts/services-networking/ingress-controllers/[Ingress Controller].
The Kamaji Console is exposed through Ingress, so the cluster needs an Ingress controller.
For RKE2 and K3s installations, you an ingress controller is installed by default.
For other Kubernetes distributions, such as AKS, EKS, or GKE, you may need to deploy the ingress controller before continuing.

* {certmanager-website}[Cert-Manager].
Kamaji takes advantage of dynamic admission control, such as validating and mutating webhook configurations.
These webhooks are secured by https://en.wikipedia.org/wiki/Transport_Layer_Security[Transport Layer Security (TLS)], and the certificates are managed by Cert-Manager.

--

* An arbitrary number of Linux machines to host multiple tenant worker nodes.
These can be bare metal systems or virtual machines, on-premises or in any cloud.
For this guide, each tenant worker node should have at least:
+
--
* 2 vCPUs

* 2 GB of RAM

* 16 GB of storage

* Swap disabled

* Full network connectivity between all machines
+
[NOTE]
====
Kubernetes components communicate through various {k8s-docs}reference/networking/ports-and-protocols/[network ports and protocols].
====


--

* A Linux workstation with the following tools installed:
+
--
* {k8s-docs}tasks/tools/#kubectl[kubectl]

* {k8s-docs}tasks/tools/#kubeadm[kubeadm]

* {helm-docs}intro/install/[Helm]

--


== Technical overview

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a technical overview of the solution.
// - Identify components.
// - Describe how the components fit together.
//   Leverage diagrams as appropriate, including (but not limited to):
//   - component architecture
//   - data flow diagram
//   - workflow diagram
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


image::rancher_clastix-kamaji_architecture.png[Kamaji + Rancher Prime Architecture, scaledwidth="75%", align="center"]


{comp2-website}[{comp2-full}] turns any CNCF-compliant Kubernetes cluster into an “Admin Cluster” to orchestrate other Kubernetes clusters called “tenant clusters.”
With {comp2}, the tenant control planes run in pods on the Admin Cluster instead of on dedicated machines.
This makes running Kubernetes at scale less costly, easier to deploy, and simpler to operate while providing users with a fully managed, native Kubernetes experience.

After a tenant cluster is created, you can import it into {comp1-website}[{comp1-full}] for centralized management of your Kubernetes landscape, empowering global administrators to streamline operations and improve consistency and security through an intuitive interface as well as GitOps-driven workflows.


=== Components and tools

Key Kamaji components discussed in this guide are:

Kamaji Operator::
+
--
The Kamaji Operator is installed on the Admin Cluster.
It is responsible for creating and monitoring multiple custom resources called tenant control planes (TCPs).

* The Kamaji Operator continuously checks for any deviations or changes in the TCPs.
If it detects any drift, such as misconfigurations or inconsistencies, it initiates immediate reconciliation to bring the TCPs back to their desired states.

* The Kamaji Operator rolls out new versions of TCPs and seamlessly migrates between different datastores.
It ensures smooth transitions and minimizes disruptions during updates or changes to the control plane infrastructure.

--

Kamaji Console::
//
+
The Kamaji Console is a Web-based, graphical interface for global administrators.

Tenant Control Plane (TCP)::
Running in the Admin Cluster as pods, tenant control planes provide dedicated control plane capabilities for each tenant cluster.
They expose the control plane endpoint (the address and port) to the tenant’s worker nodes through a balanced and secure network service.

Datastore::
Installed on the Admin Cluster, the datastore is responsible for storing the state of the tenant clusters into a multi-tenant capable datastore as etcd running on the Admin Cluster.
The relationship between the datastore and TCPs can be one-to-one or one-to-many.
Thanks to {kine-website}[kine] integration, an open source component from SUSE acting as a shim for `etcd`, Kamaji supports other datastore types, including PostgreSQL and MySQL.
+
[TIP]
====
It is highly recommended that you use a managed datastore in production, such as {cloudnativepg-website}[CloudNativePG], an open source, PostgreSQL distribution for Kubernetes.
You can also use the {{comp2-etcd-docs}}[kamaji-etcd] Helm chart to set up a multi-tenant etcd datastore, running as a StatefulSet of three replicas.
====

Tenant Worker Nodes::
Tenant worker nodes run workloads of the respective tenants.
They consist of virtual or bare metal machines that are connected to the TCP through a secure network connection.
Tenant worker nodes can be isolated by infrastructure for hard multi-tenancy and can run on different infrastructures in data centers, clouds, and edge locations.


Konnectivity::
Konnectivity is a cloud-native network technology that facilitates traffic between the TCP and the worker nodes.
It establishes a secure tunnel between the TCP and the tenant worker nodes, which is especially useful when the worker nodes are not directly reachable from the TCP.


You can find additional details in the {comp2-docs}[{comp2} documentation].


=== Workflow

The workflow for this guide and in general for working with {comp2} in a {comp1} landscape is as follows:

. Installing {comp2}

.. Installing {comp2} Operator

.. Installing {comp2} Console

.. Verifying {comp2} Operator and {comp2} Console

.. Installing {comp2} UI Extension for {comp1}

. Provisioning a tenant cluster

.. Deploying a tenant control plane

.. Joining worker nodes

.. Installing the Cluster Network Interface

. Importing the tenant cluster into {comp1}




== Installation


{comp2-full} is available for easy installation through the {comp1} User Interface (UI).
{comp1} Apps is a curated collection of software, packaged and maintained as Helm charts to simplify installation.
The {comp2} Operator and the {comp2} Console are both available as Rancher App charts.

In addition, {comp2-provider} has created a {comp1} UI Extension.
{comp1} UI Extensions allow users, developers, partners, and customers to extend and enhance the {comp1} UI.


=== Install the {comp2} Operator

Install the {comp2} Operator and default datastore with the {comp1} Apps chart.

. Log in to the {comp1} UI.

. Select the {comp2} Admin Cluster you provisioned as part of the prerequisites for this guide.

. Navigate to __Apps__ > __Charts__ and search for 'kamaji'.
+
image::clastix-kamaji_01_apps-charts.png[Kamaji Rancher Charts, scaledwidth="85%", align="center"]

. Click the __Kamaji__ chart to begin installation of the Kamaji Operator.

. In __Namespace__, select 'Create new namespace' and enter a name, such as 'kamaji-system'.
//
+
Optionally select __Customize Helm options before install__ to customize the deployment.
+
image::clastix-kamaji_02_kamaji-namespace.png[Kamaji Rancher Chart - Namespace, scaledwidth="85%", align="center"]


. Click __Next__, then click __Install__.



=== Install the {comp2} Console

Install the {comp2} Console through the {comp1} Apps chart.

. In the {comp1} UI, select __Apps__ > __Charts__ and search for 'Kamaji Console'.

. Click the __Kamaji Console__ chart to begin installation.

. Select 'kamaji-system' in __Namespace__, then click __Next__.
//
+
Optionally, select __Customize Helm options before install__ to customize the deployment.
+
image::clastix-kamaji_03_kamaji-console-step1.png[Kamaji Console Rancher Chart Installation Step 1, scaledwidth="85%", align="center"]

. Select __Console Configuration__ and make the following adjustments:

.. Enable the __Generate Console Config Secret__ option.

.. Fill in each of the required fields with appropriate values.
+
image::clastix-kamaji_04_kamaji-console-config.png[Kamaji Console Rancher Chart Installation Step 2 - Console Configuration, scaledwidth="85%", align="center"]

. Select __Ingress Configuration__ and make the following adjustments:

.. Ensure __Manage Ingress Status__ is enabled.

.. Fill in each of the required fields with appropriate values.
+
image::clastix-kamaji_05_kamaji-console-ingress.png[Kamaji Console Rancher Chart Installation Step 2 - Ingress Configuration, scaledwidth="85%", align="center"]

.. Finish the installation by clicking __Install__.


=== Verify installation of {comp2} Operator and {comp2} Console

Verify that the {comp2} Operator and {comp2} Console are installed.

. In the {comp1} UI, make sure you have selected the {comp2} Admin Cluster.

. Select __Installed Apps__.

. Verify that 'kamaji', 'console', and 'etcd' are listed.
+
image::clastix-kamaji_06_verify-apps.png[Installed Kamaji Apps, scaledwidth="85%", align="center"]



=== Install Kamaji UI Extension for {comp1}

{comp1-docs}integrations-in-rancher/rancher-extensions[{comp1} UI Extensions] allow users, developers, partners, and customers to extend and enhance the {comp1} UI.
Examples of built-in {comp2} extensions are Fleet, Explorer, and Harvester.
Other extensions that use the extensions API can be manually added.

. In the {comp1} UI, select __Extensions__.

. Add the Partner Extensions repository.

.. Click the three vertical dots in the upper right of the screen and select __Manage Repositories__ > __Create__.
+
image::clastix-kamaji_07_extensions-manage-repos.png[Rancher Extensions page, scaledwidth="85%", align="center"]

.. For __Name__, enter 'partner-extensions'.

.. For __Git Repo URL__ enter 'https://github.com/rancher/partner-extensions'.

.. For __Git Branch__, enter 'main'.

.. Click __Create__ to add the Partner Extensions repository.

. Install the {comp2} Extension.

.. Select the __Available__ tab.

.. Locate the {comp2} Extension and click __Install__.

.. After installation completes, click __Reload__.
+
image::clastix-kamaji_08_extensions-reload.png[Rancher Extensions - Reload, scaledwidth="50%", align="center"]

. Verify that the {comp2} Extension is installed.

.. Note that the {comp2} Extension appears in the __Installed__ tab of the __Extensions__ page.
+
image::clastix-kamaji_09_extension-installed.png[Rancher Extensions - Kamaji installed, scaledwidth="70%", align="center"]

.. With the {comp2} Extension installed, the {comp1} UI includes a new __Multitenancy Management__ menu option for each managed cluster.
+
image::clastix-kamaji_10_extension-menu.png[Rancher Extensions - Multitenancy Management menu, scaledwidth="70%", align="center"]




== Provision a tenant cluster

With the {comp2} Operator, the {comp2} Console, and the {comp2} Extension deployed to your {comp2} Admin Cluster, you are ready to provision tenant (downstream) clusters.



=== Deploy a tenant control plane

The first step to provision a tenant cluster is to create a tenant control plane (TCP) in the {comp2} Admin Cluster.

[IMPORTANT]
====
Tenant control plane pods are exposed by a load balancer service that is the 'ControlPlaneEndpoint' for the worker nodes.
Make sure your {comp2} Admin Cluster supports the creation of the 'LoadBalancer' service type and that IP addresses can be provisioned and assigned.
Otherwise, the {comp2} Operator will wait indefinitely to deploy your tenant control plane.
====


. In the {comp1} UI, select your {comp2} Admin Cluster.

. Select __Multitenancy Management__ and click __Kamaji Console__.
+
image::clastix-kamaji_11_extension-console.png[Rancher Extensions - launch Kamaji Console, scaledwidth="70%", align="center"]
+
The {comp2} Console opens in another tab or window of your browser.

. Log in to the {comp2} Console UI with the e-mail address and password you set during deployment.

. In the {comp2} Console, select __Tenant Control Planes__ in the left panel, then click __Create__.
+
image::clastix-kamaji_12_tcp-create.png[Tenant Control Planes - Create, scaledwidth="60%", align="center"]

. You are presented a sample TCP YAML file for configuring the tenant control plane.
Adjust Kubernetes version, ServiceType, and other values for your infrastructure.
+
image::clastix-kamaji_13_tcp-create-editor.png[Create TCP editor, scaledwidth="75%", align="center"]
+
For convenience, a sample TCP YAML file is provided below.
+
[listing]
----
apiVersion: kamaji.clastix.io/v1alpha1
kind: TenantControlPlane
metadata:
  name: sample
  namespace: default
spec:
  dataStore: default
  controlPlane:
    deployment:
      replicas: 2
    service:
      serviceType: LoadBalancer
  kubernetes:
    version: v1.25.4
    kubelet:
      cgroupfs: systemd
  networkProfile:
    port: 6443
  addons:
    coreDNS: {}
    kubeProxy: {}
    konnectivity:
      server:
        port: 8132
        version: v0.0.32
      agent:
        version: v0.0.32
----
+
[NOTE]
====
If you are not using the default namespace, make sure the namespace exists before applying the configuration.
====

. Click __Create__ to deploy the tenant control plane.
//
+
The {comp2} Operator creates the tenant control plane as declared in the TCP YAML file, including Secrets to store the certificates used to access the tenant cluster.

. You can see an overview of the 'sample' tenant control plane that was created in the {comp2} Console.
+
image::clastix-kamaji_14_tcp-sample.png[Tenant Control Plane overview, scaledwidth="75%", align="center"]

. Click __VIEW KUBECONFIG__ to retrieve the Kubeconfig for your tenant control plane and save it as the file, `default-sample.kubeconfig`.



=== Prepare worker nodes

Be sure the bare metal or virtual machines you use as your worker nodes have the following components installed:

* containerd

* crictl

* kubectl

* kubelet

* kubeadm

[TIP]
====
The https://github.com/clastix/yaki/tree/master[nodesetup.sh script] can automate installation of these prerequisites for Ubuntu 22.04 and can be modified for your preferred operating system.
====



=== Join worker nodes

The tenant control plane is made of pods running in the {comp2} Admin Cluster.
At this point, the tenant cluster has no worker nodes.
So, the next step is to join some worker nodes to the tenant control plane.

{comp2} leverages the https://github.com/kubernetes-sigs/cluster-api[Cluster Management API (CAPI)] project.
This allows you to create the tenant clusters, including worker nodes, in a completely declarative way.
Refer to the https://github.com/clastix/cluster-api-control-plane-provider-kamaji[Kamaji CAPI providers repository] to learn more about supported providers.


The current approach for joining nodes is to run a `kubeadm` command on each node.

. Open the command line on your Linux workstation.

. Store the IP address (or host name) of each node in a variable.
+
[source, console]
----
WORKER0=<address of first node>
WORKER1=<address of second node>
WORKER2=<address of third node>
----

. Store the join command in a variable.
+
[source, console]
----
JOIN_CMD=$(echo "sudo ")$(kubeadm --kubeconfig=default-sample.kubeconfig token create --print-join-command)
----

. Use a loop to log in to and run the join command on each node.
+
[source, console]
----
HOSTS=(${WORKER0} ${WORKER1} ${WORKER2})
for i in "${!HOSTS[@]}"; do
  HOST=${HOSTS[$i]}
  ssh ${USER}@${HOST} -t ${JOIN_CMD};
done
----

. You can check the status of the worker nodes from the command line with:
+
[source, console]
----
kubectl --kubeconfig=default-sample.kubeconfig get nodes
----

[TIP]
====
This process can be further automated to handle the node prerequisites and joining.
See https://github.com/clastix/yaki/tree/master[yaki nodesetup.sh script], which you could modify for your preferred operating system.
====



=== Install the Cluster Network Interface

Your tenant cluster also needs a {comp1-docs}faq/container-network-interface-providers[Container Network Interface (CNI)] plugin.
The CNI plugin enables seamless communication and connectivity between containers and external networks.
For this guide, you use the {calico-website}[Calico] CNI.

. Download the latest, stable Calico manifest to your Linux workstation.
//
+
For example:
+
[source, console]
----
curl https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml -O
----

. Apply the manifest to your tenant cluster.
+
[source, console]
----
kubectl --kubeconfig=default-sample.kubeconfig apply -f calico.yaml
----
+
[TIP]
====
You can check the status from the command line:

[source, console]
----
kubectl --kubeconfig=default-sample.kubeconfig get nodes
----
====

. When the nodes are ready, they are visible to you in the {comp2} Console.
+
image::clastix-kamaji_15_console-tenant-nodes-visible.png[Tenant Nodes in the Kamaji Console, scaledwidth="85%", align="center"]



== Import the tenant cluster into {comp1}

Bring your tenant clusters into {comp1} for unified management and oversight of your Kubernetes landscape.

. Log in to the {comp1} UI.

. In __Cluster Management__, select __Clusters__.

. Click __Import Existing__.

. Enter a 'Cluster Name'.

. Click __Create__.

. Copy the `kubectl` command displayed in the {comp1} UI to your clipboard and run it against the tenant cluster on the command line of your Linux workstation.
+
[WARNING]
====
Make sure you use the Kubeconfig related to the tenant cluster you wish to import.
====

. Your tenant cluster is in a 'Pending' state while {comp1} deploys resources to manage it.
This may take a few minutes.

. When the state changes to 'Active', your tenant cluster has been imported.
+
image::clastix-kamaji_16_imported-tenants.png[Tenant clusters imported into Rancher, scaledwidth="85%", align="center"]

. You now have a unified view and central management of your Kubernetes landscape with {comp1}.




== Summary

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Summarize:
// - Solution description
// - Motivation for the guide
// - What was done
// - Suggested next steps for the learning journey
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

{comp1-full} empowers enterprises to streamline multi-cluster Kubernetes operations everywhere with unified security, policy, and user management.
{comp2-full} delivers a highly scalable and high-density Kubernetes control plane infrastructure with reduced operational overhead, yielding faster deployment, configuration, upgrades, and maintenance.
Together, {comp1-provider} and {comp2-provider} enable enterprises, managed services providers, and others to leverage Kubernetes resources more efficiently and enable secure Kubernetes-as-a-Service to multiple departments and clients.

In this guide, you learned how to seamlessly deploy {comp2-full} into your {comp1-full} Kubernetes landscape, create tenant clusters, and import them into {comp1} for management.

Continue your journey by watching https://youtu.be/VXHNrMmlF8U[Rancher and Kamaji: solving multi-tenancy challenges in the Kubernetes world].







// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
