:docinfo:
include::./common_docinfo_vars.adoc[]

// test
// Variables & Attributes
:title: Kasten K10 by Veeam with SUSE Rancher : Getting Started
:productname: SUSE Rancher 2.6
:partnerproductname: Kasten K10 by Veeam
:author1: Adam Bergh, Solutions Architect, Cloud Native Technical Partnerships, Kasten by Veeam
:author2: Gerson Guevara, IHV Solutions Architect, SUSE
:author3: Terry Smith, Global Partner Solutions Director, SUSE
:revdate: March 08, 2022
:revnumber: 20220308
//:toc2:
//:toc-title: {title}
//:toclevels: 4


= {title}



== Introduction

=== Motivation

// Why this would be of interest
// Challenges
// Benefits

Organizations are shifting to cloud native, leveraging containerized workloads and Kubernetes management platforms, like SUSE Rancher, to gain greater flexibility, scale, and resilience in order to accelerate innovation and quickly adjust to dynamic conditions.
In this always-on IT environment, application mobility, and data protection are critical considerations. 


The Kasten K10 by Veeam® data management platform provides enterprise operations teams with an easy-to-use, scalable, and secure system for backup and restore, disaster recovery, and mobility of cloud native applications. 


=== Scope

// This guide will help you take the first steps to 
This guide provides an overview of the steps to install and set up Kasten K10 by Veeam in your SUSE Rancher Kubernetes environment and to perform a simple backup and restore of an application.


=== Audience

// This document is intended for 
This document is intended for IT operations teams, backup administrators, DevOps and DevSecOps teams, and others who are responsible for ensuring business continuity, disaster recovery, ransomware and threat reduction, and application migration for cloud native landscapes.



== Technical overview

// Description
The Kasten K10 by Veeam® data management platform has deep integrations with SUSE Rancher and has an extensive ecosystem of support across Kubernetes distributions and cloud platforms to provide enterprise operations teams flexibility to choose the deployment environments that best meet their needs - on-premises, public cloud, and hybrid.
K10 is policy-driven and extensible, delivering enterprise features, such as full-spectrum consistency, database integrations, automatic application discovery, multi-cloud mobility, and a powerful web-based user interface.

// Architecture diagram
image::rancher_veeam-kasten_architecture-1.png[scaledwidth="85%", align="center"]


== Prerequisites
// Minimum requirements (prerequisites) for this guide
// * Requirement 1 https://url[url]
// * Requirement 2 https://url[url]
For this guide, you will need the following:

* SUSE Rancher
+
See https://rancher.com/docs/rancher/v2.6/en/installation/requirements/[SUSE Rancher installation guide]

* Kubernetes cluster managed by SUSE Rancher
+
See https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-6-3/[SUSE Rancher Support Matrix]

* Storage for backup target
+
An external backup storage target, such as an NFS file server or cloud object store.  This document will use an external, S3-compatible object storage bucket.

* User application to demonstrate back up and restore capability
+
For example, WordPress can be easily installed by https://bitnami.com/stack/wordpress/helm[Helm chart].


K10 can be installed in a variety of different environments.
To ensure a smooth installation experience, run the pre-flight checks to ensure that the prerequisites are met. 

Assuming that your default kubectl context is pointed to the cluster into which you want to install K10, you can run pre-flight checks by deploying the primer tool.
This tool runs in a pod in the cluster and does the following:

* Validates that the Kubernetes settings meet the K10 requirements. 

* Catalogs the available StorageClasses. 

* If a CSI provisioner exists, it will also perform a basic validation of the cluster's CSI capabilities and any relevant objects that may be required.
+
See https://docs.kasten.io/latest/install/storage.html#csi-preflight[CSI pre-flight checks] in the documentation for more details.


Run the following command to deploy the pre-check tool: 
[source, bash]
----
curl https://docs.kasten.io/tools/k10_primer.sh | bash
----

NOTE: This will create and clean up a ServiceAccount and ClusterRoleBinding to perform sanity checks on your Kubernetes cluster.



== Installation

Kasten K10 can be easily deployed from the SUSE Rancher __Apps & Marketplace__. 

. Create a new namespace for the Kasten K10 application. 

.. In the SUSE Rancher user interface (UI), navigate to __Clusters__ -> __Project/Namespaces__.
+
image::rancher_veeam-kasten_step1-1.png[scaledwidth="85%", align="center"]

.. Create a "kasten-io" namespace for Kasten K10.
+
image::rancher_veeam-kasten_step1-2.png[scaledwidth="85%", align="center"]

. Install Kasten K10.

.. Navigate to __Apps & Marketplace__ > __Charts__ within the SUSE Rancher UI and search for “Kasten.”
+
image::rancher_veeam-kasten_step2-1.png[scaledwidth="85%", align="center"]

.. Click the K10 chart and then click __Install__.
+
image::rancher_veeam-kasten_step2-2.png[scaledwidth="85%", align="center"]

.. Set the namespace to “kasten-io” and check the __Customize Helm Options__ box.
+
image::rancher_veeam-kasten_step2-3.png[scaledwidth="85%", align="center"]

.. Customize the Kasten K10 deployment, if needed.
The chart values screen, “Install: Step 2”, allows for customization of the Kasten K10 deployment.
This may be necessary, depending on the Kubernetes distribution into which you are installing.i
See the documentation for a detailed explanation of https://docs.kasten.io/latest/install/advanced.html#complete-list-of-k10-helm-options[available installation options].
After setting your chart values, click __Next__.
+
image::rancher_veeam-kasten_step2-4.png[scaledwidth="85%", align="center"]

.. Click __Install__ to install Kasten K10.
+
image::rancher_veeam-kasten_step3-1.png[scaledwidth="85%", align="center"]


== Validating the installation

To validate that Kasten K10 has been properly installed, the following command can be run in K10's namespace ("kasten-io") to watch for the status of all K10 pods:
+
[source, bash]
----
kubectl get pods --namespace kasten-io --watch
----

It may take a couple of minutes for all pods to come up and display "Running" status.
+
[source, bash]
----
kubectl get pods --namespace kasten-io

NAMESPACE     NAME                                    READY   STATUS    RESTARTS   AGE
kasten-io     aggregatedapis-svc-b45d98bb5-w54pr      1/1     Running   0          1m26s
kasten-io     auth-svc-8549fc9c59-9c9fb               1/1     Running   0          1m26s
kasten-io     catalog-svc-f64666fdf-5t5tv             2/2     Running   0          1m26s
...
----

In the unlikely scenario that pods that are stuck in any other state, please eehe https://docs.kasten.io/latest/operating/support.html#support[support documentation] to debug further.


== Accessing the K10 Dashboard

There are several options for accessing the Kasten K10 dashboard.


=== Access via kubectl

The Kasten K10 dashboard is not exposed externally by default.  To establish a connection, use the following kubectl command:
+
[source, bash]
----
kubectl --namespace kasten-io port-forward service/gateway 8080:8000
----

NOTE: If you installed Kasten K10 with a different release name than k10 (specified via the `--name` option in the install command), replace the last occurrence of "k10" in the above URL with the specified release name. The revised URL would look like `http://127.0.0.1:8080/<release-name>/#/`.


NOTE: If you are running on GKE and want to access the dashboard without local kubectl access, see https://docs.kasten.io/latest/access/gcp_details/gcp_console_dashboard.html[K10 Dashboard Directly From the Google Cloud Console].


=== Direct access

https://docs.kasten.io/latest/access/authentication.html#id5[Direct access] to the K10 dashboard requires that an authentication method is properly configured to secure access.

If accessing the K10 API directly or using kubectl, any authentication method configured for the cluster is acceptable.  For more information, see https://kubernetes.io/docs/reference/access-authn-authz/authentication/[Kubernetes authentication].


https://docs.kasten.io/latest/access/authentication.html#id6[Basic] authentication allows you to protect access to the K10 dashboard with a user name and password. To enable basic authentication, you will first need to generate https://httpd.apache.org/docs/2.4/programs/htpasswd.html[htpasswd] credentials by either using an online http://www.htaccesstools.com/htpasswd-generator/[tool] or via the htpasswd binary found on most systems. Once generated, you need to supply the resulting string with the Helm install or upgrade command using the following flags:
+
[source, bash]
----
--set auth.basicAuth.enabled=true \
--set auth.basicAuth.htpasswd='example:$apr1$qrAVXu.v$Q8YVc50vtiS8KPmiyrkld0'
----

Alternatively, you can use an existing secret that contains a file created with htpasswd. The secret must be in the K10 namespace. This secret must be created with the key named auth and the value as the password generated using htpasswd in the data field of the secret.
+
[source, bash]
----
--set auth.basicAuth.enabled=true \
--set auth.basicAuth.secretName=my-basic-auth-secret
----

https://docs.kasten.io/latest/access/authentication.html#id7[Token] authentication is also supported.  To enable token authentication, use the following flag as part of the initial Helm install or subsequent Helm upgrade command.
+
[source, bash]
----
--set auth.tokenAuth.enabled=true
----

Once the dashboard is configured, you will be prompted to provide a bearer token that will be used when accessing the dashboard.

//kubernetes_gs_rancher_veeam-kasten_image_kasten_signin-1
image::rancher_veeam-kasten_kasten_signin-1.png[scaledwidth="85%", align="center"]


Token authentication allows using any token that can be verified by the Kubernetes server.  For more information about token authentication, see:

•  https://docs.kasten.io/latest/access/authentication.html#id8 [Obtaining Tokens]
•  https://kubernetes.io/docs/reference/access-authn-authz/authentication/#authentication-strategies [Authentication Strategies]

The most common token type that you can use is a service account bearer token.
You can use kubectl to extract such a token from a service account that you know has the proper permissions.

•  Get the SA secret

[source, bash]
----
sa_secret=$(kubectl get serviceaccount my-kasten-sa -o jsonpath="{.secrets[0].name}" --namespace kasten-io)
----

•  Extract the token

[source, bash]
----
kubectl get secret $sa_secret --namespace kasten-io -ojsonpath="{.data.token}{'\n'}" | base64 --decode
----

Alternatively, you can create a new service account from which to extract the token.

[source, bash]
----
kubectl create serviceaccount my-kasten-sa --namespace kasten-io
----

In this case, you will need to create a role binding or cluster role binding for the account to ensure that it has the appropriate permissions for K10.
To learn more about the necessary K10 permissions, see https://docs.kasten.io/latest/access/authorization.html#authz [Authorization.]

Demonstration

Kasten K10 Dashboard Overview

The K10 dashboard is broken up into several different sections. A brief description is provided in the sections below.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-1
image::rancher_veeam-kasten_K10_Dashboard-1.png[scaledwidth="85%", align="center"]

It is also possible to perform an interactive walkthrough of the K10 dashboard via a Guided Tour. The tour is available when the K10 dashboard is accessed for the first time or via the Dashboard option on the https://docs.kasten.io/latest/usage/overview.html#k10-settings [Settings] page.

System Overview

The top of the K10 dashboard displays a list of applications (currently mapped to namespaces), any policies that might exist in the system, and a summary of the cluster's backup data footprint.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-2
image::rancher_veeam-kasten_K10_Dashboard-2.png[scaledwidth="85%", align="center"]

After filtering to only include applications that have stateful services (defined as containing a persistent volume), the above screen breaks down each section into three categories:

•  Unmanaged: There are no protection policies that cover this object
•  Non-compliant: A policy applies to this object but the actions associated with the policy are failing (e.g., due to underlying storage slowness, configuration problems, etc.) or the actions haven't been invoked yet (e.g., right after policy creation)
•  Compliant: Objects that both policies apply to and the policy SLAs are being respected

Applications, Namespaces, and Workloads

The K10 platform by default equates names to applications for ease of use and consistency with Kubernetes best practices, use of RBAC, and to mirror the most common application deployment pattern. However, as shown later, policies can be defined to operate on more than one namespace or only operate on a subset of an application residing in a single namespace.

Assuming you have already installed applications, clicking on the Applications card on the dashboard will take you to the following view. Note that if you clicked on one of the Compliant/Non-Compliant/Unmanaged buttons, it would automatically filter the view below for you.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-3
image::rancher_veeam-kasten_K10_Dashboard-3.png[scaledwidth="85%", align="center"]

An application, in turn, is made up of multiple Kubernetes resources and workloads including deployments and stateful sets. 

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-4
image::rancher_veeam-kasten_K10_Dashboard-4.png[scaledwidth="85%", align="center"]

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-5
image::rancher_veeam-kasten_K10_Dashboard-5.png[scaledwidth="85%", align="center"]

Within K10, policies are used to automate your data management workflows. Going back to the main dashboard, you will find a section on how to manage policies right next to the Applications card. To achieve this, they combine actions you want to take (e.g., snapshot), a frequency or schedule for how often you want to take that action, and selection criteria for the resources you want to manage.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-6
image::rancher_veeam-kasten_K10_Dashboard-6.png[scaledwidth="85%", align="center"]

If you click on the Policies card, you will notice in the above screenshot that no default policies are created at install time but a policy can be either created from this page or from the application page shown earlier.

Creating a Location Profile 

K10 can usually invoke protection operations such as snapshots within a cluster without requiring additional credentials. While this might be sufficient if K10 is running in some of (but not all) the major public clouds and if actions are limited to a single cluster, it is not sufficient for essential operations such as performing real backups, enabling cross-cluster and cross-cloud application migration, and enabling DR of the K10 system itself.
To enable these actions that span the lifetime of any one cluster, K10 needs to be configured with access to external object storage or external NFS file storage. This is accomplished via the creation of Location Profiles.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-7
image::rancher_veeam-kasten_K10_Dashboard-7.png[scaledwidth="85%", align="center"]

Profile creation can be accessed from the Settings icon in the top-right corner of the dashboard or via the https://docs.kasten.io/latest/api/profiles.html#api-profile [CRD-based Profiles API].
Location profiles are used to create backups from snapshots, move applications and their data across clusters and potentially across different clouds, and to subsequently import these backups or exports into another cluster. To create a location profile, click New Profile on the profiles page.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-8
image::rancher_veeam-kasten_K10_Dashboard-8.png[scaledwidth="85%", align="center"]

Object Storage Location

As mentioned above, exporting or importing data requires an object storage location. You are therefore required to pick an object storage provider, a region for the bucket if being used in a public cloud, and the bucket name. If a bucket with the given name does not exist, it will be created.

If an S3-compatible object storage system is used that is not hosted by one of the supported cloud providers, an S3 endpoint URL will need to be specified and optionally, SSL verification might need to be disabled. Disabling SSL verification is only recommended for test setups.

NOTES: When certain cloud providers (e.g., AWS) are selected, provider-specific options (e.g., IAM Roles) will appear for configuration if needed.

When Validate and Save is selected, the config profile will be created and a profile similar to the following will appear:

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-9
image::rancher_veeam-kasten_K10_Dashboard-9.png[scaledwidth="85%", align="center"]

Protecting Applications

Protecting an application with K10, usually accomplished by creating a policy, requires the understanding and use of three concepts:
•  Snapshots and Backups: Depending on your environment and requirement, you might need just one or both of these data capture mechanisms
•  Scheduling: Specification of application capture frequency and snapshot/backup retention objectives
•  Selection: This defines not just which applications are protected by a policy but, whenever finer-grained control is needed, resource filtering can be used to restrict what is captured on a per-application basis

This section demonstrates how to use these concepts in the context of a K10 policy to protect applications. Today, an application for K10 is defined as a collection of namespaced Kubernetes resources (e.g., ConfigMaps, Secrets), relevant non-namespaced resources used by the application (e.g., StorageClasses), Kubernetes workloads (i.e., Deployments, StatefulSets, OpenShift DeploymentConfigs, and standalone Pods), deployment and release information available from Helm v3, and all persistent storage resources (e.g., PersistentVolumeClaims and PersistentVolumes) associated with the workloads.

While you can always create a policy from scratch from the policies page, the easiest way to define policies for unprotected applications is to click on the Applications card on the main dashboard. This will take you to a page where you can see all applications in your Kubernetes cluster.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-10
image::rancher_veeam-kasten_K10_Dashboard-10.png[scaledwidth="85%", align="center"]

To protect any unmanaged application, simply click Create a policy and, as shown below, that will take you to the policy creation section with an auto-populated policy name that you can change. The concepts highlighted above will be described in the below sections in the context of the policy creation workflow.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-11
image::rancher_veeam-kasten_K10_Dashboard-11.png[scaledwidth="85%", align="center"]

Snapshots and Backups

All policies center around the execution of actions and, for protecting applications, you start by selecting the snapshot action with an optional backup (currently called export) option to that action.

Snapshots

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-12
image::rancher_veeam-kasten_K10_Dashboard-12.png[scaledwidth="85%", align="center"]

Note

A number of public cloud providers (e.g., AWS, Azure, Google Cloud) actually store snapshots in object storage and they are retained independent of the lifecycle of the primary volume. However, this is not true of all public clouds (e.g., IBM Cloud) and you might also need to enable backups in public clouds for safety. Please check with your cloud provider's documentation for more information.

Snapshots are the basis of persistent data capture in K10. They are usually used in the context of disk volumes (PVC/PVs) used by the application but can also apply to application-level data capture (e.g., with Kanister).

Snapshots, in most storage systems, are very efficient in terms of having a very low performance impact on the primary workload, requiring no downtime, supporting fast restore times, and implementing incremental data capture.

However, storage snapshots usually also suffer from constraints such as having relatively low limits on the maximum number of snapshots per volume or per storage array. Most importantly, snapshots are not always durable. First, catastrophic storage system failure will destroy your snapshots along with your primary data. Further, in a number of storage systems, a snapshot's lifecycle is tied to the source volume. So, if the volume is deleted, all related snapshots might automatically be garbage collected at the same time. It is therefore highly recommended that you create backups of your application snapshots too.

Backups

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-13
image::rancher_veeam-kasten_K10_Dashboard-13.png[scaledwidth="85%", align="center"]

Scheduling

There are four components to scheduling:

•  How frequently the primary snapshot action should be performed
•  How often snapshots should be exported into backups
•  Retention schedule of snapshots and backups
•  When the primary snapshot action should be performed

Action Frequency

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-14
image::rancher_veeam-kasten_K10_Dashboard-14.png[scaledwidth="85%", align="center"]

Actions can be set to execute at an hourly, daily, weekly, monthly, or yearly granularity, or on demand. By default, actions set to hourly will execute at the top of the hour and other actions will execute at midnight UTC.

It is also possible to select the time at which scheduled actions will execute and sub-frequencies that execute multiple actions per frequency. See https://docs.kasten.io/latest/usage/protect.html#advanced-schedule-options [Advanced Schedule] Options below.

Sub-hourly actions are useful when you are protecting mostly Kubernetes objects or small data sets. Care should be taken with more general-purpose workloads because of the risk of stressing underlying storage infrastructure or running into storage API rate limits. Further, sub-frequencies will also interact with retention (described below). For example, retaining 24 hourly snapshots at 15-minute intervals would only retain 6 hours of snapshots.

Snapshot Exports to Backups

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-15
image::rancher_veeam-kasten_K10_Dashboard-15.png[scaledwidth="85%", align="center"]

Backups performed via exports, by default, will be set up to export every snapshot into a backup. However, it is also possible to select a subset of snapshots for exports (e.g., only convert every daily snapshot into a backup).

Retention Schedules

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-16
image::rancher_veeam-kasten_K10_Dashboard-16.png[scaledwidth="85%", align="center"]

A powerful scheduling feature in K10 is the ability to use a https://en.wikipedia.org/wiki/Backup_rotation_scheme#Grandfather-father-son [GFS retention scheme] for cost savings and compliance reasons. With this backup rotation scheme, hourly snapshots and backups are rotated on an hourly basis with one graduating to daily every day and so on. It is possible to set the number of hourly, daily, weekly, monthly, and yearly copies that need to be retained and K10 will take care of both cleanup at every retention tier as well as graduation to the next one. For on demand policies it is not possible to set a retention schedule

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-17
image::rancher_veeam-kasten_K10_Dashboard-17.png[scaledwidth="85%", align="center"]

By default, backup retention schedules will be set to be the same as snapshot retention schedules but these can be set to independent schedules if needed. This allows users to create policies where a limited number of snapshots are retained for fast recovery from accidental outages while a larger number of backups will be stored for long-term recovery needs. This separate retention schedule is also valuable when limited number of snapshots are supported on the volume but a larger backup retention count is needed for compliance reasons.

The retention schedule for a policy does not apply to snapshots and backups produced by https://docs.kasten.io/latest/usage/protect.html#manual-policy-runs [manual policy runs]. Any artifacts created by a manual policy run will need to be manually cleaned up. Snapshots and backups created by scheduled runs of a policy can be retained and omitted from the retention counts by adding a k10.kasten.io/doNotRetire: "true" label to the https://docs.kasten.io/latest/api/actions.html#api-run-action[RunAction] created for the policy run.

Advanced Schedule Options

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-18
image::rancher_veeam-kasten_K10_Dashboard-18.png[scaledwidth="85%", align="center"]

By default, actions set to hourly will execute at the top of the hour and other actions will execute at midnight UTC.

The Advanced Options settings enable picking how many times and when actions are executed within the interval of the frequency. For example, for a daily frequency, what hour or hours within each day and what minute within each hour can be set.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-19
image::rancher_veeam-kasten_K10_Dashboard-19.png[scaledwidth="85%", align="center"]

The retention schedule for the policy can be customized to select which snapshots and backups will graduate and be retained according to the longer period retention counts.

By default, hourly retention counts apply to the hourly at the top of the hour, daily retention counts apply to the action at midnight, weekly retention counts refer to midnight Sunday, monthly retention counts refer to midnight on the 1st of each month, and yearly retention counts refer to midnight on the 1st of January (all UTC).

When using sub-frequencies with multiple actions per period, all of the actions are retained according to the retention count for that frequency.
The Advanced Options settings allows a user to display and enter times in either local time or UTC. All times are converted to UTC and K10 policy schedules do not change for daylight savings time.

Application Selection and Exceptions

This section describes how policies can be bound to applications, how namespaces can be excluded from policies, how policies can protect cluster-scoped resources, and how exceptions can be handled.

Application Selection

You can select applications by two specific methods:
•  Application Names
•  Labels

Selecting By Application Name

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-20
image::rancher_veeam-kasten_K10_Dashboard-20.png[scaledwidth="85%", align="center"]

The most straightforward way to apply a policy to an application is to use its name (which is derived from the namespace name). Note that you can select multiple application names in the same policy.

Selecting By Application Name Wildcard

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-21
image::rancher_veeam-kasten_K10_Dashboard-21.png[scaledwidth="85%", align="center"]

For policies that need to span similar applications, you can select applications by an application name wildcard. Wildcard selection will match all application that start with the wildcard specified.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-22
image::rancher_veeam-kasten_K10_Dashboard-22.png[scaledwidth="85%", align="center"]

For policies that need to span all applications, you can select all applications with a * wildcard.

Selecting No Applications

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-23
image::rancher_veeam-kasten_K10_Dashboard-23.png[scaledwidth="85%", align="center"]

For policies that protect only cluster-scoped resources and do not target any applications, you can select "None". For more information about protecting cluster-scoped resources, see https://docs.kasten.io/latest/usage/clusterscoped.html#clusterscoped [Cluster-Scoped Resources.]

Selecting By Labels

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-24
image::rancher_veeam-kasten_K10_Dashboard-24.png[scaledwidth="85%", align="center"]

For policies that need to span multiple applications (e.g., protect all applications that use MongoDB or applications that have been annotated with the gold label), you can select applications by label. Any application (namespace) that has a matching label as defined in the policy will be selected. Matching occurs on labels applied to namespaces, deployments, and statefulsets. If multiple labels are selected, a union (logical OR) will be performed when deciding to which applications the policy will be applied. All applications with at least one matching label will be selected.

Note that label-based selection can be used to create forward-looking policies as the policy will automatically apply to any future application that has the matching label. For example, using the heritage: Tiller (Helm v2) or heritage: Helm (Helm v3) selector will apply the policy you are creating to any new Helm-deployed applications as the Helm package manager automatically adds that label to any Kubernetes workload it creates.

Working With Policies

Once you have created a policy and have navigated back to the main dashboard, you will see the selected applications quickly switch from unmanaged to non-compliant (i.e., a policy covers the objects but no action has been taken yet). They will switch to compliant as snapshots and backups are run as scheduled or manually and the application enters a protected state. You can also scroll down on the page to see the activity, how long each snapshot took, and the generated artifacts. Your page will now look similar to this:

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-25
image::rancher_veeam-kasten_K10_Dashboard-25.png[scaledwidth="85%", align="center"]

More detailed job information can be obtained by clicking on the in-progress or completed jobs.

Manual Policy Runs

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-26
image::rancher_veeam-kasten_K10_Dashboard-26.png[scaledwidth="85%", align="center"]

It is possible to manually create a policy run by going to the policy page and clicking the run once button on the desired policy. Note that any artifacts created by this action will not be eligible for automatic retirement and will need to be manually cleaned up.


Restoring Existing Applications

Restoring an application is accomplished via the Applications page. One needs to simply click the Restore icon. 

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-27
image::rancher_veeam-kasten_K10_Dashboard-27.png[scaledwidth="85%", align="center"]

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-28
image::rancher_veeam-kasten_K10_Dashboard-28.png[scaledwidth="85%", align="center"]

While the UI uses the Export term for backups, no Import policy is needed to restore from a backup. Import policies are only needed when you want to restore the application into a different cluster.

At that point, one has the option to pick a restore point, a grouped collection of data artifacts belonging to the application, to restore from. As seen above, this view distinguishes manually generated restore points from automated policy-generated ones.

It also distinguishes between snapshots and backups. When both are present, as seen above, a layered box is shown to indicate more than one kind of restore point is present for the same data. If you want to restore a version of the application stack, clicking on the layered restore point will present the below option to select between the local snapshot and exported backup.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-29
image::rancher_veeam-kasten_K10_Dashboard-29.png[scaledwidth="85%", align="center"]

Selecting a restore point will bring up a side-panel containing more details on the restore point for you to preview, if needed, before you initiate an application restore.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-30
image::rancher_veeam-kasten_K10_Dashboard-30.png[scaledwidth="85%", align="center"]

Note

Restored PersistentVolumes may not have the annotations specified in the original PersistentVolume.

After the restore completes, you will be able to go back to your application and verify that the state was restored to what existed at the time the restore point was obtained.

Restoring Deleted Applications

The process of restoring a deleted application is nearly identical to the above process. The only difference is that, by default, removed applications are not shown on the Applications page. To discover them, you simply need to filter and select Removed.

//kubernetes_gs_rancher_veeam-kasten_image_K10_Dashboard-31
image::rancher_veeam-kasten_K10_Dashboard-31.png[scaledwidth="85%", align="center"]

Once the filter is in effect, you will see applications that K10 has previously protected but no longer exist. These can now be restored using the normal restore workflow.

== Use case and demonstration

//Describe use case and demonstration.

//[NOTE]
//====
// Use admonitions (i.e., NOTE, TIP, IMPORTANT, CAUTION, WARNING) as needed.
//====


//Use sections to break up major actions, such as:
//=== Prepare the demonstration environment (beyond installation)
//=== Perform action (to illustrate capability)
//=== Examine results (verifying capability)
//Use ordered lists for steps.
//Minimize/consolidate images whenever possible.



== Summary

//Summarize the motivation
//Summarize what was demonstrated
//Hint at other capabilities.



== Additional resources

//Learn more about the capabilities of {title} with these additional references.
//Use an unordred lists for references
//* https://url[reference-title]
//* https://url/[reference-title]


++++
<?pdfpagebreak?>
++++

// Standard SUSE Technical Reference Documents includes

:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
