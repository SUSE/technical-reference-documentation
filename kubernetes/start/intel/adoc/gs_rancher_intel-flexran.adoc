:docinfo:
include::./common_docinfo_vars.adoc[]

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// General comments
// Keep in mind that this is a "getting started" guide and the
//   audience that you are trying to reach.
// Leverage ASCIIDoc features to make this document readable and usable:
//   - Text highlights (follow SUSE style guides)
//   - Admonitions (i.e., NOTE, TIP, IMPORTANT, CAUTION, WARNING)
//   - Code blocks
//   - Lists (ordered and unordered, as appropriate)
//   - Links (to other resources)
//   - Images
//     - Place image files under the ./media directory tree
//       (e.g., ./media/src/svg, ./media/src/png)
//     - Format preference: svg > png > jpg
//     - Consolidate images wherever possible
//       (i.e., don't use two images when one conveys the message)
//   - Use sections and subsections to organize and group related
//     steps.
// 
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Variables & Attributes
//
// NOTES:
// 1. Update variables below and adjust docbook file accordingly.
// 2. Comment out any variables/attributes not used.
// 3. Follow the pattern to include additional variables.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// organization - do NOT modify
// -
:trd: Technical Reference Documentation
:type: Getting Started
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// document and product
// -
:title: Intel® FlexRAN™ - SUSE Reference Solution Cloud-Native Setup
:subtitle: FlexRAN™ Deployment Guide on SUSE platform
:productname:
//  SLES 15 SP3 RT, SLE Micro 5.2 RT, RANCHER 2.6, RKE2
:platform1: SUSE Linux Enterprise Server 15.3 Real Time
:platform1short: SLES 15 RT
:platform2: SUSE Linux Enterprise Micro 5.2 Real Time
:platform2short: SLE Micro 5.2 RT
:platform3: Rancher 2.6 by SUSE
:otherproduct1: Intel® FlexRAN™ 22.07
:otherproduct1short: Intel® FlexRAN™
:k8s1: Rancher Kubernetes Engine 2
:k8s1short: RKE2
:k8s2short: K3s
:usecase: SUSE/Intel Solution for Telco
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// contributors
// specify information about authors, editors, and others here,
// then update docinfo file as appropriate
// -
:author1_firstname: Alex
:author1_surname: Zacharow
:author1_jobtitle: ISV Certification Engineer
:author1_orgname: SUSE
:author2_firstname: Jose
:author2_surname: Betancourt
:author2_jobtitle: Director, Solution Partners & Alliances
:author2_orgname: SUSE
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// define any additional variables here for use within the document
// -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


= {title}: {subtitle}



== Introduction


=== Motivation

Intel® FlexRAN™ is a reference implementation for cloud enabled wireless access VNFs.  It shows how to efficiently implement wireless access loads through flexible software architecture, Intel® Xeon® Scalable processors using Intel® Advanced Vector Extensions 512 (Intel® AVX 512) instruction set, and optimized NFVi with DPDK.

Executing said implementation is a multi-step process, starting with the Operating System and extensions running on the right hardware, installing Intel® tools, and finally, containerizing and deploying FlexRAN™ in a cloud-native, software-defined infrastructure.

This guide will help you configure and deploy an Intel® FlexRAN™ implementation using SUSE products such as Rancher Server, SUSE Linux Enterprise Server (with Real Time extensions) and SUSE Linux Enterprise Micro Real Time to create and manage Rancher Kubernetes Engine v2 (RKE2) clusters for the containerized implementation.
 

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a motivation for the document in 2-4 sentences to identify:
//   - what the document is about
//   - why it may be of interest to the reader (e.g., a use case)
//   - what products are being highlighted
// Include an approved SUSE | Partner logo lock-up if possible
// Include any additional, relevant details
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Specify what this guide covers in no more than 2 sentences.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

=== Scope
This document provides the necessary steps to setup a cloud-native stack for Intel® FlexRAN™ on SUSE Rancher Kubernetes cluster.
This demonstration shows the required steps to install and configure Intel's FlexRAN PHY Reference Design using SUSE Linux Enterprise Server Real Time as the base Operating System and SUSE Linux Enterprise Micro Real Time as target hosts for Rancher RKE2 or K3s Kubernetes cluster with SUSE Rancher Server to manage this deployment.

=== Audience

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify for whom this document is intended, perhaps with:
//   - topics of interests (e.g., machine learning, security, etc.)
//   - job roles (e.g., developer, administrator, platform architect, etc.)
//   - required skills
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

The goal is to empower Telco developers and/or engineers putting together an Intel® FlexRAN™ test and/or proof-of-concept (PoC) environment leveraging the benefits of the complete SUSE stack from the real-time OS to Kubernetes orchestration and management.

== Prerequisites
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify minimum requirements (prerequisites) the reader
// would need in order to follow the steps of this guide.
// - Link to existing resources whenever possible.
// - Keep this section brief but elaborate as needed.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

This section presents some guidance for the environment into which you deploy {otherproduct1short}.

=== Hardware

For the FlexRAN™ configuration on the physical development node the following hardware components were used:

[cols="1,1"]
|===
|Component |Specification

|Processor
|Intel® Xeon® Silver 4316 @ 2.30Ghz

|Memory
|128 GB RAM

|Network
|Intel® vRAN ACC100-based accelerator

Intel® E810 100Gb Ethernet controller

|Storage
|480GB SSD SATA Read Intensive 6Gbps
960GB Data Center NVMe
|===

// image::eth1.png[networking components, scaledwidth="75%", align="center"]

[NOTE]
====
For more details on server components, see Intel® FlexRAN™ reference documentation: _Installation Guide Software Release v22.07 (Doc. No.: 575834-15.0)_ and _FlexRAN™ 5GNR Reference Solution 22.07 - PHY Software Documentation (Doc. No.: 603577)_.
====

//image::lscpu.png[CPU, scaledwidth="65%", align="center"]

=== BIOS Configuration

A server's system BIOS provides runtime services for operating systems and performs hardware initialization during the booting process.

BIOS settings can influence how hardware behaves under different workloads.

Among the most important BIOS settings for implementing {otherproduct1short} are the CPU p-states (optimization of the voltage and CPU fequency during operation) and c-states (optimization of the power consumption if a core does not have to execute any instructions).

BIOS configuration may be different for each server, but most modern servers should have similar settings.


//For example:

image::bios1.png[bios1,scaledwidth="75%", align="center"]

For CPU power management, use _OS DBPM_ or a similar control setting to allow the operating system to manipulate processor frequencies.

Depends on the BIOS version, it's also recommended to use a _Custom_ profile with a _Telco Optimized_ or _Maximum Performance_ profile BIOS settings.

// Other BIOS settings used for the development of this guide are illustrated here.

// image::bios2.png[bios2,scaledwidth="75%", align="center"]

// image::bios3.png[bios3,scaledwidth="75%", align="center"]

// image::bios4.png[bios4,scaledwidth="75%", align="center"]

[NOTE]
====
For more details, please review section 2.4.2 of _FlexRAN Software Reference Solution Cloud-Native Setup. (Intel® Doc. No. 575834-15.0)_ and _BIOS Settings for FlexRAN Platforms Based on Intel® Xeon® Processors. (Doc. No.: 640685)_.
====

=== OS requirements

Intel® FlexRAN™ stipulates a real-time kernel, as listed in _Intel® FlexRAN™  Installation Guide (Doc. No.: 575834-15.0)_.

For this particular test we used a baremetal node as a development host running SUSE Linux Enterprise 15 SP3 Real-Time to preconfigure and test FlexRAN functionality and build container image before exporting it to a Rancher Kubernetes cluster.

.Test Setup Diagram
image::setup-diagram.png["Setup diagram", scaledwidth="75%", align="center"]


SUSE Linux Enterprise Real Time is a real time operating system designed to reduce latency and increase the predictability and reliability of time-sensitive, business-critical applications.

For more details about SLES RT please review https://www.suse.com/products/realtime/

==== Install SUSE Linux Enterprise Server 15 Real Time


Please refer to the SLES Setup Guide: https://documentation.suse.com/sle-rt/15-SP3/ 

When installing SLES 15 RT, during the installation make sure to unmark _kernel-default_.

image::slesRT-install1.png[sle RT installation, scaledwidth="75%", align="center"]

Verify _kernel-rt_ is selected:

image::slesRT-install2.png[sle RT installation 2, scaledwidth="75%", align="center"]


[NOTE]
====
 During the installation, add sufficient space to the /opt or /var directories which will be used for most Intel components and containers. We would recommend 200 Gb of storage for these directories. Don’t install FlexRAN under root directory.
====

==== Real Time configuration


Isolate CPU cores with the following steps:


- Verify that _tuned_ installed:
[source, bash]
----
zypper in tuned*
----

image::tuned1.png[tuned1,scaledwidth="65%", align="center"]

In our case we have 1 socket and 40 cores.


- Add isolated cores to the configuration

[source, bash]
----
vi /etc/tuned/cpu-partitioning-variables.conf
----

image::tuned2.png[tuned2,scaledwidth="65%", align="center"]
//
- Activate RT profile
//
[source, bash]
----
tuned-adm profile cpu-partitioning
----

 For UEFI modify /boot/efi/EFI/sle_rt/grub.cfg
 as following:
//

[source, bash]
----
set tuned_params="skew_tick=1 nohz=on nohz_full=2-39 rcu_nocbs=2-39 nosoftlockup isolcpus=2-39"
----
//
image::grub1.png[grub1,scaledwidth="75%", align="center"]

[NOTE]
====
 Settings depend on the number of CPU and isolated cores.
 Please review section 2.4.3 of Intel’s document 575834-15.0
====
//
- Save changes
[source, bash]
----
grub2-mkconfig -o /boot/grub2/grub.cfg
----
or for UEFI:
[source, bash]
----
grub2-mkconfig -o /boot/efi/EFI/sle_rt/grub.cfg
----
//
- Reboot server and verify parameters:
//
[source, bash]
----
grep tuned_params= /boot/grub2/grub.cfg
----
image::grub2.png[scaledwidth="75%", align="center"]

[source, bash]
----
cat /proc/cmdline
----
image::grub3.png[scaledwidth="75%", align="center"]

=== Set CPU Frequency

AVX512 CPU frequency of your specific CPU should be adjusted according to Figure 4 of Intel’s doc Reference Number: 637779, Revision: 1.2 3rd Gen Intel® Xeon® Scalable Processors, Codename Ice Lake NDA Specification Update June 2021
 or #613537 for Skylake processor family

image::cpu-avx.png[scaledwidth="75%", align="center"]

In this deployment Intel® Xeon® 4316 was used with 2.6 GHz.

There are two options to setup your CPU frequency: :::

* **Use cpupower tool**

By running
[source, bash]
----
 cpupower frequency-info
----
you can check available frequencies for your CPU and drivers.

image::frequency1.png[scaledwidth="65%", align="center"]

In this example intel_cpufreq was used.
The userspace governor is available with the older acpi-cpufreq driver (which will be automatically used if you disable intel_pstate at boot time; you then set the governor/frequency with cpupower)

Set intel_pstate driver to passive in grub (intel_pstate=passive):

[source, bash]
----
echo passive | sudo tee /sys/devices/system/cpu/intel_pstate/status
----

or add intel_pstate=passive to the grub:

[source, bash]
----
modprobe cpufreq_userspace
----

Set cpu governor to userspace:
[source, bash]
----
cpupower frequency-set --governor userspace
----
Set frequency according to the AVX-512 table (2600MHz in this case):
[source, bash]
----
cpupower --cpu all frequency-set --freq 2600MHz
----

[NOTE]
====
 It’s important to set C-state and P-state on the Bios settings as well as on the kernel side. If you don't do this, you won't be able to change governors from the cpupower command and set the cpu frequency. Also, make sure that the BIOS can be changed from the OS by proper setting.
====

Verify that settings applied by running:
[source, bash]
----
turbostat -i 1
----

image::turbostat1.png[scaledwidth="75%", align="center"]

You can also check with other available tools:

image::mperf.png[scaledwidth="50%", align="center"]

image::cpuinfo.png[scaledwidth="50%", align="center"]

* **The second option to change AVX512 frequency is to install Intel® msr-tools with the following commands:**

[source, bash]
----
git clone https://github.com/intel/msr-tools/
cd msr-tools/
git checkout msr-tools-1.3
make
modprobe msr
----


Create bash script setFreq.sh with the following context:
[source, bash]
----

#!/bin/bash 

cpupower frequency-set -g performance 

for i in {0..39} 

do

/home/Intel/msr-tools/msr-tools/wrmsr -p $i 0x199 0x1A00

done 

#Set Uncore max frequency

/home/Intel/msr-tools/msr-tools/wrmsr -p 0 0x606A6 0x1A00 

/home/Intel/msr-tools/msr-tools/wrmsr -p 39 0x606A6 0x1A00
----


[NOTE]
====
 Values in the script were taking from Intel document #637779 (for Ice Lake family)  specific to your CPU avx512 numbers. (2.6 GHz in the above example)
====

image::avx2.png[scaledwidth="75%", align="center"]

Run the above bash script with your specific numbers which should be changed to the required frequency and verify that required frequency was applied.

Review performance with a Cyclictest:

image::cyclic1.png[scaledwidth="75%", align="center"]

For more details review [SLE RT Hardware Testing] https://documentation.suse.com/sle-rt/15-SP3/pdf/article-hardware-testing_color_en.pdf



=== Install Intel® oneAPI

* Install Intel GPU drivers

That step will eliminate any prerequisites failures for oneAPI.

Review https://dgpu-docs.intel.com/installation-guides/suse/suse-15sp3.html for more details.

[source, bash]
----
zypper addrepo -r https://repositories.intel.com/graphics/sles/15sp3/intel-graphics.repo

zypper install   intel-opencl   intel-media-driver libmfx1   intel-level-zero-gpu level-zero
----

* Download and install Intel® oneAPI

[source, bash]
----
wget https://registrationcenter-download.intel.com/akdlm/irc_nas/18236/l_BaseKit_p_2021.4.0.3422_offline.sh

bash l_BaseKit_p_2021.4.0.3422_offline.sh
----

[NOTE]
====
 Make sure that the installation directory has enough space. Intel® oneAPI utilizes approximately 40Gb of space.
====
image::oneapi1.png[scaledwidth="60%", align="center"]

image::oneapi2.png[scaledwidth="60%", align="center"]

Source the environment and verify installed version:

image::oneapi3.png[scaledwidth="60%", align="center"]

Make sure that GCC is installed to work with ICX compiler:

image::gcc.png[scaledwidth="60%", align="center"]


// == Technical overview

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a technical overview of the solution.
// - Identify components.
// - Describe how the components fit together.
//   Leverage diagrams as appropriate, including (but not limited to):
//   - component architecture
//   - data flow diagram
//   - workflow diagram
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =





== Intel® FlexRAN™ Installation

Review Compilation tools section of FlexRAN 5GNR Reference Solution 22.07 PHY Software Documentation - Document #603577

Make sure that your instance has installed cmake, meson and ninja.

In order to build the L1 application and L1 standalone Test Application, the following steps are required (in order):



=== Install pkgconf tool

[source, bash]
----
zypper in automake
zypper in libtool
git clone https://github.com/pkgconf/pkgconf.git
cd pkgconf/
./configure
make
make install
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib
----

image::pkgconf.png[scaledwidth="75%", align="center"]


=== Download and Install DPDK

[NOTE]
====
 Don’t use /root directory for the installation.
You need to get the dpdk patch from Intel which is required.
====
[source, bash]
----
wget http://static.dpdk.org/rel/dpdk-21.11.tar.xz
tar xf dpdk-21.11.tar.xz
export RTE_SDK=/var/dpdk/dpdk-21.11
----

Copy patch to RTE_SDK directory and apply dpdk patch:
[source, bash]
----
patch -p1 < dpdk_patch_21.11.patch
----

=== Download and install FlexRAN™

Download FlexRAN™ release as per Intel document 645964.

Extract file and source the environment:
[source, bash]
----
tar -zxvf FlexRan-22.07.tar.gz
./extract.sh
export RTE_SDK=/var/dpdk/dpdk-21.11
source ./set_env_var.sh
----

image::source-flexran.png[scaledwidth="60%", align="center"]

=== Compile SDK

Get _gcc11-c++_:
[source, bash]
----
zypper in gcc11-c++
----

Export PKG_CONFIG_PATH:
[source,bash]
----
export PKG_CONFIG_PATH=$DIR_WIRELESS_SDK/pkgcfg:$PKG_CONFIG_PATH
----

Source oneAPI:
[source, bash]
----
source /opt/intel/oneapi/setvars.sh --force

export PATH=/opt/intel/oneapi/compiler/2022.0.2/linux/bin-llvm/:$PATH
----


Review possible compilation options from _./flexran_build.sh -h_ command:

image::flexran-options.png[scaledwidth="65%", align="center"]

Compile SDK:
[source, bash]
----
./flexran_build.sh -x icx -e -r 5gnr -i avx512 -m sdk
----

[NOTE]
====
 The FlexRAN SDK libraries must be built first to the provided path before starting the DPDK build process so that software FEC libraries are present.
====
=== Patch and Compile DPDK

[source, bash]
----
zypper in python3-pyelftools.rpm
----

Create dpdk script:
[source, bash]
----
vi dpdk-dep.sh
----

[source, bash]
----
#! /bin/bash
work_path=$PWD
sdk_path= /var/FlexRan22.07/sdk
echo "------------build base dpdk -------------------"
cd $RTE_SDK; meson build; cd build; meson configure -Dflexran_sdk=$sdk_path/build-avx512-icx/install; ninja
----

Run dpdk script:
[source, bash]
----
./dpdk-dep.sh
----

Create dpdk-kmods:
[source, bash]
----
git clone http://dpdk.org/git/dpdk-kmods
cd dpdk-kmods/linux/igb_uio/
make
modprobe uio
insmod $RTE_SDK_KMOD/linux/igb_uio/igb_uio.ko
export RTE_SDK_KMOD=/var/dpdk/dpdk-kmods
----

=== Build the L1 Application, L1 Standalone Test Application, and Test MAC in Linux:

Verify that you have numa*, libhuge* and libnuma-dev* installed.

Mount hugepages:
[source, bash]
----
mount -t hugetlbfs nodev /mnt/huge
----

Compile 5gnr:
[source, bash]
----
./flexran_build.sh -x icx -e -r 5gnr
----

Compile lte:
[source, bash]
----
./flexran_build.sh -x icx -e -r lte -i avx512
----

After following above steps, upon a successful build, a new L1 application file <install_dir>/bin/nr5g/gnb/l1 will be created.
L1 standalone Test Application will be created in <install_dir>/tests/nr5g/nr5g_testapp

* For ACC100 acceleration

Verify acc card:

[source, bash]
----
lspci | grep acc 
51:00.0 Processing accelerators: Intel Corporation Device 0d5c
----

image::dev-bind.png[scaledwidth="75%", align="center"]

When using Mount Bryce (ACC100) acceleration follow https://github.com/intel/pf-bb-config

[source, bash]
----
git clone https://github.com/intel/pf-bb-config 
cd pf-bb-config/  
make  
----
//
`For Physical Function (PF) option:`

Bind the PF with the igb_uio module (or alternatively with pci-pf-stub):
[source, bash]
----
/var/dpdk/dpdk-21.11/usertools/dpdk-devbind.py --bind=igb_uio 51:00.0
----

Configure the devices using the pf_bb_config application:
[source, bash]
----
/var/dpdk/dpdk-21.11/usertools/dpdk-devbind.py --bind=igb_uio 52:00.0 52:00.1
----

image::pf-bb1.png[scaledwidth="75%", align="center"]
//
`For Virtual Function (VF) option:`

Create 2 VFs from the PF:

image::max-vf.png[scaledwidth="65%", align="center"]

Check available interfaces:
[source, bash]
----
/opt/dpdk/dpdk-stable-20.11.3/usertools/dpdk-devbind.py -s
----

image::dev-bind2.png[scaledwidth="75%", align="center"]

In the above example there are 2 VFs created.

Bind with VF:
[source, bash]
----
/var/dpdk/dpdk-21.11/usertools/dpdk-devbind.py --bind=igb_uio 52:00.0 52:00.1
----

Configure the devices using the pf_bb_config application for VF usage with both 5G and 4G enabled.

Select the proper config file for your test for VF:
[source, bash]
----
./pf_bb_config ACC100 -c acc100/acc100_config_2vf_4g5g.cfg
----
Check available interfaces and verify number of acc:

image::acc2.png[scaledwidth="75%", align="center"]


Test that the VF is functional on the device using bbdev-test:
[source, bash]
----
/var/dpdk/dpdk-21.11/app/test-bbdev # /var/dpdk/dpdk-21.11/build/app/dpdk-test-bbdev -c F0 -a 52:00.0 -- -c validation -v ./ldpc_dec_default.data
----

image::bbdev-test.png[scaledwidth="65%", align="center"]

== Baremetal Host Testing

=== FlexRAN L1 and Testmac test

Follow steps from the TestMac section of FlexRAN 5GNR Reference Solution 22.07 PHY Software Documentation - Intel's Document #603577

image::testmac1.png[scaledwidth="75%", align="center"]

[NOTE]
====
Always source FlexRAN environment and the oneAPI in each tab and make sure that all paths are exported.
For simplicity create a script to source all paths every time when running tests in each terminal.
====

Change dpdkBasebandDevice values from phycfg_timer.xml to either physical or virtual acc:

[source, bash]
----
/opt/FlexRan/bin/nr5g/gnb/l1 # vi phycfg_timer.xml
----
For example:

image::timer1.png[scaledwidth="75%", align="center"]

Where FecMode is set to 1 (HW accelertor) and 0000:52:00.0 is the VF value from acc. 
//
Set dpdkBasebandFecMode to VF value according to your specific card.

From terminal 1 run:
[source, bash]
----
./FlexRAN-<version>/bin/nr5g/gnb/l1/l1.sh -e
----

You should be able to see the following console:

image::console1.png[scaledwidth="75%", align="center"]

From the 2nd terminal run:
[source, bash]
----
/var/FlexRan22.07/bin/nr5g/gnb/testmac # ./l2.sh

run 1 1 3 100 1005
----

image::console2.png[scaledwidth="65%", align="center"]

See examples from Document #603577 TestMac section:

image::testmac-2.png[scaledwidth="65%", align="center"]

The connection should be established in the 1st terminal once you’ll run l2.sh from the 2nd terminal:

image::console2-2.png[scaledwidth="60%", align="center"]

In the 2nd terminal you should be able to see test result:

image::console2-3.png[scaledwidth="60%", align="center"]

Another test case is to use a preconfigured test file.

For example from the 2nd terminal run:
[source, bash]
----
/var/FlexRan22.07/bin/nr5g/gnb/testmac # ./l2.sh --testfile=/var/FlexRan22.07/bin/nr5g/gnb/testmac/icelake-sp/icxsp_mu1_100mhx_4x4_hton.cfg
----
image::console2-4.png[scaledwidth="60%", align="center"]
image::console2-5.png[scaledwidth="60%", align="center"]
image::console2-6.png[scaledwidth="60%", align="center"]

[NOTE]
====
Number of failed tests listed above related to a different number of CPU cores defined in the test file.
====

=== CPU set shielding

Another tool for more tuned cores isolation is cpu set shielding.

You can also review CPU manipulation commands from  -> https://documentation.suse.com/sle-rt/15-SP3/pdf/book-shielding_color_en.pdf

Some examples of using shielding on CPU with integrated tools like cset.

* Create a cset called flexran_set

image::cset1.png[scaledwidth="60%", align="center"]

Example of moving _top_ command from root set to flexran_set:

image::cset2.png[scaledwidth="60%", align="center"]

When starting a testmac you can move pid to a dedicated cset:

image::cset3.png[scaledwidth="60%", align="center"]
image::cset4.png[scaledwidth="60%", align="center"]

 To move all siblings from pid use _-–threads_ option:   
[source, bash]
----
cset proc -m -p 16165 --threads -t two
----


For all features of CPU manipulations please review shielding tasks documents for CPU isolations:

    • https://www.suse.com/c/cpu-isolation-introduction-part-1/

    • https://documentation.suse.com/sle-rt/15-SP3/pdf/book-shielding_color_en.pdf

    • https://documentation.suse.com/sle-rt/15-SP3/pdf/article-virtualization_color_en.pdf#%5B%7B%22num%22%3A30%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C63.779%2C788.031%2Cnull%5D


To run Testmac with VF set, change setting to proper VF value and configuration

image::vf1.png[scaledwidth="65%", align="center"]

and from the 1st terminal run:
[source, bash]
----
./l1.sh -e
----

From the 2nd terminal run:
[source, bash]
----
run 1 1 3 100 1005
----

image::testmac-3.png[scaledwidth="65%", align="center"]

image::cset5.png[scaledwidth="65%", align="center"]

image::cset6.png[scaledwidth="65%", align="center"]

If using a config file, from the 2nd terminal run:
[source, bash]
----
./l2.sh --testfile=/var/FlexRan22.07/bin/nr5g/gnb/testmac/icelake-sp/icxsp_mu1_100mhz_mmimo_32x32_hton.cfg
----

If using a _taskset_, from terminal 1 run:
[source, bash]
----
~/gnb/l1 # taskset -c 12-19 ./l1.sh -e
----

and from terminal 2 run:
[source, bash]
----
~/gnb/testmac # taskset -c 12-19 ./l2.sh
----


== Deploy FlexRAN™ on Container through Kubernetes

=== Generate LTE/5G Docker Images with pre-build FlexRAN™

All prerequisite components and FlexRAN™ should be installed as descrtibe in the previous sections.

The main document to follow: FlexRAN Reference Solution Cloud-Native Setup (Intel Document Number: 575834-15.0)

Use existing FlexRAN directory or create a FlexRAN pre-configured directory which will be used for the container image.

Source all environment variables:
[source, bash]
----
export RTE_SDK=/var/dpdk/dpdk-21.11 
source /opt/intel/oneapi/setvars.sh 
export PKG_CONFIG_PATH=$DIR_WIRELESS_SDK/pkgcfg:$PKG_CONFIG_PATH 
source set_env_var.sh
----

==== Create a Dockerfile

If you want to deploy a SUSE Linux Enterprise-based container to deploy to the cluster in the future, follow the steps below.

- Modify _flexran_build_dockerfile.sh_ from the flexran directory:

image::docker4.png[scaledwidth="75%", align="center"]

[NOTE]
====
Modify according to your local setup. If local RMT server is used, you need to post rmt-server.crt file on your RMT server in the location which can be reachable from url. So, on the local RMT server copy _/etc/rmt/ssl/rmt-server.crt_ file to the _/usr/share/rmt/public/repo_ directory, which creates symb link to _./var/lib/rmt/public/repo_ which is a public repo of RMT server. 
Setup a proper permission to _/usr/share/rmt/public/repo_ directory. 
Sync rmt server.
====

- Build a docker image:

[source, bash]
----
./flexran_build_dockerfile_suse.sh -v -e avx512 -r 5gnr -m all –x icx
----

image::docker-built.png[scaledwidth="65%", align="center"]

- Tag a docker image:
[source, bash]
----
docker tag flexran.docker.registry/flexran_vdu:latest flexran.docker.registry/flexran_vdu:22.07
----

image::docker-image.png[scaledwidth="65%", align="center"]

[NOTE]
====
Another alternative and recommended tool to use is podman since it’s daemonless and has integration with cockpit web console on Sle Micro.
For that you need to replace _docker build_ command with _podman_ in flexran_build_dockerfile.sh file and run:
[source, bash]
----
podman build -t
----
For more details review the Podman guide: https://documentation.suse.com/sle-micro/5.1/pdf/article-podman_color_en.pdf
====
- Prepare file to export to the target node and save docker as: 
[source, bash]
----
docker save flexran.docker.registry/flexran_vdu:22.07|gzip > flexranimage.tar.gz
----


=== Create an RKE2 cluster

==== Install SUSE Linux Enterprise Micro

In this test deployment, SUSE Linux Enterprise Micro 5.2 (SLE Micro) was used as a server host for the Rancher server test deployment.

SUSE Linux Enterprise Micro is a lightweight and secure OS platform purpose built for containerized and virtualized workloads.

For more details on installation of SLE Micro, review: https://documentation.suse.com/sle-micro/5.2/pdf/book-deployment-slemicro_color_en.pdf


After installing a SLE Micro you can enable a cockpit console for easy management:
[source, bash]
----
systemctl enable --now cockpit.socket
----
and open console in the browser as 'https://your-ip:9090/'

image::rancher-console.png[scaledwidth="75%", align="center"]
//

For more details review:

https://documentation.suse.com/sle-micro/5.2/

==== Install a Rancher server

`Install K3s:`
[source, bash]
----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION="v1.23.9+k3s1" INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_EXEC='server --cluster-init --write-kubeconfig-mode=644' sh -s -
----

`Install certificates and verify:`
[source, bash]
----
kubectl apply --validate=false -f https://github.com/cert-manager/cert-manager/releases/download/v1.7.1/cert-manager.crds.yaml 
 helm repo add jetstack https://charts.jetstack.io 
 helm repo update 
 export KUBECONFIG=/etc/rancher/k3s/k3s.yaml 
helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.7.1 
----

[source, bash]
----
kubectl get pods --namespace cert-manager
----

image::get-pods.png[scaledwidth="60%", align="center"]

`Install Rancher:`
[source, bash]
----
helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
kubectl create namespace cattle-system
export HOSTNAME="rancher-server2.isv.suse"
export RANCHER_VERSION="2.6.5"
helm install rancher rancher-stable/rancher --namespace cattle-system --set hostname=rancher-server2.isv.suse --set version=2.6.5 --set replicas=1
----

Go to Rancher url and login.

For more details on Rancher installation, review > https://documentation.suse.com/trd/kubernetes/pdf/kubernetes_ri_rancher-k3s-slemicro_color_en.pdf

==== Create a custom RKE2 cluster

- From the Rancher server create a custom cluster > switch to rke2

image::cluster-1.png[scaledwidth="75%", align="center"]

- Copy registration script to a new node to add it to the cluster:

image::cluster-2.png[scaledwidth="75%", align="center"]

- Verify if machines got provisioned:

image::cluster-2.png[scaledwidth="75%", align="center"]

[source, bash]
----
kubectl get nodes  
NAME     STATUS   ROLES                              AGE     VERSION  
xr12-a   Ready    control-plane,etcd,master,worker   7d21h   v1.23.10+rke2r1  
xr12-c   Ready    control-plane,etcd,master,worker   7d20h   v1.23.10+rke2r1
----


In this test case 2 Dell XR12 nodes were used with Sle Micro 5.2 RT installed as part of the RKE2 cluster. Both target nodes should have dpdk with a patch and Intel oneAPI installed.
 
For core isolation on Sle Micro RT, install _tuned_ package with additional dependencies. 
[source, bash]
----
transactional-update pkg install tuned.rpm python3-configobj.rpm python3-linux-procfs.rpm python3-pyudev.rpm virt-what.rpm
----
[NOTE]
====
For this test, SLES 15 repositories were used with _curl_ commands to download packages locally. For a large scale deployment a local repository can be made with required RPMs.
====

Modify _/etc/default/grub_ to the required tuned parameters with isolcpu and run transactional-update grub.cfg to save changes and reboot. 

[NOTE]
====
When setting up CPU Manager for Kubernetes* (CMK*) it should be based on isolcpu settings in GRUB.
Make sure that all required plugins for Kubernetes for your test are installed on tested nodes as described in section 4 of Intel’s document 575834-15.0
====

[NOTE]
====
It's not recommended to add a FlexRAN™ development node to the RKE2 cluster. Instead, move image to the FlexRAN™ RKE2 cluster, either manually or with a repo.
====

During our RKE2 cluster deployment, Rancher provides an option to select Multus and Calico as default plugins, so no needs to install them manually.

=== Build SR-IOV Network Device Plugin
The setup details for virtual or physical functions of the SR-IOV Network Device Plugins can be found at: https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin

[source, bash]
----
cd /root/go/src/github.com/intel/
~/go/src/github.com/intel # git clone https://github.com/intel/sriov-network-device-plugin 
cd sriov-network-device-plugin/
git checkout v3.5.1
mkdir bin
cp ~/go/bin/golint bin/
~/go/src/github.com/intel/sriov-network-device-plugin # make
make image
----

Tag with:
[source, bash]
----
docker tag ghcr.io/k8snetworkplumbingwg/sriov-network-device-plugin:latest nfvpe/sriov-device-plugin:v3.5
----

image::docker-tag.png[scaledwidth="75%", align="center"]

Save with:
[source, bash]
----
docker save nfvpe/sriov-device-plugin:v3.5|gzip > sriov-device-plugin.tar.gz
----


=== Create FlexRAN™ Pods

Label nodes as:
[source, bash]
----
kubectl label nodes xr12-b testnode=worker1
----

image::show-labels.png[scaledwidth="75%", align="center"]

* Configure FEC and FVL SRIOV

To reconfigure pf_bb_config run:
[source, bash]
----
pkill pf_bb_config
modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
insmod /var/dpdk/dpdk-kmods/linux/igb_uio/igb_uio.ko
/var/dpdk/dpdk-21.11/usertools/dpdk-devbind.py -b igb_uio 18:00.0
----
where 18:00.0 is acc pf address


Check available accelerator cards:
[source, bash]
----
lspci|grep acc  
18:00.0 Processing accelerators: Intel Corporation Device 0d5c
----
Add 4 VFs to acc:
[source, bash]
----
echo 4 > /sys/bus/pci/devices/0000:18:00.0/max_vfs
----

Verify:

image::dev-bind4.png[scaledwidth="75%", align="center"]
In the below example 4 new were created:
[source, bash]
----
lspci|grep acc  
18:00.0 Processing accelerators: Intel Corporation Device 0d5c  
19:00.0 Processing accelerators: Intel Corporation Device 0d5d  
19:00.1 Processing accelerators: Intel Corporation Device 0d5d  
19:00.2 Processing accelerators: Intel Corporation Device 0d5d  
19:00.3 Processing accelerators: Intel Corporation Device 0d5d
----

image::dev-bind5.png[scaledwidth="65%", align="center"]

image::dev-bind6.png[scaledwidth="65%", align="center"]

Modify configMap as following:
[source, bash]
----
vi ~/go/src/github.com/intel/sriov-network-device-plugin/deployments
----

image::configmap1.png[scaledwidth="55%", align="center"]

[source, bash]
----
kubectl create -f configMap.yaml
----

Modify /var/flexran/build/docker/flexran_testmac_mode.yaml according to your specs:

[source, bash]
----
kubectl create -f flexran_testmac_mode.yaml
----

image::pods-test.png[scaledwidth="60%", align="center"]


// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Detail the steps of the installation procedure.
// The reader should be able to copy and paste commands to
// a local environment or follow along locally with screenshots.
// Include one or more verification steps to validate installation.
//
// Leverage:
// - Ordered lists
// - Code blocks
// - Screenshots
// - Admonitions
//
// If multiple installation methods are to be detailed, then
// - Create a summary list here
// - Detail each method in its own subsection.
//
// NOTE: For solutions involving SUSE Rancher, it is preferred
//       to detail two installation methods:
//       - Through the Rancher Apps Catalog with appropriate
//         screenshots and SUSE branding.
//       - A more manual approach (e.g., on the command-line).
//
// Complex configuration procedures may be broken out into one or more
// Configuration sections.
// These may be subsections of Installation or separate sections at
// the same level as Installation.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =



=== Testing FlexRAN™ Timer Mode in Containers

To demonstrate simple functionality:

In the 1st terminal run:
[source, bash]
----
kubectl exec –it flexran-binary-release –c flexran-l1app – bash
Start l1.sh -e 
----

In the 2nd terminal run:
[source, bash]
----
kubectl exec –it flexran-binary-release –c flexran-testmac -- bash
----
[NOTE]
====
 Make sure that your dpdk directory mapped in the yaml file.
====

 Other tests such as xRAN Mode and a Helm Chart test can be run as well as described in
section 5.2 and 5.3 of Intel document 575834-15.0 [Installation Guide Software Release v22.07]

As a simplified solution, a pre-configured Intel® FlexRAN™ helm chart as well as all required CNI plugins, can be posted on Rancher Marketplace to simplify deployment at a large scale.



// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Illustrate functionality with a demonstration.
// Begin with a description or outline of the demonstration.
// Provide clear steps (in ordered lists) for the reader to follow.
// Typical demonstration flow is:
// 1. Prepare the environment for the demonstration.
//    This should be minimal, such as downloading some data to use.
//    If this requires more than a couple steps, consider putting it
//    in a subsection.
// 2. Perform the demonstration.
//    Be careful not to overuse screenshots.
// 3. Verify.
//    This may be interwoven into performing the demonstration.
//
// As with Installation, leverage ordered lists, code blocks,
// admonitions, and screenshots.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =





== Summary

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Summarize:
// - Motivation (1 sentence)
// - What was covered (1-2 sentences)
// - Next steps (unordered list of 2-4 further learning resources)
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Building, testing, and deploying a properly configured Intel® FlexRAN™  implementation can show the benefits of VNFs and vRAN with Intel® Xeon® Scalable Processors and Intel® Advanced Vector Extensions.

SUSE provides all the elements for an open-source, enterprise-grade, software-defined stack for cloud-native orchestration and management.  SUSE Linux Enterprise (with Real Time extensions), SUSE Linux Enterprise Micro Real Time, Rancher Kubernetes Engine v2 (RKE2) and Rancher Management were used and illustrated as key ingredients to simplify the deployment of Intel® FlexRAN™.


== Reference

    • https://github.com/intel/FlexRAN

    • https://www.intel.com/content/www/us/en/developer/videos/how-radio-access-network-is-being-virtualized-and-the-role-of-flexran.html?wapkw=FlexRan

    • https://www.intel.com/content/www/us/en/developer/topic-technology/edge-5g/tools/flexran.html?wapkw=FlexRan

    • https://www.intel.com/content/www/us/en/communications/5g-get-your-infrastructure-ready-guide.html
    • https://docs.rke2.io/install/quickstart/
    • https://documentation.suse.com/trd/kubernetes/pdf/kubernetes_ri_rancher-k3s-slemicro_color_en.pdf
    • https://documentation.suse.com/sle-rt/15-SP3/

    • https://documentation.suse.com/sles/15-SP3/pdf/book-container_color_en.pdf

    • https://documentation.suse.com/sle-micro/5.3/





// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
