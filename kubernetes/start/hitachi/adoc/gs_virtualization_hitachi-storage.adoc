:docinfo:
include::./common_docinfo_vars.adoc[]
include::./gs_virtualization_hitachi-storage-vars.adoc[]
[#art-{article-id}]

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// SUSE Technical Reference Documentation
// Getting Started Guide
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
//
// DOCUMENT ATTRIBUTES AND VARIABLES
//
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// 1. Define variables (document attributes) in the vars file.
// 2. Develop content and reference variables in the adoc file.
// 3. Update the docinfo.xml file as needed.
// 4. Update DC file (at a minimum, deactivate DRAFT mode)
//
// For further guidance, see
//   https://github.com/SUSE/technical-reference-documentation/blob/main/common/templates/start/README.md
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// JI: This is a good start.
// JI: Be sure to put yourself in the perspective of your reader, who may not have your skill level.
// JI: For example, if you provide a sample YAML file, be sure to tell the reader what to do with it.
// JI: I have started making some edits and including comments like this one to guide you on your next steps.
// JI: Be sure to review the vars file.
// JI: You will see several new variables that you should reference in this document;
// JI: for example, see 'svirt' and 'hitachivsp-long'.


= {title}: {subtitle}



== Introduction

As organizations evolve toward modernized hybrid infrastructures, unified virtualization and enterprise storage become critical for reliability, scalability, and efficiency.
SUSE Virtualization, built on Harvester and powered by Kubernetes, provides a robust foundation for running both virtual machines and containerized workloads on a single platform.
When deployed with {hitachivsp-long} ({hitachivsp}), enterprises can access data management, high performance storage, and data protection for mission-critical workloads.

This document provides technical guidance for integrating {hitachivsp} with {svirt}.




=== Scope

This guide provides detailed instructions for integrating {hitachivsp-long} with {svirt} clusters.
It includes steps for configuring multipath I/O, installing the {hspc-long}, creating Kubernetes resources (secrets, StorageClass, PVC, SnapshotClass), and validating the deployment using SUSE Virtualization UI and CLI.


=== Audience

This guide is intended for platform teams, systems and storage administrators, and DevOps engineers supporting modern virtualization, featuring {svirt} and {hitachivsp-long}.

The reader should have basic familiarity with the Linux, Kubernetes, storage networking protocols (iSCSI or FC), {svirt}, and {hitachivsp-long}.



=== Acknowledgements

The authors acknowledge contributions from Hitachi Vantara engineering teams for validation and joint testing of the HSPC CSI integration with {svirt}.



== Overview

// JI: Use this section to provide a brief, a component-level architecture.
// JI: I refocused this section on three, key components.
// JI: The others (e.g., multipathd, secrets, and storageclass) should be noted when they are configured.
// JI: Insert an architecture diagram at the top of this section
// JI: See the SUSE Virtualization + Portworx + Pure Storage FlashArray whitepaper for an example


The integrated solution architecture brings together {svirt} as the cloud native virtualization with {hitachivsp-long} as the enterprise storage back-end.

//{empty} +

// Architecture diagram
image::SUSE-Virt-Hitachi.png[Architecture diagram, scaledwidth="85%", align="center"]

Key elements of this solution include:

SUSE Virtualization::
{svirt-website}[{svirt}] provides Kubernetes-native virtual machine and container management.
This guide is valid for {svirt} {svirt-version1} and later.

{hitachivsp-long}::
{hitachivsp-website}[{hitachivsp}] provides enterprise storage for backing container and virtual machine workloads.
Supported platforms include Hitachi VSP, VSP One Block as well as VSP One SDS in Azure, AWS, and GCP public clouds.

{hspc-long} ({hspc})::
{hspc-website}[{hspc}] provides integration between {hitachivsp} and {svirt}, enabling you to create and utilize Hitachi storage volumes for stateful container applications and virtual machines.


{empty} +
In general, the integration works as follows:

. {hspc} receives a PersistentVolumeClaim (PVC) request.

. {hspc} calls the Hitachi REST/API to provision a logical volume on the configured pool.

. {hitachivsp} array exposes the LUN via iSCSI or FC and attaches the volume to the {svirt} node where the workload (container pod or VM) runs.


{empty} +
The following design considerations are recommended:

* Use at least three controller/worker nodes for production-grade availability.

* Use dedicated a dedicated storage network.

//JI: This should be part of the multipathd installation and configuration steps in the "Prepare the environment" section:
//JI: Ensure the {svirt-docs-multipathd}[multipath] configuration matches your SAN fabric (ALUA settings for FC and appropriate path checker for iSCSI).



// JI: I think we can separate the procedure into sections, with the first being about preparation.
== Prepare the environment

This section provides detailed steps required to prepare your environment for the integration.

// JI: We do not explicitly number steps, the rendering engine will do this based on the order of full and number of stops ('.').
. Install {svirt}::
+
Access the {svirt} {svirt-docs}[documentation] for detailed technical information, including Hardware and Network Requirements and {svirt-docs-iso}[installation guidance].  This guide references {svirt} {svirt-version1} and later.
+
// JI: Describe HA deployment, provide links to hardware and network requirements, link to installation instructions.


. Configure multipathd on all SUSE Virtualization worker nodes.
+
The multipathd service provides redundant I/O paths (iSCSI or Fibre Channel) from {svirt} worker nodes to {hitachivsp} to ensure high availability and fail-over.
+
// JI: It may be helpful to add a step to tell the reader to log into each node.
.. Enable and start {svirt-docs-multipathd}[multipathd].
+
Login on all the theree nodes of {svirt} and apply below commands to start and enable multipathd service.
+
[source, console]
----
systemctl enable multipathd
systemctl start multipathd
systemctl status multipathd
----
// JI: Each numbered step should be an action.  What does the reader do with this file?

.. Sample /etc/multipath.conf configuration (adjust vendor/product rules to match your array):
+
[source, json]
----
   defaults {
       user_friendly_names yes
       find_multipaths yes: 
   }
   devices {
       device {
           vendor "HITACHI"
           product "OPEN-.*"
           path_grouping_policy "multibus"
           path_checker "tur"
           failback immediate
           prio "const"
           rr_weight uniform
           no_path_retry 10
       }
   }
----

.. Restart all nodes.

. Install {hspc-long}.
.. Clone operator repository and change directory.
+
[source, json]
----
   git clone https://github.com/hitachi-vantara/csi-operator-hitachi
   cd csi-operator-hitachi/hspc/<version>/operator
----   
.. Create operator namespace and deploy.
+
[source, json]
----   
   kubectl apply -f hspc-operator-namespace.yaml
   kubectl apply -f hspc-operator.yaml
   kubectl -n hspc-operator-system get deploy
----


. Deploy HSPC instance.
+
.. Customize hspc_v1_hspc.yaml with your namespace and settings, then apply
+
[source, json]
----
   apiVersion: csi.hitachi.com/v1 
   kind: HSPC
   metadata:
     name: hspc
     namespace: vspone
   spec: {}
----

+
[source, json]
----
   kubectl apply -f hspc_v1_hspc_vspone.yaml

   kubectl get hspc -n vspone
----   
. Create Secret with Hitachi credentials.

.. Encode credentials to base64 and create secret YAML (example uses placeholders):
+
[source, json]
----
   apiVersion: v1
   kind: Secret
   metadata:
     name: secret-vsp-112
     namespace: vspone
   type: Opaque
   data:
     url: <base64-url>
     user: <base64-username>
     password: <base64-password>

----
.. Apply via CLI or SUSE Virtualization UI.


. Create StorageClass

.. Example StorageClass YAML:
+
[source, json]
----
   apiVersion: storage.k8s.io/v1
   kind: StorageClass
   metadata:
     name: vsp-5500-112-sc-iscsi
   provisioner: hspc.csi.hitachi.com
   reclaimPolicy: Delete
   volumeBindingMode: Immediate
   parameters:
     poolID: "<pool-id>"
     csi.storage.k8s.io/provisioner-secret-name: secret-vsp-112
     csi.storage.k8s.io/provisioner-secret-namespace: vspone   
----

.. Apply with kubectl apply -f storageclass.yaml

. Create PVC and Validate

.. PVC YAML example (50Gi):
+
[source, json]
----

   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: hitachi-pvc-50g
     namespace: vspone
   spec:
     accessModes:
     - ReadWriteOnce
     resources:
       requests:
         storage: 50Gi
     storageClassName: vsp-5500-112-sc-iscsi
----

.. Verify PVC and PV binding:
   
   kubectl get pvc -n vspone

   kubectl get pv

. Optional Test Pod

.. Apply the test pod YAML to validate mount and I/O.
+
[source, json]
----

   apiVersion: v1
   kind: Pod
   metadata:
   name: hitachi-test-pod
   namespace: vspone
   spec:
   restartPolicy: Never
   containers:
   - name: app
      image: busybox
      command: ['sh', '-c', 'echo Hello Hitachi VSP > /data/out.txt && sleep 3600']
      volumeMounts:
      - mountPath: /data
         name: hitachi-vol          
   volumes:
   - name: hitachi-vol
      persistentVolumeClaim:
         claimName: hitachi-pvc-50g 
----

. Enable and Test Snapshots

.. Create VolumeSnapshotClass.

+
[source, json]
----
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: hitachi-snapclass
driver: hspc.csi.hitachi.com
deletionPolicy: Delete   
parameters:
  poolID: "2"
  csi.storage.k8s.io/snapshotter-secret-name: secret-vsp-112
  csi.storage.k8s.io/snapshotter-secret-namespace: vspone
----

+
[source, json]
----
kubectl apply -f hitachi-snapclass.yaml
----
.
.. Create a VolumeSnapshot resource referencing the PVC and verify:
+
[source, json]
----
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: hitachi-snap-example
  namespace: vspone
spec:
  volumeSnapshotClassName: hitachi-snapclass
  source:
    persistentVolumeClaimName: hitachi-pvc-50g
----

+
[source, json]
----
kubectl apply -f hitachi-snap-example.yaml
kubectl -n vspone get volumesnapshot
----
.


. Configure SUSE Virtualization CSI Settings for Snapshots

.. In SUSE Virtualization UI: Advanced → Settings → csi-driver-config → Edit

.. Add Provisioner: hspc.csi.hitachi.com and Volume Snapshot Class Name: `hitachi-snapclass`

. Validate with a VM in SUSE Virtualization

.. Create or edit a VM in SUSE Virtualization and choose the Hitachi StorageClass when adding disks.

.. Power on the VM and verify the guest OS sees the attached disk and that I/O works.

== Troubleshooting tips:

. If PV remains in Pending state, check HSPC controller logs and CSI driver pods for errors.

. Use `multipath -ll` on nodes to verify paths and LUN presentation.

. Confirm base64-encoded credentials are correct and the Hitachi REST endpoint is reachable.

. Configure multipath on all SUSE Virtualization nodes.

. Install the HSPC Operator from the Hitachi repository.

. Create the HSPC instance in Kubernetes.

. Create a Secret with Hitachi VSP credentials.

. Create the StorageClass for dynamic provisioning.

. Create and validate PVC and snapshots.


== Summary

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Summarize:
// - Solution description
// - Motivation for the guide
// - What was done
// - Suggested next steps for the learning journey
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


This guide provided a detailed procedure for integrating Hitachi VSP Storage with SUSE Virtualization using the HSPC CSI Operator. The integration delivers enterprise-grade reliability, dynamic storage provisioning, and snapshot capabilities, enabling consistent storage services across workloads.




// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
