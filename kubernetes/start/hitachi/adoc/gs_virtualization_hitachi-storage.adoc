:docinfo:
include::./common_docinfo_vars.adoc[]
include::./gs_virtualization_hitachi-storage-vars.adoc[]
[#art-{article-id}]

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// SUSE Technical Reference Documentation
// Getting Started Guide
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
//
// DOCUMENT ATTRIBUTES AND VARIABLES
//
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// 1. Define variables (document attributes) in the vars file.
// 2. Develop content and reference variables in the adoc file.
// 3. Update the docinfo.xml file as needed.
// 4. Update DC file (at a minimum, deactivate DRAFT mode)
//
// For further guidance, see
//   https://github.com/SUSE/technical-reference-documentation/blob/main/common/templates/start/README.md
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// JI: This is a good start.
// JI: Be sure to put yourself in the perspective of your reader, who may not have your skill level.
// JI: For example, if you provide a sample YAML file, be sure to tell the reader what to do with it.
// JI: I have started making some edits and including comments like this one to guide you on your next steps.
// JI: Be sure to review the vars file.
// JI: You will see several new variables that you should reference in this document;
// JI: for example, see 'svirt' and 'hitachivsp-long'.


= {title}: {subtitle}



== Introduction

As organizations evolve toward modernized hybrid infrastructures, unified virtualization and enterprise storage become critical for reliability, scalability, and efficiency.
{svirt}, built on {svirt} and powered by Kubernetes, provides a robust foundation for running both virtual machines and containerized workloads on a single platform.
When deployed with {hitachivsp-long} ({hitachivsp}), enterprises can access data management, high performance storage, and data protection for mission-critical workloads.

This document provides technical guidance for integrating {hitachivsp} with {svirt}.




=== Scope

This guide provides detailed instructions for integrating {hitachivsp-long} with {svirt} clusters.
It includes steps for configuring multipath I/O, installing the {hspc-long}, creating Kubernetes resources (secrets, StorageClass, PVC, SnapshotClass), and validating the deployment using {svirt} UI and CLI.


=== Audience

This guide is intended for platform teams, systems and storage administrators, and DevOps engineers supporting modern virtualization, featuring {svirt} and {hitachivsp-long}.

The reader should have basic familiarity with the Linux, Kubernetes, storage networking protocols (iSCSI or FC), {svirt}, and {hitachivsp-long}.



=== Acknowledgements

The authors acknowledge contributions from Hitachi Vantara engineering teams for validation and joint testing of the HSPC CSI integration with {svirt}.



== Overview

// JI: Use this section to provide a brief, a component-level architecture.
// JI: I refocused this section on three, key components.
// JI: The others (e.g., multipathd, secrets, and storageclass) should be noted when they are configured.
// JI: Insert an architecture diagram at the top of this section
// JI: See the SUSE Virtualization + Portworx + Pure Storage FlashArray whitepaper for an example


The integrated solution architecture brings together {svirt} as the cloud native virtualization with {hitachivsp-long} as the enterprise storage back-end.

//{empty} +

// Architecture diagram
image::SUSE-Virt-Hitachi.png[Architecture diagram, scaledwidth="85%", align="center"]

Key elements of this solution include:

SUSE Virtualization::
{svirt-website}[{svirt}] delivers a fully integrated, cloud-native virtualization platform built on modern Kubernetes technologies. It simplifies VM lifecycle management, enhances platform resiliency, and provides a secure foundation for enterprise workloads. {svirt} brings the advantages of container orchestration into traditional virtualization environments, enabling organizations to modernize infrastructure without complexity.

Key benefits include:
{empty} +

* High Availability (HA)
+
High Availability in {svirt} is grounded in a combination of Kubernetes-native resilience and enterprise storage capabilities. RKE2 provides an HA control plane and manages cluster health, automatically restarting or relocating workloads if nodes become unresponsive. When paired with Hitachi VSP storage using multipath I/O, VM disks remain accessible across multiple storage paths, protecting against network, node, or controller failures. This multi-layered HA architecture ensures uninterrupted operations for critical applications.

* Enhanced Security
+
Built on SLE Micro OS, {svirt} uses an immutable filesystem, transactional updates, and minimal footprint security principles. This significantly reduces the attack surface. Integrated Kubernetes security mechanisms—including role-based access control, hardened RKE2, and kernel-level controls—protect workloads from misconfigurations and intrusions.

* RKE2 Enterprise Kubernetes
+
RKE2 provides a secure-by-default Kubernetes distribution with enterprise lifecycle management. {svirt} uses RKE2 as the control plane, enabling robust orchestration, consistent upgrades, and strong compliance with industry standards.

* {svirt} UI
+
The {svirt} dashboard offers an intuitive, centralized interface for managing VMs, networks, storage, and snapshots. Administrators can create, clone, and migrate virtual machines without requiring deep Kubernetes knowledge, improving team productivity.

* External Storage Compatibility
+
{svirt}’s CSI-based design enables seamless integration with enterprise SANs like Hitachi VSP. This allows virtual machines to boot directly from external storage, ensuring high-performance data access, centralized storage management, and advanced storage features such as snapshots, replication, and compression.

This guide is valid for {svirt} {svirt-version1} and later.

{empty} +

{hitachivsp-long}::
{hitachivsp-website}[{hitachivsp}] provides enterprise storage for backing container and virtual machine workloads.
Supported platforms include Hitachi VSP, VSP One Block as well as VSP One SDS in Azure, AWS, and GCP public clouds.

{hspc-long} ({hspc})::
{hspc-website}[{hspc}] provides integration between {hitachivsp} and {svirt}, enabling you to create and utilize Hitachi storage volumes for stateful container applications and virtual machines.


{empty} +
In general, the integration works as follows:

. {hspc} receives a PersistentVolumeClaim (PVC) request.

. {hspc} calls the Hitachi REST/API to provision a logical volume on the configured pool.

. {hitachivsp} array exposes the LUN via iSCSI or FC and attaches the volume to the {svirt} node where the workload (container pod or VM) runs.


{empty} +
The following design considerations are recommended:

* Use at least three controller/worker nodes for production-grade availability.

* Use dedicated a dedicated storage network.

//JI: This should be part of the multipathd installation and configuration steps in the "Prepare the environment" section:
//JI: Ensure the {svirt-docs-multipathd}[multipath] configuration matches your SAN fabric (ALUA settings for FC and appropriate path checker for iSCSI).



== Prepare the environment
Create or edit a VM in SUSE Virtualization and choose the Hitachi Stora
This section provides detailed steps required to prepare your environment for the integration.

. Install {svirt}.
+
Access the {svirt} {svirt-docs}[documentation] for detailed technical information, including Hardware and Network Requirements and {svirt-docs-iso}[installation guidance].  This guide references {svirt} {svirt-version1} and later.
+
// JI: Describe HA deployment, provide links to hardware and network requirements, link to installation instructions.


. Configure multipathd on all SUSE Virtualization worker nodes.
+
The {svirt-docs-multipathd}[multipathd] service provides redundant I/O paths (iSCSI or Fibre Channel) from {svirt} worker nodes to {hitachivsp} to ensure high availability and fail-over.
To configure and start multipathd, log into each of the three {svirt} nodes and perform the following operations on the command line:

.. Enable and start multipathd.
+
[source, console]
----
systemctl enable multipathd
systemctl start multipathd
systemctl status multipathd
----

.. Create the file, `/etc/multipath.conf`.
//
+
Be sure vendor and product rules match your array.
+
[source, yaml]
----
defaults {
    user_friendly_names yes
    find_multipaths yes

}

blacklist {
}

devices {
    device {
        vendor "(HITACHI|HP)"
        product "OPEN-.*"
        path_grouping_policy "multibus"
        path_checker "tur"
        features "0"
        failback immediate
        features "0"
        hardware_handler "0"
        prio "const"
        rr_weight uniform
        no_path_retry 10
    }
}
----

.. With `/etc/multipath.conf` created, restart the `multipathd` service.
+
[source, console]
----
systemctl restart multipathd
----

. Install {hspc-long}.
//
+
The following steps can be performed on any one of master node of the {svirt} cluster.

.. Clone operator repository and change directory.
+
[source, console]
----
git clone https://github.com/hitachi-vantara/csi-operator-hitachi
cd csi-operator-hitachi/hspc/<version>/operator
----   

.. Create operator namespace and deploy the operator.
+
[source, yaml]
----   
kubectl apply -f hspc-operator-namespace.yaml
kubectl apply -f hspc-operator.yaml
----

.. Verify the deployments in hspc-operator-system
+
[source, console]
----
kubectl get deployment -n hspc-operator-system
----
+
image::verify-hspc-operator.png[Architecture diagram, scaledwidth="90%", align="left"]


. Deploy the HSPC instance.

.. Customize the file, `hspc_v1_hspc_vspone.yaml`, with your namespace and settings.
+
[source, yaml]
----
apiVersion: csi.hitachi.com/v1 
kind: HSPC
metadata:
  name: hspc
  namespace: vspone
spec: {}
----

.. Create the operator namespace.
+
[source, yaml]
----
kubectl apply -f hspc_v1_hspc_vspone.yaml
kubectl get hspc -n vspone
----   

.. Verify hspc pods and operator controller status
+
image::hspc-instance.png[Architecture diagram, scaledwidth="90%", align="left"]


. Create Secret with Hitachi credentials.

.. Encode credentials to base64 and define the secret in `secret-vsp-112.yaml`.
+
[source, yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: secret-vsp-112
  namespace: vsponevia CLI or SUSE Virtualization UI.
type: Opaque
data:
  url: <base64-url>
  user: <base64-username>
  password: <base64-password>
----

.. Create the secret.
+
[source, console]
----
kubectl apply -f secret-vsp-112.yaml
----


. Create the StorageClass.
//
+
This provides a StorageClass mapping to the HSPC CSI driver.

.. Modify the StorageClass YAML.
+
[source, yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: vsp-5500-112-sc-iscsi
provisioner: hspc.csi.hitachi.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
parameters:
  poolID: "<pool-id>"
  csi.storage.k8s.io/provisioner-secret-name: secret-vsp-112
  csi.storage.k8s.io/provisioner-secret-namespace: vspone   
----

.. Activate the StorageClass.
+
[source, console]
----
kubectl apply -f storageclass.yaml
----

.. Verify the StorageClass.
+
image::validate-sc.png[Architecture diagram, scaledwidth="90%", align="left"]

. Create a PersistentVolumeClaim (PVC).

.. Define a PVC.
//
+
Create 'hitachi-pvc-50g.yaml', Specify the storage capacity such as 50GiB, as follows:
+
[source, yaml]
----
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: hitachi-pvc-50g
     namespace: vspone
   spec:
     accessModes:
     - ReadWriteOnce
     resources:
       requests:
         storage: 50Gi
     storageClassName: vsp-5500-112-sc-iscsi
----

.. Activate the PVC.
+
[source, console]
----
kubectl apply -f hitachi-pvc-50g.yaml
----

.. Verify the PVC binding.
+
[source, console]
----
kubectl get pvc -n vspone
----
+
image::validate-pvc.png[Architecture diagram, scaledwidth="90%", align="center"]


. Optional Test Pod

.. Create 'pod.yaml' to validate mount and I/O.
+
[source, yaml]
----
apiVersion: v1
kind: Pod
metadata:
name: hitachi-test-pod
namespace: vspone
spec:
restartPolicy: Never
containers:
- name: app
   image: busybox
   command: ['sh', '-c', 'echo Hello Hitachi VSP > /data/out.txt && sleep 3600']
   volumeMounts:
   - mountPath: /data
      name: hitachi-vol          
volumes:
- name: hitachi-vol
   persistentVolumeClaim:
      claimName: hitachi-pvc-50g 
----

.. Activate the pod.
+
[source, console]
----
kubectl apply -f pod.yaml
----

.. Verify the pod.
+
[source, console]
----
kubectl get pods -n vspone
----
+
image::validate-pod.png[Architecture diagram, scaledwidth="80%", align="center"]


. Enable and Test Snapshots.

.. Define a VolumeSnapshotClass in `volumesnapshotclass-hspc.yaml`.
+
[source, yaml]
----
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: hitachi-snapclass
driver: hspc.csi.hitachi.com
deletionPolicy: Delete 
parameters:
  poolID: "2"
  csi.storage.k8s.io/snapshotter-secret-name: secret-vsp-112
  csi.storage.k8s.io/snapshotter-secret-namespace: vspone
----

.. Apply the VolumeSnapshotClass.
+
[source, console]
----
kubectl apply -f hitachi-snapclass.yaml
----

.. Verify the VolumeSnapshotClass.
+
[source, console]
----
kubectl get VolumeSnapshotClass
----
+
image::validate-vol-snapshot.png[Architecture diagram, scaledwidth="80%", align="left"]


.. Create a VolumeSnapshot resource referencing the PVC and verify.
+
[source, yaml]
----
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: hitachi-snap-example
  namespace: vspone
spec:
  volumeSnapshotClassName: hitachi-snapclass
  source:
    persistentVolumeClaimName: hitachi-pvc-50g
----

.. Apply the VolumeSnapshot resource.
+
[source, console]
----
kubectl apply -f hitachi-snap-example.yaml
----

.. Verify the VolumeSnapshot.
+
[source, console]
----
kubectl -n vspone get volumesnapshot
----
+
image::verify-snapshot.png[Architecture diagram, scaledwidth="120%", align="center"]


. Configure SUSE Virtualization CSI Settings for Snapshots.

.. In the SUSE Virtualization UI, select __Advanced__ → __Settings__ → __csi-driver-config__ → __Edit__.

.. Click __Add__.

.. For __Provisioner__, choose or type: `hspc.csi.hitachi.com`.

.. For __Volume Snapshot Class Name__, enter: `hitachi-snapclass`.

.. Leave __Backup Volume Snapshot Class Name__ blank.

.. Save the changes.

+
image::hspc-snapshot-mapping.png[Architecture diagram, scaledwidth="100%", align="left"]

. Validate with a VM in SUSE Virtualization.

.. Create or edit a VM in SUSE Virtualization and choose the Hitachi StorageClass when adding disks.
+
image::hitachi-disks.png[Architecture diagram, scaledwidth="100%", align="left"]

.. Power on the VM and verify the guest OS sees the attached disk and that I/O works.
+
image::hitachi-vm-running.png[Architecture diagram, scaledwidth="100%", align="left"]




== Troubleshooting tips

* If PV remains in Pending state

. check HSPC controller logs and CSI driver pods for errors.

. Use `multipath -ll` on nodes to verify paths and LUN are present.

. Confirm base64-encoded credentials are correct and the Hitachi REST endpoint is reachable.

. Make sure multipath is enabled on all SUSE Virtualization nodes.



== Summary

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Summarize:
// - Solution description
// - Motivation for the guide
// - What was done
// - Suggested next steps for the learning journey
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


This guide provided a detailed procedure for integrating Hitachi VSP Storage with SUSE Virtualization using the HSPC CSI Operator. The integration delivers enterprise-grade reliability, dynamic storage provisioning, and snapshot capabilities, enabling consistent storage services across workloads. get more information about {hspc-website}[{hspc}] 




// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
