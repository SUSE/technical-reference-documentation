:docinfo:
include::./common_docinfo_vars.adoc[]

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// General comments
// Keep in mind that this is a "getting started" guide and the
//   audience that you are trying to reach.
// Leverage ASCIIDoc features to make this document readable and usable:
//   - Text highlights (follow SUSE style guides)
//   - Admonitions (i.e., NOTE, TIP, IMPORTANT, CAUTION, WARNING)
//   - Code blocks
//   - Lists (ordered and unordered, as appropriate)
//   - Links (to other resources)
//   - Images
//     - Place image files under the ./media directory tree
//       (e.g., ./media/src/svg, ./media/src/png)
//     - Format preference: svg > png > jpg
//     - Consolidate images wherever possible
//       (i.e., don't use two images when one conveys the message)
//   - Use sections and subsections to organize and group related
//     steps.
// 
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Document attributes and variables
//
// NOTES:
// 1. Update variables below and adjust docbook file accordingly.
// 2. Comment out any variables/attributes not used.
// 3. Follow the pattern to include additional variables.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// organization - do NOT modify
// -
:trd: Technical Reference Documentation
:type: Getting Started
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// document
// -
:title: Kubeflow on Rancher
:subtitle: Deploying Kubeflow with SUSE Rancher

:product1: Harvester
:product1_full: Harvester by SUSE
:product1_url: www.suse.com/products/harvester/
:product2: Rancher
:product2_full: Rancher Prime by SUSE
:product2_version: 2.7
:product2_url: www.suse.com/solutions/enterprise-container-management/#rancher-product
:product3: RKE2
:product3_full: Rancher Kubernetes Engine 2
:product3_url: docs.rke2.io/
:product4: Kubeflow

:usecase: Simplify deployment of machine learning (ML) workflows on Kubernetes

:executive_summary: Kubeflow simplifies deployment of machine learning (ML) workflows on Kubernetes clusters.  This document provides step-by-step guidance for deploying Kubeflow on an {product3} cluster with {product2_full}.

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// contributor
// specify information about authors, editors, and others here,
// then update docinfo file as appropriate
// -
:author1_firstname: Alex
:author1_surname: Arnoldy
:author1_jobtitle: job title
:author1_orgname: SUSE
:author2_firstname: Mark
:author2_surname: Gonnelly
:author2_jobtitle: job title
:author2_orgname: SUSE
:author3_firstname: Terry
:author3_surname: Smith
:author3_jobtitle: Director, Global Partner Solutions
:author3_orgname: SUSE
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// miscellaneous
// define any additional variables here for use within the document
// -
:kubeflow_url: www.kubeflow.org/
:kubeflow_url_docs: www.kubeflow.org/docs/
:kubeflow_url_repo: github.com/kubeflow/
:k8s_ver: v1.24.9+rke2r2
:kustomize_ver: 5.0.0

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


= {title}: {subtitle}



== Introduction

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a brief statement (1-4 sentences) of the purpose of the guide.
// This is could be the same as the executive summary.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Machine learning (ML) is driving innovation across many domains.
The importance of ML is reflected by its adoption by the enterprise.
Businesses use ML to achieve deeper insights, grow capabilities, improve efficiencies, and accelerate results.

ML workflows can be complex, and managing them can be difficult.
This is particularly true at production scale.
https://{kubeflow_url}[Kubeflow] is an open source project that aims to make deploying and scaling ML models as simple as possible.
To achieve this, Kubeflow takes a cloud-native approach, deploying workloads in containers on https://kubernetes.io[Kubernetes] clusters.

Rancher Prime by SUSE empowers organizations to unify their Kubernetes landscape with secure, streamlined management.
By deploying Kubeflow into a https://www.rancher.com/products/rancher[Rancher by SUSE] Kubernetes landscape, you can address the operational and security challenges of managing ML workflows at scale.

The modern, enterprise Kubernetes landscape leverages computing, storage, and networking resources in a variety of environments.
https://www.suse.com/products/harvester/[Harvester by SUSE] is a next generation open source, hyperconverged infrastructure (HCI) solution designed for modern cloud-native environments.
Harvester provides operators with a cohesive platform to manage virtual machine and container workloads such as Kubeflow.


=== Scope

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Specify what this guide covers in no more than 2 sentences.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

In this guide, you deploy Kubeflow onto a {product3} Kubernetes cluster managed by {product2_full}.
This guide also illustrates provisioning virtual machines with {product1_full} to host the {product3} cluster.

After completing these steps, you are able to access the Kubeflow user interface.


=== Audience

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify for whom this document is intended, perhaps with:
//   - topics of interests (e.g., machine learning, security, etc.)
//   - job roles (e.g., developer, administrator, platform architect, etc.)
//   - required skills
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Data scientists, ML researchers, developers, and operational teams with responsibility for ensuring the success of ML projects.

To be successful with this guide, you should have basic knowledge of Kubernetes operations in a Rancher environment as well as standard Linux command line container management with Kubectl, Helm, and Kustomize.


== Technical overview

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a technical overview of the solution.
// - Identify components.
// - Describe how the components fit together.
//   Leverage diagrams as appropriate, including (but not limited to):
//   - component architecture
//   - data flow diagram
//   - workflow diagram
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

In this guide, you:

. use {product1} to provision virtual machines for your Kubernetes cluster.

. instantiate and configure your {product3} Kubernetes cluster with {product2}.

. deploy Kubeflow onto your {product3} cluster.

. access the Kubeflow user interface (UI).


[NOTE]
====
Add diagram and say a few more words about the various pieces

. Why RKE2?

. Why Rancher?

. Why Harvester?

. Why MetalLB load-balancer?

====



== Prerequisites

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify minimum requirements (prerequisites) the reader
// would need in order to follow the steps of this guide.
// - Link to existing resources whenever possible.
// - Keep this section brief but elaborate as needed.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


For this guide, you will use {product1} to create a five-node, {product3} Kubernetes cluster managed by {product2}.



For this guide, we use Harvester, managed through Rancher, to create a five node RKE2 cluster. However, the same steps should work on any other RKE2 cluster that has a default Storage Class. You will need at least five nodes - three of them for the control plane and two for the workload plane.

* All nodes should be in the same Harvester namespace
* The operating systems for all nodes is the SUSE Linux Enterprise 15 SP4 minimal QCOW2 image with cloud-init enabled (previously known as the OpenStack image)
* All nodes are connected to a single VLAN with DHCP, DNS, and routing to the Internet
* The three VMs with the roles of control-plane and etcd pool are configured with resources of 4 vCPU, 8GB RAM, 40GB boot drive
* The two VMs that make up the workload-plane pool are configured with 8 vCPU, 16GB RAM, 40GB boot drive
* The SSH user for the operating system image is 
+
[source, console]
----
sles
----

[NOTE]
====
The node sizings used here were for basic testing purposes only. It is likely that more CPU and RAM would be required for the workload-plane VMs in particular to support a useful Kubeflow workload.
====

You will also need a workstation with the following software installed. Please see the respective links for installation instructions:

* Kubectl https://kubectl.docs.kubernetes.io/installation/kubectl/[Installation]

* Helm https://helm.sh/docs/intro/install/[Installation]

* Kustomize https://kubectl.docs.kubernetes.io/installation/kustomize/[Installation] 




== Deploy an RKE2 cluster with Harvester

We begin by using the Rancher UI to create a project in Harvester named _kubeflow-on-harvester_ that contains a namespace named _kubeflow-cluster_.
The cluster name should be set to: 
[source, console]
----
kubeflow-on-harvester
----

We now need to define our five nodes as described above in the prerequisites.

The following User Data cloud-config (under __Show Advanced__) should be applied to all nodes during RKE2 cluster creation:

[listing]
----
### cloud-init
#cloud-config
chpasswd:
  list: |
    root:SUSE
    sles:SUSE
  expire: false
ssh_authorized_keys:
  - >-
    <REPLACE WITH SSH PUBLIC KEY OF THE WORKSTATION>
runcmd:
  - SUSEConnect --url <REPLACE WITH RMT SERVER ADDRESS>
  - zypper -n in -t pattern apparmor
  - zypper -n up
  - zypper in --force-resolution --no-confirm --force kernel-default
  - zypper rm kernel-default-base
----

Configure the Kubernetes `Cluster Configuration` as follows:

. On the __Basic__ tab:

.. Select Kubernetes version {k8s_ver}.
+
[NOTE]
====
This version is currently deprecated but is needed for Harvester Cloud Provider support. If you are not using Harvester, you can select a newer version.
====

.. Enable the Harvester Cloud Provider CSI driver.

.. Set __Container Network__ Interface to `Calico`.

.. Set the __Default Security Pod Policy__ to `unrestricted`.

.. Disable the Nginx Ingress controller under __System Services__.

. On the __Labels and Annotations__ tab:

.. Apply a cluster label where they key is `app` and the value is `kubeflow`. 

. Click __Create__.


After the cluster has been created, verify and reboot the RKE2 nodes:

. SSH to each node as the user `sles`.

. Verify that the `kernel-default` kernel has been installed and `kernel-default-base` kernel has been removed: 
+
[source, console]
----
sudo zypper se kernel-default
----
+
[TIP]
====
After `kernel-default` has been installed, you can remove the `kernel-default-base` kernel with: 
+
[source, console]
----
sudo zypper rm kernel-default-base
----
====

. Verify that all operating system software has been patched to the latest update: 
+
[source, console]
----
sudo zypper up
----

. Reboot each node, in turn to enable the `kernel-default` kernel.

After the RKE2 cluster has been created, gather the KUBECONFIG data from the Rancher Management server and provide it to a workstation with Kubectl and Helm installed.


== Deploy the MetalLB load balancer

[NOTE]
====
*Terry*: 
I have combined the deployment and testing of MetalLB into one section. Three things...
I think we want to remove this to a separate doc. 
I changed the formatting of the part about IP addressing. I hope I retained the correct sense.
The test procedure mentions nginx. Is this appropriate to our case?
====

MetalLB is a network load-balancer implementation for Kubernetes clusters on bare metal.
This allows you to create Kubernetes services of the type `LoadBalancer` to provide two important features:

* _address allocation_: MetalLB will take care of assigning individual IP addresses from configured address pools as services are launched and reclaiming those addresses when the services end.

* _external announcement_: MetalLB uses standard network or routing protocols to announce that the IP address is in use by a service.

=== Deploy MetalLB from the kubectl server

. Pull and apply the MetalLB manifests
+
[source, console]
----
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml
# On first install only
kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
----

. Set the address ranges below:
** Use CIDR notation (e.g.10.0.0.10/32) if only assigning a single IP address
** Otherwise, set at least two IP addresses in the default IP range and optionally, at least two IP addresses in the reserved IP range, which will not be auto-assigned
** IP ranges can also be defined by CIDR notation. Adjust these variables and the configmap file as needed. For example, put a CIDR block in the START variable (e.g. export DEFAULT_IP_RANGE_START="10.0.0.10/32") and leave the END variable empty. When creating the metallb-config.yaml, make sure you do not include any variables (or the dashes between variables) that have not been defined.
+
[source, console]
----
export DEFAULT_IP_RANGE_START=
export DEFAULT_IP_RANGE_END=
export RESERVED_IP_RANGE_START=
export RESERVED_IP_RANGE_END=
----

. Create the MetalLB configuration file for layer 2 routing. See https://metallb.universe.tf/configuration/ for other routing options and https://raw.githubusercontent.com/google/metallb/v0.9.3/manifests/example-config.yaml for lots of configuration options
+
[source, console]
----
cat <<EOF> metallb-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - ${DEFAULT_IP_RANGE_START}-${DEFAULT_IP_RANGE_END}
    - name: rsvd
      protocol: layer2
      auto-assign: false
      addresses:
      - ${RESERVED_IP_RANGE_START}-${RESERVED_IP_RANGE_END}
EOF
----

. Create the configmap: 
+
[source, console]
----
kubectl apply -f metallb-config.yaml
----
. Verify the configuration was applied correctly (especially review the IP address pool): 
+
[source, console]
----
kubectl get configmap config -n metallb-system -o yaml
----
. Verify the MetalLB load balancer is running: 
+
[source, console]
----
kubectl get all -n metallb-system
----


=== Validate MetalLB

. Set this variable with the target namespace.
+
[source, console]
----
NAMESPACE="kubeflow"
----

. Create the namespace. 
+
[source, console]
----
kubectl create namespace ${NAMESPACE}
----

. Create the manifest for an nginx pod, PVC, and load balancer service.
+
[source, console]
----
cat <<EOF> nginx-metallb-test.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: ${NAMESPACE}
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1
        ports:
        - name: http
          containerPort: 80
        volumeMounts:
        - mountPath: /mnt/test-vol
          name: test-vol
      volumes:
      - name: test-vol
        persistentVolumeClaim:
          claimName: nginx-pvc


---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-pvc
  namespace: ${NAMESPACE}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi


---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: ${NAMESPACE}
spec:
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  type: LoadBalancer
EOF
----

. Create the pod, service, and the PVC. 
+
[source, console]
----
kubectl apply -f nginx-metallb-test.yaml
----

. Verify the pod is "Running", the `harvester` StorageClass is the `(default)`, the persistentvolumeclaim is `Bound`, and the service has an `EXTERNAL-IP`.
+ 
[source, console]
----
kubectl get pod,sc,pvc,svc -n ${NAMESPACE}
----

. Verify that the service is reachable through the load-balancer IP address from outside the cluster:
+
[source, console]
----
IPAddr=$(kubectl get svc -n ${NAMESPACE} | grep -w nginx | awk '{print$4":"$5}' | awk -F: '{print$1":"$2}')
curl http://${IPAddr} | grep "Thank you"
----
An HTML encoded output should display the phrase "Thank you for using nginx."

. Verify that the volume is mounted in the test pod. 
+
[source, console]
----
TEST_POD=$(kubectl get pods -n ${NAMESPACE} | awk '/nginx/ {print$1}')
kubectl exec -it ${TEST_POD} -n ${NAMESPACE} -- mount | grep test-vol
----
+
The output should show that the volume is mounted at the location `/mnt/test-vol`.

. When finished with testing, delete the pod and service. 
+
[source, console]
----
kubectl delete -f nginx-metallb-test.yaml
----


== Install Kubeflow

The Kubeflow Manifests Working Groups provides two options for installing Kubeflow:

* A single command that installs all components and targets ease of deployment for end users. 

* A multi-command approach that allows individual components to be included or excluded. 

In this guide, we will take the simpler approach and install everything. Please refer to the https://github.com/kubeflow/manifests#install-individual-components[Kubeflow Installation] documentation for more details of the alternative approach.

Assuming your workstation meets the prerequisites described above, then the following command will download and install Kubeflow:
[source, console]
----
while ! kustomize build example | awk '!/well-defined/' | kubectl apply -f -; do echo "Retrying to apply resources"; sleep 10; done
----

Note that the `kubectl apply` command may fail on the first try. Hence the use of the while loop to retry until success is achieved. 

After installation completes, it can take some time for all of the components to deploy their pods. To check that all Kubeflow-related pods are ready, use the following commands:
[source, console]
----
kubectl get pods -n cert-manager
kubectl get pods -n istio-system
kubectl get pods -n auth
kubectl get pods -n knative-eventing
kubectl get pods -n knative-serving
kubectl get pods -n kubeflow
kubectl get pods -n kubeflow-user-example-com
----

== Validation

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Illustrate functionality with a demonstration.
// Begin with a description or outline of the demonstration.
// Provide clear steps (in ordered lists) for the reader to follow.
// Typical demonstration flow is:
// 1. Prepare the environment for the demonstration.
//    This should be minimal, such as downloading some data to use.
//    If this requires more than a couple steps, consider putting it
//    in a subsection.
// 2. Perform the demonstration.
//    Be careful not to overuse screenshots.
// 3. Verify.
//    This may be interwoven into performing the demonstration.
//
// As with Installation, leverage ordered lists, code blocks,
// admonitions, and screenshots.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =




[NOTE]

====
Need screenshot from Alex. Also, how exactly do we do this? The Kubeflow docs give two approaches: using port-forwarding and using NodePort/LoadBalancer/Ingress. Which is relevant to our case?
====




== Summary

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Summarize:
// - Motivation (1 sentence)
// - What was covered (1-2 sentences)
// - Next steps (unordered list of 2-4 further learning resources)
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Kubeflow is an important tool for deploying and scaling ML workloads in containers on Kubernetes clusters. SUSE Rancher faciliates deploying and managing Kubeflow at production scale. In this guide, we used Harvester to provision nodes for an RKE2 cluster and installed Kubeflow into that so it could be managed by Rancher.

To learn more about Rancher, see

To get started working with Kubeflow, refer to https://www.kubeflow.org/docs/[Kubeflow Docuemtation] for more details.







// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
