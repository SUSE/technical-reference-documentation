:docinfo:
include::./common_docinfo_vars.adoc[]

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// General comments
// Keep in mind that this is a "getting started" guide and the
//   audience that you are trying to reach.
// Leverage ASCIIDoc features to make this document readable and usable:
//   - Text highlights (follow SUSE style guides)
//   - Admonitions (i.e., NOTE, TIP, IMPORTANT, CAUTION, WARNING)
//   - Code blocks
//   - Lists (ordered and unordered, as appropriate)
//   - Links (to other resources)
//   - Images
//     - Place image files under the ./media directory tree
//       (e.g., ./media/src/svg, ./media/src/png)
//     - Format preference: svg > png > jpg
//     - Consolidate images wherever possible
//       (i.e., don't use two images when one conveys the message)
//   - Use sections and subsections to organize and group related
//     steps.
// 
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Document attributes and variables
//
// NOTES:
// 1. Update variables below and adjust docbook file accordingly.
// 2. Comment out any variables/attributes not used.
// 3. Follow the pattern to include additional variables.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// organization - do NOT modify
// -
:trd: Technical Reference Documentation
:type: Getting Started
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// document
// -
:title: Kubeflow with Rancher
:subtitle: Deploying Kubeflow onto an RKE2 cluster with Rancher

:product1: Rancher
:product1_full: Rancher by SUSE
:product1_version: 2.7
:product1_url: www.suse.com/solutions/enterprise-container-management/#rancher-product
:product2: RKE2
:product2_full: Rancher Kubernetes Engine 2
:product2_url: docs.rke2.io/
:product3: Harvester
:product3_full: Harvester by SUSE
:product3_url: www.suse.com/products/harvester/
:product4: SLES
:product4_full: SUSE Linux Enterprise
:product4_version: 15 SP4
:product4_url: www.suse.com/products/server/

:usecase: Simplify deployment of machine learning (ML) workflows on Kubernetes

:executive_summary: Kubeflow simplifies deployment of machine learning (ML) workflows on Kubernetes clusters.  This document provides step-by-step guidance for deploying Kubeflow on an {product2} cluster with {product1} in a {product3} hyperconverged infrastructure.

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// contributor
// specify information about authors, editors, and others here,
// then update docinfo file as appropriate
// -
:author1_firstname: Alex
:author1_surname: Arnoldy
:author1_jobtitle: Solutions Architect, Integrated Systems Solution
:author1_orgname: SUSE
:author2_firstname: Mark
:author2_surname: Gonnelly
:author2_jobtitle: Solutions Architect, GSI/IHV
:author2_orgname: SUSE
:author3_firstname: Terry
:author3_surname: Smith
:author3_jobtitle: Director, Global Partner Solutions
:author3_orgname: SUSE
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// miscellaneous
// define any additional variables here for use within the document
// -
:kubeflow_url: www.kubeflow.org/
:kubeflow_url_docs: www.kubeflow.org/docs/
:kubeflow_url_repo: github.com/kubeflow/
:k8s_ver: v1.24.9+rke2r2
:kustomize_ver: 5.0.0

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


= {title}: {subtitle}



== Introduction

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a brief statement (1-4 sentences) of the purpose of the guide.
// This is could be the same as the executive summary.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Machine learning (ML) is driving innovation across many domains.
The importance of ML is reflected by its adoption by the enterprise.
Businesses use ML to achieve deeper insights, grow capabilities, improve efficiencies, and accelerate results.

ML workflows can be complex, and managing them can be difficult.
This is particularly true at production scale.
https://{kubeflow_url}[Kubeflow] is an open source project that aims to make deploying and scaling ML models as simple as possible.
To achieve this, Kubeflow takes a cloud-native approach, deploying workloads in containers on https://kubernetes.io[Kubernetes] clusters.

https://{product1_url}[Rancher by SUSE] empowers organizations to unify their Kubernetes landscape with secure, streamlined management.
By deploying Kubeflow into a Rancher Kubernetes landscape, you can address the operational and security challenges of managing ML workflows at scale.

The modern, enterprise Kubernetes landscape leverages computing, storage, and networking resources in a variety of environments.
https://{product3_url}[Harvester by SUSE] is a next generation open source, hyperconverged infrastructure (HCI) solution designed for modern cloud-native environments.
Harvester provides operators with a cohesive platform to manage virtual machine and container workloads such as Kubeflow.


=== Scope

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Specify what this guide covers in no more than 2 sentences.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

This document provides guidance for deploying Kubeflow onto a {product2} Kubernetes cluster managed by {product1_full}.
This guide also illustrates provisioning virtual machines to host the {product2} cluster using {product3_full}.

After completing these steps, you are able to access the Kubeflow user interface.


=== Audience

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify for whom this document is intended, perhaps with:
//   - topics of interests (e.g., machine learning, security, etc.)
//   - job roles (e.g., developer, administrator, platform architect, etc.)
//   - required skills
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Data scientists, ML researchers, developers, and operational teams with responsibility for ensuring the success of ML projects.

To be successful with this guide, you should have basic knowledge of Kubernetes operations in a Rancher environment as well as standard Linux command line container management with Kubectl, Helm, and Kustomize.


== Technical overview

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a technical overview of the solution.
// - Identify components.
// - Describe how the components fit together.
//   Leverage diagrams as appropriate, including (but not limited to):
//   - component architecture
//   - data flow diagram
//   - workflow diagram
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

In this guide, you:

. Provision virtual machines for your Kubernetes cluster nodes with {product3_full}.

. Instantiate and configure a {product2} cluster with {product1_full}.

. Deploy Kubeflow onto your cluster.

. Access the Kubeflow user interface (UI).

. Enable TLS for secure access to Kubeflow.


image::Kubeflow-arch.svg[Solution Architecture, scaledwidth="75%", align="center"]

In this guide, {product3} is used to provision virtual machines for your RKE2 cluster nodes. This is not strictly necessary and Kubeflow can be deployed on any RKE2 cluster, regardless of how it is provisioned. For this reason, we will not discuss the creation of the Harvester cluster itself here. Instead, we will begin with the creation of the RKE2 cluster.


== Prerequisites

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify minimum requirements (prerequisites) the reader
// would need in order to follow the steps of this guide.
// - Link to existing resources whenever possible.
// - Keep this section brief but elaborate as needed.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

For this guide, you need the following:

* Management workstation
+
Your workstation must have access to the cluster environment and have these software tools:
+
--
* https://kubectl.docs.kubernetes.io/installation/kubectl/[Kubectl]

* https://helm.sh/docs/intro/install/[Helm]

* https://kubectl.docs.kubernetes.io/installation/kustomize/[Kustomize]
--
+
[NOTE]
====
A workstation running https://{product4_url}[{product4}] {product4_version} was used in the development of this guide.
====

* Cluster infrastructure
//
+
--
The minimum recommended cluster consists of five nodes as follows:

* Control plane and etcd pool nodes
//
+
Three nodes with vCPUs, 8 GB of RAM, and 40 GB storage volume.

* Workload nodes
//
+
Two nodes with 8 vCPUs, 16 GB of RAM, and 40 GB storage volume.

* Operating system
//
+
All nodes are configured with {product4} {product4_version} minimal, using the QCOW2 image with cloud-init enabled (previously known as the OpenStack image).
+
[IMPORTANT]
====
The SSH user for the operating system image is `sles`.
====

* Networking
//
+
All nodes are connected to a single VLAN with DHCP, DNS, and routing to the Internet.


[NOTE]
====
Node sizings used here are for basic testing purposes only.
To support a useful Kubeflow workload, you will likely need more vCPU cores and RAM, particularly on the workload nodes.
====

--


== Preparing the Cluster

=== Deploy an RKE2 cluster with Harvester

Begin by using the Rancher UI to create a project in Harvester named _kubeflow-on-harvester_ that contains the _kubeflow-cluster_ namespace.

. Login to the Harvester UI and go to the __Virtual Machines__ page.
. Choose the option to create either multiple VM instances.
. Set the namespace of your VMs to _kubeflow-cluster_.
. The VM Name is a required field.
. Configure the virtual machines' CPU and memory as defined above in the prerequisites.
. Select SSH keys or upload new keys.
. Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM.
. To configure networks, go to the Networks tab. The Management Network is added by default, you can remove it if the VLAN network is configured. You can also add additional networks to the VMs using VLAN networks. You may configure the VLAN networks on Advanced > Networks first. 
. Advanced options such as run strategy, os type and cloud-init data are optional. You may configure these in the Advanced Options section when applicable.
. Select the check-box to __Install guest agent__.


The following User Data cloud-config (under __Show Advanced__) should be applied to all nodes during RKE2 cluster creation:

[listing]
----
### cloud-init
#cloud-config
chpasswd:
  list: |
    root:SUSE
    sles:SUSE
  expire: false
ssh_authorized_keys:
  - >-
    <REPLACE WITH SSH PUBLIC KEY OF THE WORKSTATION>
runcmd:
  - SUSEConnect --url <REPLACE WITH RMT SERVER ADDRESS>
  - zypper -n in -t pattern apparmor
  - zypper -n up
  - zypper in --force-resolution --no-confirm --force kernel-default
  - zypper rm kernel-default-base
----

Configure the Kubernetes `Cluster Configuration` as follows:

. On the __Basic__ tab:

.. Select Kubernetes version {k8s_ver}.
+
[NOTE]
====
This version is currently deprecated but is needed for Harvester Cloud Provider support. If you are not using Harvester, you can select a newer version.
====

.. Enable the Harvester Cloud Provider CSI driver.

.. Set __Container Network__ Interface to `Calico`.

.. Ensure the Default Security Pod Policy is set to __Default - RKE2 Embedded__.

.. Leave Pod Security Admission Configuration Template set to __(None)__.

.. Disable the Nginx Ingress controller under __System Services__.

. On the __Labels and Annotations__ tab:

.. Apply a cluster label where they key is `app` and the value is `kubeflow`. 

. Click __Create__.


After the cluster has been created, verify and reboot the RKE2 nodes:

. SSH to each node as the user `sles`.

. Verify that the `kernel-default` kernel has been installed and `kernel-default-base` kernel has been removed: 
+
[source, console]
----
sudo zypper se kernel-default
----
+
[TIP]
====
After `kernel-default` has been installed, you can remove the `kernel-default-base` kernel with: 

[source, console]
----
sudo zypper rm kernel-default-base
----
====

. Verify that all operating system software has been patched to the latest update: 
+
[source, console]
----
sudo zypper up
----

. Reboot each node, in turn to enable the `kernel-default` kernel.

After the RKE2 cluster has been created, gather the KUBECONFIG data from the Rancher Management server and provide it to a workstation with Kubectl and Helm installed.


=== Configure the MetalLB load balancer

MetalLB is a network load balancer implementation for Kubernetes clusters on bare metal.
This allows you to create Kubernetes services of the type `LoadBalancer` to provide two important features:

* _address allocation_: MetalLB will take care of assigning individual IP addresses from configured address pools as services are launched and reclaiming those addresses when the services end.

* _external announcement_: MetalLB uses standard network or routing protocols to announce that the IP address is in use by a service.


==== Deploy the MetalLB Load Balancer

. Open a terminal on your workstation.

. Pull and apply the MetalLB manifests.
+
[source, console]
----
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml
# On first install only
kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
----

. Specify IP address ranges for default and reserved pools.
//
+
MetalLB assigns IP addresses to services from these pools.
+
--
.. Define the default IP pool.
+
MetalLB will automatically assign IP addresses from the default pool to services.
Specify the starting and ending IP address for this default pool by replacing the
addresses shown in the following commands with the appropriate addresses for your en-
vironment.
+
[source, console]
----
export DEFAULT_IP_RANGE_START=10.0.0.10
export DEFAULT_IP_RANGE_END=10.0.0.40
----

.. Define the reserved IP pool. Reserved IP addresses are not autoassigned by MetalLB.
+
[source, console]
----
export RESERVED_IP_RANGE_START=aa.bb.cc.dd
export RESERVED_IP_RANGE_END=ee.ff.gg.hh
----
--

. Create the MetalLB configuration file for layer 2 routing.
//
+
For other routing options, see https://metallb.universe.tf/configuration/#layer-2-configuration[MetalLB: Layer 2 Routing] and https://raw.githubusercontent.com/google/metallb/v0.9.3/manifests/example-config.yaml[MetalLB: Example Configuration].
+
[source, console]
----
cat <<EOF> metallb-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - ${DEFAULT_IP_RANGE_START}-${DEFAULT_IP_RANGE_END}
    - name: rsvd
      protocol: layer2
      auto-assign: false
      addresses:
      - ${RESERVED_IP_RANGE_START}-${RESERVED_IP_RANGE_END}
EOF
----

. Create the configmap. 
+
[source, console]
----
kubectl apply -f metallb-config.yaml
----

. Verify the configuration was applied correctly.
+
[source, console]
----
kubectl get configmap config -n metallb-system -o yaml
----
+
[IMPORTANT]
====
Be sure to verify the IP address pools.
====

. Verify the MetalLB load balancer is running. 
+
[source, console]
----
kubectl get all -n metallb-system
----


==== Validate MetalLB and the Harvester/Longhorn CSI

. Set the `NAMESPACE` variable to the target namespace.
+
[source, console]
----
NAMESPACE="kubeflow"
----

. Create the namespace. 
+
[source, console]
----
kubectl create namespace ${NAMESPACE}
----

. Create the manifest for an nginx pod, PVC, and load balancer service.
+
[source, console]
----
cat <<EOF> nginx-metallb-test.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: ${NAMESPACE}
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1
        ports:
        - name: http
          containerPort: 80
        volumeMounts:
        - mountPath: /mnt/test-vol
          name: test-vol
      volumes:
      - name: test-vol
        persistentVolumeClaim:
          claimName: nginx-pvc


---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-pvc
  namespace: ${NAMESPACE}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi


---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: ${NAMESPACE}
spec:
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  type: LoadBalancer
EOF
----

. Create the pod, service, and the PVC by applying the manifest. 
+
[source, console]
----
kubectl apply -f nginx-metallb-test.yaml
----

. Verify the pod is "Running", that the `harvester` StorageClass is the `(default)`, that the persistentvolumeclaim is `Bound`, and that the service has an external IP address.
+ 
[source, console]
----
kubectl get pod,sc,pvc,svc -n ${NAMESPACE}
----

. Verify that the service is reachable through the load balancer IP address from outside the cluster.
+
[source, console]
----
IPAddr=$(kubectl get svc -n ${NAMESPACE} | grep -w nginx | awk '{print$4":"$5}' | awk -F: '{print$1":"$2}')
curl http://${IPAddr} 2>/dev/null | grep "Thank you for using nginx"
----
An HTML encoded output should display the phrase "Thank you for using nginx."

. Verify that the volume is mounted in the test pod. 
+
[source, console]
----
TEST_POD=$(kubectl get pods -n ${NAMESPACE} | awk '/nginx/ {print$1}')
kubectl exec -it ${TEST_POD} -n ${NAMESPACE} -- mount | grep test-vol
----
+
The output should show that the volume is mounted at the location `/mnt/test-vol`.

. When finished with testing, delete the pod and service. 
+
[source, console]
----
kubectl delete -f nginx-metallb-test.yaml
----

== Install Kubeflow

=== Running the Installer

The Kubeflow Manifests Working Groups provides two options for installing Kubeflow:

* A single command that installs all components and targets ease of deployment for end users. 

* A multi-command approach that allows individual components to be included or excluded. 

In this guide, we will take the simpler approach and install everything. Please refer to the https://github.com/kubeflow/manifests#install-individual-components[Kubeflow Installation] documentation for more details of the alternative approach.

Assuming your workstation meets the prerequisites described above, then the following single command will download and install Kubeflow:
[source, console]
----
while ! kustomize build example | awk '!/well-defined/' | kubectl apply -f -; do echo "Retrying to apply resources"; sleep 10; done
----

Note that the `kubectl apply` command may fail on the first try. Hence the use of the while loop to retry until success is achieved. This method method is recommended by the Kubeflow documentation and has been confirmed to work at the time this document was written. However, the reader should review the latest Kubeflow Installation guide in case there have been updates.

=== Validating the Installation

After installation completes, it can take some time for all of the components to deploy their pods. To check that all Kubeflow-related pods are ready, you can use the following commands:
[source, console]
----
kubectl get pods -n cert-manager
kubectl get pods -n istio-system
kubectl get pods -n auth
kubectl get pods -n knative-eventing
kubectl get pods -n knative-serving
kubectl get pods -n kubeflow
kubectl get pods -n kubeflow-user-example-com
----

Next, you should enable kubectl port-forwarding and ensure the Kubeflow UI permits login:
[source, console]
----
kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80
----

NOTE: In the following step ensure the connection protocol is HTTP, not HTTPS.

* In a browser on the Linux workstation, connect to:
+
[source, console]
----
http://127.0.0.1:8080
----

* Login with the credentials: 
`Email address: user@example.com`
`Password: 12341234`

* Use `Ctrl+c` to close the kubectl port-forward session

=== Troubleshooting the Kubeflow installation

Some things that could prevent connecting or loggging into the Kubeblow dashboard include:

. The local copy of the https://github.com/kubeflow/manifests git repo doesn't match the origin

** While in the `manifests` directory, run `git status` to see if any files are different from the origin repo

** Remove the `manifests` directory and clone the repo again

. Using a web browser that is not running on the Linux desktop

** The kubectl port-forwarding opens a tunnel from the Linux workstation to the Kubeflow gateway service that only a web browser running on the same system can utilize.

. The Kubeflow installation has not completed or failed to complete

** Return to the beginning of this `Verify the Kubeflow installation` section and ensure all containers and pods are running correctly

** A high number of container restarts can indicate other issues preventing the installation from completing sucessfully

. The cluster's resources are saturated

** Use the Linux `top` command on the worker nodes to ensure the system's CPU/memory are not overburdened

** Check the Harvester dashboard to ensure the physical Harvester nodes are not overburdened or experiencing failures

== Configure TLS

In order to connect to Kubeflow, you need to setup HTTPS. This is because many of the Kubeflow components use Secure Cookies. This means that accessing Kubeflow with HTTP over a non-localhost domain (for example, via MetalLB) does not work. To enable this, we need to configure Istio to use the MetalLB load balancer and then enable HTTPS on the Kubeflow Istio Gateway. Obviously, we will need a certificate for this. For the purposes of this guide, we document the process by using a public Let's Encrypt certificate.

Let’s Encrypt provides certificates for free, but it has a challenge process to make sure you actually control the domain for which you are requesting a certificate.  One of the challenge methods is via DNS-01, which we can satisfy using AWS Route 53.  This is not the only way to handle this step for Let’s Encrypt (see https://letsencrypt.org/docs/challenge-types/ for other options) and obviously other certificate providers may be different.

=== Update Istio to use the MetalLB load balancer

Enable HTTPS on the Kubeflow Istio Gateway

* Verify the current `istio-ingressgateway` service type (Likely `ClusterIP`):
+
[source, console]
----
kubectl -n istio-system get svc istio-ingressgateway -o jsonpath='{.spec.type}' ; echo ""
----

* Patch the service to change the type to LoadBalancer:
+
[source, console]
----
kubectl -n istio-system patch svc istio-ingressgateway -p '{"spec": {"type": "LoadBalancer"}}'
----

* Verify the service is a type of `LoadBalancer` and take note of the IP address:
+
[source, console]
----
kubectl -n istio-system get svc istio-ingressgateway
----

=== Enable HTTPS on the Kubeflow Istio Gateway

* Edit the kubeflow-gateway resource to add HTTPS routing:
+
[source, console]
----
kubectl edit -n kubeflow gateways.networking.istio.io kubeflow-gateway
----

* Add this portion to the bottom of the `spec:` section:
+
[source, console]
----
    tls:
      httpsRedirect: false
  - hosts:
    - "*"
    port:
      name: https
      number: 443
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: kubeflow-certificate-secret
----

* The entire `spec:` section should look like this:
+
[source, console]
----
spec:
  selector:
    istio: ingressgateway
  servers:
  - hosts:
    - '*'
    port:
      name: http
      number: 80
      protocol: HTTP
    tls:
      httpsRedirect: false
  - hosts:
    - "*"
    port:
      name: https
      number: 443
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: kubeflow-certificate-secret
----

* Update the AWS Route53 DNS provider (or whatever DNS provider you use) wih the Kubeflow IP address and the desired Fully Qualified Domain Name for the Kubeflow UI.

* Use a browser to connect, with HTTP (not HTTPS), to the Kubeflow UI at the FQDN.

* The screen should redirect to dex and offer a login prompt.

* Login with the credentials:

`Email address`

[source, console]
----
user@example.com
----

`Password`

[source, console]
----
12341234
----

IMPORTANT: Proceed to the next section only after being able to connect to and log into the Kubeflow UI


=== Configure cert-manager to manage Let’s Encrypt certificates, using Route 53 DNS records

Kubeflow's cert-manager can manage certificates from any public provider. See the cert-manager documentation at https://cert-manager.io/docs/configuration/acme/ for more information. As discused above, we are going to use Let's Encrypt sertifactes validated through DNS-01 and AWS Route53.

NOTE: An AWS user with appropriate IAM policies and API access keys is needed for cert-manager to access the Route53 DNS records. See the cert-manager documentation at https://cert-manager.io/docs/configuration/acme/dns01/route53/ for more information.

* Set these variables:
+
[source, console]
----
# aws_access_key_id and aws_secret_access_key for the configured AWS user:
export AWS_ACCESS_KEY_ID=""
export AWS_SECRET_ACCESS_KEY=""
export AWS_REGION="" # E.g. "us-west-2"
export DNSZONE="" # E.g. "suse.com"
export FQDN="" # E.g. "kubeflow.suse.com"
export EMAIL_ADDR="" # valid email address for the Let's Encrypt certificate

----

NOTE: To avoid potential issues with rate limiting on issuing Let's Encrypt production TLS certificates, when initially creating the cert-manager Issuer, ensure the server:  https://acme-staging-v02  line is uncommented and the server: https://acme-v02  line is commented out. After verifying that the certicate can be issued correctly, we will reverse this to obtain the valid, production certificate

* Create the cert-manager Issuer file.
+
[source, console]
----
cat <<EOF> letsencrypt-issuer.yaml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: letsencrypt-issuer
  namespace: istio-system
spec:
  acme:
    email: ${EMAIL_ADDR}
    server: https://acme-staging-v02.api.letsencrypt.org/directory # Use this line to test the process of issuing a certificate to avoid the Let's Encrypt production rate limits
#    server: https://acme-v02.api.letsencrypt.org/directory # Use this line after the certificate issues correctly
    privateKeySecretRef:
      name: letsencrypt-issuer-priv-key # K8s secret that will contain the private key for this, specific issuer
    solvers:
    - selector:
        dnsZones: 
          - "${DNSZONE}"
      dns01:
        route53:
          region: ${AWS_REGION}
          accessKeyID: ${AWS_ACCESS_KEY_ID}
          secretAccessKeySecretRef:
            name: route53-credentials-secret
            key: secret-access-key
EOF
----

IMPORTANT: Review the letsencrypt-issuer.yaml file for accuracy before continuing

* Verify the update to the file.
+
[source, console]
----
cat letsencrypt-issuer.yaml
----


* Create the `letsencrypt-issuer` resource.
+
[source, console]
----
kubectl apply -f letsencrypt-issuer.yaml
----

* Create the Kubernetes secret containing the aws_secret_access_key for the AWS user.
+
[source, console]
----
kubectl create -n istio-system secret generic route53-credentials-secret --from-literal=secret-access-key=${AWS_SECRET_ACCESS_KEY}
----

* Verify the contents of the secret.
+
[source, console]
----
kubectl get -n istio-system secret route53-credentials-secret -o jsonpath={.data.secret-access-key} | base64 -d; echo ""
----

* Verify the hostname for the certificate resolves correctly.
+
[source, console]
----
getent hosts ${FQDN}
----

* Create the cert-manager Certificate resource file.
+
[source, console]
----
cat <<EOF> kubeflow-certificate.yaml 
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: kubeflow-certificate
  namespace: istio-system
spec: 
  secretName: kubeflow-certificate-secret # Kubernetes secret that will contain the tls.key and tls.crt of the new cert
  commonName: ${FQDN}
  dnsNames:
    - ${FQDN}
  issuerRef:
    name: letsencrypt-issuer
    kind: Issuer
EOF
----

* Verify the Certificate resource file.
+
[source, console]
----
cat kubeflow-certificate.yaml
----

* Create the Certificate resource.
+
[source, console]
----
kubectl apply -f kubeflow-certificate.yaml
----

* Check the status of the certificate.
+
[source, console]
----
kubectl get -w -n istio-system certificate
----

* Use `Ctrl+c` to exit the kubectl -w (watch) command


NOTE: The certificate commonly takes 100 seconds to be issued but can take up to three minutes. The `READY` status will change to `True` when it is issued.


* If needed, check the progress of the certificate:
+
[source, console]
----
kubectl describe -n istio-system certificate kubeflow-certificate
----

IMPORTANT: If the certificate seems to be taking a long time to be issued, review the cert-manager logs for clues. Common errors are related to DNS resolution, credentials, and IAM policies. Keep checking back for the status of the certificate since it will likely keep working in the background. 

* If needed, review the cert-manager logs:
+
[source, console]
----
kubectl logs -n cert-manager -l app=cert-manager
----

IMPORTANT: Proceed to the next section only after the certificate shows a `READY` status of `True` 


* Use a web browser to connect to Kubeflow UI with the Let's Encrypt Staging certificate

NOTE: Since the certificate was issued by the Let's Encrypt *Staging* servers, it will cause an error in the browser that it is untrusted. 

* Click the lock icon in the browser's URL pane, then continue selecting appropriate options until you are able to review the connection certificate. It should say that the certificate was issued by Let's Encrypt (Staging)

=== Update the configuration to use a Let’s Encrypt production certificate

* Update the letsencrypt-issuer.yaml file to comment out the `server: https://acme-staging-v02` line and uncomment the `server: https://acme-v02` line:

* Update the `letsencrypt-issuer` resource:
+
[source, console]
----
kubectl apply -f letsencrypt-issuer.yaml
----

* Remove the certificatate and its associated secret:
+
[source, console]
----
kubectl -n istio-system delete secret kubeflow-certificate-secret
kubectl -n istio-system delete certificate kubeflow-certificate
----

* Recreate the certificate:
+
[source, console]
----
kubectl apply -f kubeflow-certificate.yaml
----

* Check the status of the certificate.
+
[source, console]
----
kubectl get -w -n istio-system certificate
----

* Use `Ctrl+c` to exit the kubectl watch (-w) command


NOTE: The certificate can take up to three minutes to be issued, as indicated by the `READY` status becoming `True`


* Refresh the istio-gateway deployment to use the new certificate:
+
[source, console]
----
kubectl rollout restart deployment -n istio-system istio-ingressgateway
----

== Validation

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Illustrate functionality with a demonstration.
// Begin with a description or outline of the demonstration.
// Provide clear steps (in ordered lists) for the reader to follow.
// Typical demonstration flow is:
// 1. Prepare the environment for the demonstration.
//    This should be minimal, such as downloading some data to use.
//    If this requires more than a couple steps, consider putting it
//    in a subsection.
// 2. Perform the demonstration.
//    Be careful not to overuse screenshots.
// 3. Verify.
//    This may be interwoven into performing the demonstration.
//
// As with Installation, leverage ordered lists, code blocks,
// admonitions, and screenshots.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


* Close and reopen your browser to verify the publicly signed certificate is used at the Kubeflow UI's HTTPS URL.

* Login with the credentials:

`Email address`

[source, console]
----
user@example.com
----

`Password`

[source, console]
----
12341234
----

=== Optionally, set the kubeflow-gateway to redirect HTTP to HTTPS 

* Edit the kubeflow resource.
+
[source, console]
----
kubectl edit -n kubeflow gateways.networking.istio.io kubeflow-gateway
----
** Change `httpsRedirect: false` to `httpsRedirect: true`

* Confirm, by reloading your browser, that attempting to access the login page via HTTP results in a redirtection to HTTPS.


== Summary

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Summarize:
// - Motivation (1 sentence)
// - What was covered (1-2 sentences)
// - Next steps (unordered list of 2-4 further learning resources)
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Kubeflow is an important tool for deploying and scaling ML workloads in containers on Kubernetes clusters. SUSE Rancher faciliates deploying and managing Kubeflow at production scale. In this guide, we used Harvester to provision nodes for an RKE2 cluster and installed Kubeflow into that so it could be managed by Rancher.

To learn more about Rancher, see

To get started working with Kubeflow, refer to https://www.kubeflow.org/docs/[Kubeflow Docuemtation] for more details.







// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
