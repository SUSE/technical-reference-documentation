:docinfo:
include::./common_docinfo_vars.adoc[]

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// General comments
// Keep in mind that this is a "getting started" guide and the
//   audience that you are trying to reach.
// Leverage ASCIIDoc features to make this document readable and usable:
//   - Text highlights (follow SUSE style guides)
//   - Admonitions (i.e., NOTE, TIP, IMPORTANT, CAUTION, WARNING)
//   - Code blocks
//   - Lists (ordered and unordered, as appropriate)
//   - Links (to other resources)
//   - Images
//     - Place image files under the ./media directory tree
//       (e.g., ./media/src/svg, ./media/src/png)
//     - Format preference: svg > png > jpg
//     - Consolidate images wherever possible
//       (i.e., don't use two images when one conveys the message)
//   - Use sections and subsections to organize and group related
//     steps.
// 
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Document attributes and variables
//
// NOTES:
// 1. Update variables below and adjust docbook file accordingly.
// 2. Comment out any variables/attributes not used.
// 3. Follow the pattern to include additional variables.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// organization - do NOT modify
// -
:trd: Technical Reference Documentation
:type: Getting Started
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// document
// -
:product-bci: SLE BCI
:product-bci-full: SUSE Linux Enterprise Base Container Image
:product-bci-version: 15 SP5
:product-bci-url: registry.suse.com/bci/bci-base-15sp5/index.html
:product-rke2: RKE2
:product-rke2-full: Rancher Kubernetes Engine 2
:product-rke2-url: www.suse.com/products/rancher-kubernetes-engine/
:product-sle: SLES
:product-sle-version: 15
:product-sle-sp-version: SP5
:product-sle-sp-version-short: 5
:product-sle-full: SUSE Linux Enterprise Server
:product-sle-version: {product-bci-version}
:product-sle-url: https://www.suse.com/products/server/
:suse-customer-center-name: SUSE Customer Center
:product-rmt: RMT
:nvidia-name: NVIDIA
:nvidia-op: {nvidia-name} GPU Operator
:nvidia-url: https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html
:nvidia-drv: {nvidia-name} GPU driver
:nvidia-cuda: {nvidia-name} CUDA driver

:title: {nvidia-drv} With {product-bci-full} on {product-rke2-full}
:subtitle: Creating and deploying an {nvidia-drv} on {product-bci-full} on an {product-rke2} Kubernetes cluster with Helm
:usecase: {nvidia-name} GPU Driver based on SLE BCI

// :sle:bci-base_version: 15.5
:golang-version: 1.18
:cuda-version: 12.2.2
:gpu-driver-version: 535.104.05
:gpu-operator-version: v23.6.1

:nvidia-driver-download-url: https://www.nvidia.com/download/index.aspx
:nvidia-driver-gpu-operator-url: https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html
:nvidia-helm-url: https://helm.ngc.nvidia.com/nvidia
:nvidia-git-lab-url: https://gitlab.com/nvidia/container-images/driver
:nvidia-git-lab-directory: driver/sle15
:nvidia-license-image-registry: docker.io
//:build-variables-file: /tmp/.build-variables.sh

:rancher-kubectl-url: https://www.suse.com/c/rancher_blog/how-to-manage-kubernetes-with-kubectl/  
:rke2-cluster-access-url: https://docs.rke2.io/cluster_access
:podman-login-url: https://docs.podman.io/en/latest/markdown/podman-login.1.html

// :executive_summary: A brief statement of what this document provides (e.g., This document provides a brief introduction to implementing {usecase} with {product-rke2-full} and {product-bci-full}.)
:executive_summary: This document describes the process of creating an {usecase} and deploying it an {nvidia-op} on the {product-rke2-full} distribution of Kubernetes.
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// contributor
// specify information about authors, editors, and others here,
// then update docinfo file as appropriate
// -
:author1_firstname: Alex
:author1_surname: Arnoldy
:author1_jobtitle: Embedded Solutions Architect
:author1_orgname: SUSE Alliance Architects
//:author2_firstname: first (given) name
//:author2_surname: surname
//:author2_jobtitle: job title
//:author2_orgname: organization affiliation
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// miscellaneous
// define any additional variables here for use within the document
// -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


= {title}: {subtitle}



== Introduction

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a brief statement (1-4 sentences) of the purpose of the guide.
// This is could be the same as the executive summary.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

This Getting Started guide provides comprehensive instructions for two imporant tasks. 
The first is the creation of an OCI compliant container image based on the {product-bci-full}, that runs the {nvidia-drv}. 
The second is deploying the container image on {product-rke2-full}. 
The primary objective is to enable you to seamlessly integrate the {nvidia-op}, simplifying GPU management and support within Kubernetes clusters for GPU-intensive workloads. 

The choice of {product-bci-full} is motivated by the unparalleled security certifications and enhanced supportability it offers, particularly when operating heterogeneous software stacks. 
SUSE's {product-bci-full} is distinguished by a wide range of security certifications, including Common Criteria, FIPS, and EAL, making it a trusted choice for organizations with stringent security requirements.


== Scope

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Specify what this guide covers in no more than 2 sentences.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

//This guide will help you take the first steps to ...

This guide will help you:

* Build an OCI compliant container image that incorporate an appropriate {nvidia-drv} installed into a {product-bci-full}.
* Validate the functionality of that container image.
* Push the image to a central container image registry so that it can be accessed by a {product-rke2} Kubernetes cluster.
* Deploy the {nvidia-op} Helm chart to an {product-rke2} Kubernetes cluster in a way that leverages the {nvidia-drv} container image.
* Verify the Helm installation process completed correctly.

[TIP]
====
SUSE always recommends you use the most current Service Pack of {product-sle-full} that is available.
====

[IMPORTANT]
====
It is assumed that you are using Data Center class {nvidia-name} GPU(s). Integrating consumer grade GPUs is beyond the scope of this document.
====

=== Audience

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify for whom this document is intended, perhaps with:
//   - topics of interests (e.g., machine learning, security, etc.)
//   - job roles (e.g., developer, administrator, platform architect, etc.)
//   - required skills
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// This document is intended for ...

This guide is intened for an audience comprising Kubernetes administrators, proficient DevOps practitioners, and application developers. 
It assumes a foundational understanding of Podman and/or Docker, Kubernetes, and {nvidia-name} GPU technologies. 
This guide should be suitable for most high technology professionals seeking to unlock the full potential of their GPU accellerated containerized applications.

== Prerequisites

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify minimum requirements (prerequisites) the reader
// would need in order to follow the steps of this guide.
// - Link to existing resources whenever possible.
// - Keep this section brief but elaborate as needed.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

.Before embarking on the procedures outlined in this guide, you should ensure the following prerequisites have been met

* You have access to a {product-sle}{product-bci-version} build host. 
You should ensure the build host is the same Service Pack (e.g. {product-bci-version}) version as the {product-bci} image to be used. 

[TIP] 
====
The build host does not require access to an {nvidia-name} GPU.
====

* The {product-sle} Containers Module has been enabled on the build host and the {product-sle} Containers Module plus the {nvidia-name} Compute Module have been enabled on all Kubernetes worker nodes. 
You can use the following commands to enable the appropriate software modules. on each host/node.
//
+
[NOTE]
====
Change the variable SLE15_SP_VERSION in the following command to match the service pack release of your {product-sle-full} Kubernetes worker nodes.
====
//
+
[console, bash]
----
export SLE15_SP_VERSION=5
sudo SUSEConnect -p sle-module-containers/15.${SLE15_SP_VERSION}/x86_64
sudo SUSEConnect -p sle-module-desktop-applications/15.${SLE15_SP_VERSION}/x86_64
sudo SUSEConnect -p sle-module-development-tools/15.${SLE15_SP_VERSION}/x86_64
sudo SUSEConnect -p sle-module-NVIDIA-compute/15/x86_64
----

* You have installed the required {product-sle-full} {nvidia-name} software packages on each Kubernetes worker node that is configured with an {nvidia-name} GPU. 
You can use the following commands to install the appropriate software.
//
+
[console, bash]
----
sudo zypper install \
  kernel-firmware-nvidia \
  libnvidia-container-tools \
  libnvidia-container1 \
  nvidia-container-runtime \
  sle-module-NVIDIA-compute-release
----

* Podman is installed on the build host and at least one of the {nvidia-name} GPU equiped Kubernetes worker nodes. 
You can use the following command to install Podman.
//
+
[console, bash]
----
sudo zypper install podman
----

* The git utility is installed on the build host. 
You can use the following command to install git.
//
+
[console, bash]
----
sudo zypper install git-core
----

* You have access to a container image registry that is available to the build host AND the target Kubernetes cluster. 
This will allow the {nvidia-drv} to be deployed across all Kubernetes worker nodes that have {nvidia-name} GPUs, when installing the {nvidia-op} Helm chart.
//
+
[NOTE]
====
The container registry does not need to support authenication, but should be configured with a valid TLS certificate.
====

* You have access to a Kubernetes cluster that is equipped and correctly configured with Data Center class {nvidia-name} GPUs. 
These instructions leverage SUSE's security focused Kubernetes distribution, {product-rke2}. 

* You possess a basic familiarity with Podman and/or Docker, Kubernetes, and {nvidia-name} GPU concepts.


== Technical overview

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a technical overview of the solution.
// - Identify components.
// - Describe how the components fit together.
//   Leverage diagrams as appropriate, including (but not limited to):
//   - component architecture
//   - data flow diagram
//   - workflow diagram
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// Resources:
// https://gitlab.com/rdoxenham/driver/-/tree/main/sle15
// https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html#suse15
// https://docs.asciidoctor.org/asciidoc/latest/verbatim/source-blocks/
// https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#vmware-vsphere-with-tanzu
// https://docs.nvidia.com/grid/12.0/index.html

The process of building the OCI compliant {nvidia-drv} container image using {product-bci} and deploying it to an {product-rke2} cluster will leverage the following primary components:
//
+

* A build host. This can be an {product-rke2} worker node or a separate node or VM with, or without access to an {nvidia-name} GPU.
** If the build host has access to an {nvidia-name} GPU, the specified validations can be done on that node.
* A container image registry to which the build host and the {product-rke2} cluster can access.
* An {product-rke2} Kubernetes cluster with at least one worker node that is confgiured with an {nvidia-name} GPU. 
* Access to the {product-sle-full} software modules through the {suse-customer-center-name} or an {product-rmt} server.

The process of building the OCI compliant container image and deploying it to an {product-rke2} cluster will entail the following steps:
//
+

. Build the container image
. Validate the state of the image
. Save it to a container image registry so it can be accessed by the {product-rke2} cluster
. Deploy the entire {nvidia-op} Helm chart, including the {nvidia-drv} image to the {product-rke2} cluster
. Validate the state of the {nvidia-op} installation


== Build the {product-bci} container image with {nvidia-drv}

This section offers a detailed explanation of the steps required to create OCI compliant container images with the {nvidia-op} inside the {product-bci-full}.

[IMPORTANT] 
====
The following commands must be run on the {product-sle-full} build host.
====

. [[step_1]]Set the following variables that will be consumed throughout this procedure:
//
+
.. Create the /tmp/.build-variables.sh file:
//
+
[TIP]
====
For the REGISTRY variable, provide the URL of the registry where the new image will be saved.

For the SLE15_SP_VERSION, enter the number for the {product-sle} {product-sle-version} service pack to be used for the container image (i.e. {product-sle-sp-version-short} for {product-sle-sp-version}).

For the DRIVER_VERSION variable, provide the {nvidia-drv} version (e.g. {gpu-driver-version}). Find the latest "Data Center Driver for Linux x64" version for your GPU at {nvidia-driver-download-url}.

For the OPERATOR_VERSION variable, provide the {nvidia-op} version (e.g.{gpu-operator-version}). Find the associated {nvidia-op} version at {nvidia-driver-gpu-operator-url}.

For the CUDA_VERSION variable, provide the required CUDA version (e.g. {cuda-version}) for {nvidia-drv} by selecting your driver version at {nvidia-driver-gpu-operator-url}. The CUDA version will be listed under "Software Versions".
====
//
+
[console, bash]
----
cat <<EOF> /tmp/.build-variables.sh
export REGISTRY=""
export SLE15_SP_VERSION=""
export DRIVER_VERSION=""
export OPERATOR_VERSION=""
export CUDA_VERSION=""
EOF
----
//
+
.. Edit the /tmp/.build-variables.sh file to provide the appropriate values.
//
+
.. Source the variables into your current terminal environment:
//
+
[TIP]
====
Disconnecting from your current terminal environment will cause the variables to be lost. 
Repeat the following step to set the variables again.
====
//
+
[IMPORTANT]
====
After setting and sourcing the variables, you will be able to copy and paste the entire code blocks from this document to the command line and execute them without editing. 
For the best results, be sure and execute each code block in its entirety as a single command.
====
//
+
[console, bash]
----
source /tmp/.build-variables.sh
----
//
+
. Clone the {nvidia-name} GitLab repository and change to the `{nvidia-git-lab-directory}` directory:
//
+
[console, subs="attributes"]
----
git clone {nvidia-git-lab-url} && cd {nvidia-git-lab-directory}
----
//
+
. Update the Dockerfile in this directory:
//
+
. Make a backup copy of the Dockerfile before modifying:
//
+
[console, bash]
----
cp Dockerfile /tmp/Dockerfile.orig
----
//
+
. Update the golang build container image to version `{golang-version}`:
//
+
[console, subs="attributes"]
----
sed -i '/^FROM/ s/golang\:1\.../golang\:{golang-version}/' Dockerfile
----
//
+
. Update the Dockerfile's base container image to the {product-bci-version} BCI:
//
+
[console, bash]
----
sed -i '/^FROM/ s/suse\/sle15/bci\/bci-base/' Dockerfile
----
//
+
. Verify the changes that have been made to the Dockerfile:
//
+
[console, bash]
----
diff /tmp/Dockerfile.orig Dockerfile
----
//
+
. Build the {product-sle} container image with the {nvidia-drv} installed:
//
+
* Build the container:
//
+
[IMPORTANT] 
====
When building the container image, you may be prompted for the registry that contains the `nvidia/cuda` image. 
If so, select the image located in {nvidia-license-image-registry}.
====
//
+
[NOTE] 
====
If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing.
====
//
+
[console, bash]
----
## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}
CUDA_VERSION=${CUDA_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && 

## Build the container
sudo podman build -t \
${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION} \
  --build-arg SLES_VERSION="15.${SLE15_SP_VERSION}" \   
  --build-arg DRIVER_ARCH="x86_64" \     
  --build-arg DRIVER_VERSION="${DRIVER_VERSION}" \     
  --build-arg CUDA_VERSION="${CUDA_VERSION}" \
  --build-arg PRIVATE_KEY=empty  \
.
----
//
+
* Watch the build output for errors, warnings, and failures. You can safely ignore errors and warnings that don't stop the build process.
//
+
* The build process should finish with a message saying that the final image was committed and tagged. 
For example:
//
+
[listing]
====
 COMMIT registry.susealliances.com/nvidia-sle15sp5-535.104.05
--> cf976870489
Successfully tagged registry.susealliances.com/nvidia-sle15sp5-535.104.05:latest
cf9768704892c4b8b9e37a4ef591472e121b81949519204811dcc37d2be9d16c
====
//
+
. Remove the intermediate build container image that was created as part of the build process (and any other leftover artifacts):
//
+
[console, bash]
----
for EACH in $(sudo podman images | awk '/none/ {print$3}'); do sudo podman rmi ${EACH}; done
----
//
+
. Push the newly build image to the container registry:
//
+
[IMPORTANT] 
====
If the target container registry requires authentication, use the `Podman login` command to successfully authenticate before continuing. 
See {podman-login-url} for more information.
====
//
+
[NOTE] 
====
If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing.
====
//
+
[console, bash]
----
## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && 

## Tag the image with the format that Helm will need when deploying on Kubernetes
sudo podman tag ${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION} ${REGISTRY}/driver:${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} &&
## Push the image (with both tags) to the container registry
sudo podman push ${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION} &&
sudo podman push ${REGISTRY}/driver:${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION}
----
//
+
* Verify the image is saved in the registry, and remotely available:
//
+
[NOTE] 
====
If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing.
====
//
+
[console, bash]
----
sudo podman search --list-tags ${REGISTRY}/driver:${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION}
----
//
+
. Validate the container image
//
+
[NOTE] 
====
This step is optional and requires running the newly created {nvidia-drv} container locally with `Podman`, outside of the Kubernetes context. 
This can be done on a {product-bci-version} host configured with the same kind of {nvidia-name} GPU the container was created for, or on a Kuberentes worker node that is configured with an {nvidia-name} GPU.
====
//
+
* Open a command line session to the host or Kubernetes worker node on which you will test the container image.
//
+
* Create the `/run/nvidia` directory, if it does not yet exist:
//
+
[console, bash]
----
sudo mkdir -p /run/nvidia
----
//
+
* Run the {nvidia-drv} container locally:
//
+
[NOTE] 
====
If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing.
====
//
+
[console, bash]
----
## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Run the container image 
sudo podman run -d \
  --name driver.${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} \
  --privileged \
  --pid=host \
  -v /run/nvidia:/run/nvidia:shared \
  -v /var/log:/var/log \
  --restart=unless-stopped \
${REGISTRY}/driver:${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION}
----
//
+
* Verify the container is running:
//
+
[console, bash]
----
sudo podman ps -a
----
//
+
** The container's `STATUS` field should show that it is "Up" and the amount of time it has been up should increment with repeated runs of the command.
//
+
* Monitor the deployment of the {nvidia-drv}:
//
+
[NOTE] 
====
If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing.
====
//
+
[console, bash]
----
## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Review the standard output of the running container
sudo podman logs -f driver.${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} 
----
//
+
* The deployment process is complete when the following message is shown:
//
+
[listing]
====
Mounting {nvidia-name} driver rootfs...
Done, now waiting for signal
====
//
+
* Press `Ctrl+c` to close the log viewing session
//
+
* Ensure the {nvidia-name} kernel modules have been loaded:
//
+
[console, bash]
----
sudo lsmod | grep nvidia
----
//
+
** You should see modules such as `nvidia`, `nvidia_modeset`, and `nvidia_uvm`
//
+
* Verify the `nvidia-smi` utility can communicate withe the GPU:
//
+
[NOTE] 
====
If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing.
====
//
+
[console, bash]
----
## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Verify the nvidia-smi utitlity can communicate with the GPU
sudo podman exec -it driver.${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} nvidia-smi
----
//
+
* When ready to move forward, stop and remove the Podman container:
//
+
[NOTE] 
====
If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing.
====
//
+
[console, bash]
----
## Validate the variables before using them in the subsequent command
echo &&
echo "
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && 

## Stop and remove the container instance
sudo podman stop driver.${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} &&
sudo podman rm driver.${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} 
----
//
+
* Before continuing to the Kubernetes deployment procedure, ensure the {nvidia-name} kernel modules are not loaded on any of the {nvidia-name} GPU equipped Kuberenetes worker nodes:
//
+
[console, bash]
----
sudo lsmod | grep nvidia
----
//
+
** You should receive no output.
//+
[NOTE]
====
If you see any modules containing the name `nvidia`, use the command `sudo modprobe -r <module name>` to unload them. 
If any modules fail to unload, reboot the node.
====

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Detail the steps of the installation procedure.
// The reader should be able to copy and paste commands to
// a local environment or follow along locally with screenshots.
// Include one or more verification steps to validate installation.
//
// Leverage:
// - Ordered lists
// - Code blocks
// - Screenshots
// - Admonitions
//
// If multiple installation methods are to be detailed, then
// - Create a summary list here
// - Detail each method in its own subsection.
//
// NOTE: For solutions involving SUSE Rancher, it is preferred
//       to detail two installation methods:
//       - Through the Rancher Apps Catalog with appropriate
//         screenshots and SUSE branding.
//       - A more manual approach (e.g., on the command-line).
//
// Complex configuration procedures may be broken out into one or more
// Configuration sections.
// These may be subsections of Installation or separate sections at
// the same level as Installation.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

== Deployment to a Kubernetes {product-rke2} cluster


[NOTE] 
====
The preferred method for installing the {nvidia-op} is with the Helm Kuberenetes package manager.
====

[IMPORTANT] 
====
The following steps must be run from a Linux system that has the kubectl and Helm (version 3) utilities, as well as the KUBECONFIG file for the target Kubernetes cluster available to it. 
See these documents for more information: {rancher-kubectl-url} and {rke2-cluster-access-url}.

In addition, if the container build host is a different system than the one being used to perform the Helm install, the /tmp/.build-variables.sh file will need to be created on the second system. 
Return to <<step_1, Step 1>> in the proceeding procedure before continuing.
====


* Add the {nvidia-name} helm software repository:
//
+
[console, subs="attributes"]
----
helm repo add {nvidia-helm-url}
helm repo update
----

* Deploy the {nvidia-op} with Helm:
//
+
[NOTE]
====
If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in the proceeding procedure before continuing.
====
//
+
// [console, subs="attributes"]
[console, bash]
----
## Verify the selected cluster before deploying
echo &&
echo "Cluster name: $(kubectl config current-context)" && 
echo "" && 
kubectl get nodes -o wide && 
echo "" && 
read -n1 -p "Is this the target Kuberentes cluster for the Helm chart? (y/n) " YESNO && 
echo "" && 
[ ${YESNO} != y ] && { echo "Exiting."; echo ""; exit; } || echo "" && 

## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
OPERATOR_VERSION=${OPERATOR_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && 

## Deploy the Helm chart
helm install -n gpu-operator \
  --generate-name \
  --wait \
  --create-namespace \
  --version=${OPERATOR_VERSION} \
    nvidia/gpu-operator \
  --set driver.repository=${REGISTRY} \
  --set driver.version=${DRIVER_VERSION} \
  --set operator.defaultRuntime=containerd \
  --set toolkit.env[0].name=CONTAINERD_CONFIG \
  --set toolkit.env[0].value=/var/lib/rancher/rke2/agent/etc/containerd/config.toml \
  --set toolkit.env[1].name=CONTAINERD_SOCKET \
  --set toolkit.env[1].value=/run/k3s/containerd/containerd.sock \
  --set toolkit.env[2].name=CONTAINERD_RUNTIME_CLASS \
  --set toolkit.env[2].value=nvidia \
  --set toolkit.env[3].name=CONTAINERD_SET_AS_DEFAULT \
  --set-string toolkit.env[3].value=true
----

* Verify the {nvidia-op}, {nvidia-drv} and associated elements have been deployed correctly with the command: 
//
+
[console, bash]
----
kubectl get pods -n gpu-operator
----
//
+
** The output should be similar to the following:
//
+
[listing]
====
NAME                                                          READY   STATUS      RESTARTS   AGE
gpu-feature-discovery-crrsq                                   1/1     Running     0          60s
gpu-operator-7fb75556c7-x8spj                                 1/1     Running     0          5m13s
gpu-operator-node-feature-discovery-master-58d884d5cc-w7q7b   1/1     Running     0          5m13s
gpu-operator-node-feature-discovery-worker-6rht2              1/1     Running     0          5m13s
gpu-operator-node-feature-discovery-worker-9r8js              1/1     Running     0          5m13s
nvidia-container-toolkit-daemonset-lhgqf                      1/1     Running     0          4m53s
nvidia-cuda-validator-rhvbb                                   0/1     Completed   0          54s
nvidia-dcgm-5jqzg                                             1/1     Running     0          60s
nvidia-dcgm-exporter-h964h                                    1/1     Running     0          60s
nvidia-device-plugin-daemonset-d9ntc                          1/1     Running     0          60s
nvidia-device-plugin-validator-cm2fd                          0/1     Completed   0          48s
nvidia-driver-daemonset-5xj6g                                 1/1     Running     0          4m53s
nvidia-mig-manager-89z9b                                      1/1     Running     0          4m53s
nvidia-operator-validator-bwx99                               1/1     Running     0          58s
====



== Validation

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Illustrate functionality with a demonstration.
// Begin with a description or outline of the demonstration.
// Provide clear steps (in ordered lists) for the reader to follow.
// Typical demonstration flow is:
// 1. Prepare the environment for the demonstration.
//    This should be minimal, such as downloading some data to use.
//    If this requires more than a couple steps, consider putting it
//    in a subsection.
// 2. Perform the demonstration.
//    Be careful not to overuse screenshots.
// 3. Verify.
//    This may be interwoven into performing the demonstration.
//
// As with Installation, leverage ordered lists, code blocks,
// admonitions, and screenshots.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

[NOTE]
====
The {nvidia-op} Helm chart provides two pods that validate the state of the installed software.
====

* Validate the state of the {nvidia-op} software:


[console, bash]
----
kubectl logs -n gpu-operator -l app=nvidia-operator-validator
----

** The output should be similar to:

[listing]
====
Defaulted container "nvidia-operator-validator" out of: nvidia-operator-validator, driver-validation (init), toolkit-validation (init), cuda-validation (init), plugin-validation (init)
*all validations are successful*
====

* Validate the state of the {nvidia-cuda} software:

[console, bash]
----
kubectl logs -n gpu-operator -l app=nvidia-cuda-validator
----

** The output should be similar to the following:

[listing]
====
Defaulted container "nvidia-cuda-validator" out of: nvidia-cuda-validator, cuda-validation (init)
*cuda workload validation is successful*
====

* To validate that the {nvidia-drv} is communicating with the GPU, you can run this command to view the statics of the Kubernetes workers that are configured with GPUs:
//
+
[console, bash]
----
kubectl exec -it \
"$(for EACH in \
$(kubectl get pods -n gpu-operator \
-l app=nvidia-driver-daemonset \
-o jsonpath={.items..metadata.name}); \
do echo ${EACH}; done)" \
-n gpu-operator \
nvidia-smi
----
//
+
[NOTE]
====
This command can also be used to verify which application processes are running on the {nvidia-name} GPUs, and how many resources are being consumed.
====

== Summary

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Summarize:
// - Motivation (1 sentence)
// - What was covered (1-2 sentences)
// - Next steps (unordered list of 2-4 further learning resources)
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// In this guide, you learned ...

This guide has effectively steered the creation of OCI compliant container images leveraging the {product-bci-full} and incorporating the {nvidia-drv}. 
Furthermore, it has provided coherent instructions for validating the functionality of the container image and the seamless deployment of the image within a Kubernetes cluster, specifically {product-rke2}. 
This integrated solution is aimed at affording containerized applications GPU-acceleration, while avoiding the need to manage additional software on each GPU equiped node.
The strategic choice of SUSE's {product-bci-full} as the foundation for this integration underscores an organization's commitment to security, certifications, and supportability. 
Organizations with exacting security requirements depend on {product-sle-full}'s numerous certifications such as Common Criteria, FIPS, and EAL. 
Additionally, SUSE's commitment to providing robust support for heterogeneous software stacks guarantees customer's the freedom to design their IT landscape to suit their unique business challenges. 

A pivotal point to underscore is the indispensability of a Kubernetes cluster, preferably {product-rke2}, that provides full {nvidia-name} GPU support and the requisite {nvidia-op} to fully harness GPU resources when deploying GPU intensive workloads.



// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

// vim: set syntax=asciidoc:

//end
