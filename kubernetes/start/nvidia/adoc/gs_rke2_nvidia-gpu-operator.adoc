:docinfo:
include::./common_docinfo_vars.adoc[]

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// General comments
// Keep in mind that this is a "getting started" guide and the
//   audience that you are trying to reach.
// Leverage ASCIIDoc features to make this document readable and usable:
//   - Text highlights (follow SUSE style guides)
//   - Admonitions (i.e., NOTE, TIP, IMPORTANT, CAUTION, WARNING)
//   - Code blocks
//   - Lists (ordered and unordered, as appropriate)
//   - Links (to other resources)
//   - Images
//     - Place image files under the ./media directory tree
//       (e.g., ./media/src/svg, ./media/src/png)
//     - Format preference: svg > png > jpg
//     - Consolidate images wherever possible
//       (i.e., don't use two images when one conveys the message)
//   - Use sections and subsections to organize and group related
//     steps.
// 
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Document attributes and variables
//
// NOTES:
// 1. Update variables below and adjust docbook file accordingly.
// 2. Comment out any variables/attributes not used.
// 3. Follow the pattern to include additional variables.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// organization - do NOT modify
// -
:trd: Technical Reference Documentation
:type: Getting Started
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// DOCUMENT REVISION DATE
//-
:revision-date: 2023-12-04
:docdate: {revision-date}
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// components
// -
:rke2: RKE2
:rke2-full: Rancher Kubernetes Engine 2
:rke2-url: https://www.suse.com/products/rancher-kubernetes-engine/
:rke2-support-matrix: https://www.suse.com/suse-rke2/support-matrix/all-supported-versions/
:rke2-docs: https://docs.rke2.io/
:rke2-version: 1.25

:sles: SLES
:sles-full: SUSE Linux Enterprise Server
:sles-version: 15
:sles-sp: 5
:sles-sp-full: SP{sles-sp}
:sles-url: https://www.suse.com/products/server/
:sles-docs: https://documentation.suse.com/sles/{sles-version}-{sles-sp-full}
:sles-docs-modules: {sles-docs}/single-html/SLES-modules/#sec-modules-install

:sle-bci: SLE BCI
:sle-bci-full: SUSE Linux Enterprise Base Container Images
:sle-bci-version: {sles-version {sles-sp-full}
:sle-bci-registry: https://registry.suse.com
:sle-bci-url: {sle-bci-registry}/bci/bci-base-{sles-version}sp{sles-sp}/index.html

:golang-version: 1.21
:golang-base: 121
:sle-bci-golang-url: {sle-bci-registry}/bci/golang{golang-base}

:scc-full: SUSE Customer Center
:scc-url: https://scc.suse.com

:suse-container-docs: https://documentation.suse.com/container/all/single-html/Container-guide/

:rancher-url: https://www.suse.com/solutions/enterprise-container-management/#rancher-product
:rancher-docs: https://ranchermanager.docs.rancher.com
:rancher-docs-rke2: {rancher-docs}/pages-for-subheaders/launch-kubernetes-with-rancher#rke2

:rmt: RMT
:rmt-docs: {sles-docs}/single-html/SLES-rmt/

:nvidia-org: NVIDIA
:nvidia-url: https://nvidia.com
:nvidia-dc-url: {nvidia-url}/en-us/data-center/
:nvidia-drv: {nvidia-org} GPU Driver
:nvidia-drv-version: 535.104.05
:nvidia-drv-url: https://www.nvidia.com/download/index.aspx
:nvidia-op: {nvidia-org} GPU Operator
:nvidia-op-version: v23.6.1
:nvidia-op-url: https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html
:nvidia-op-platforms-url: https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html
:nvidia-cuda: {nvidia-org} CUDA Toolkit
:nvidia-cuda-url: https://developer.nvidia.com/cuda-toolkit
:nvidia-cuda-version: 12.2.2
:nvidia-helm-url: https://helm.ngc.nvidia.com/nvidia
:nvidia-gitlab-url: https://gitlab.com/nvidia
:nvidia-gitlab-drv-url: {nvidia-gitlab-url}/container-images/driver
:nvidia-gitlab-drv-dir: sle{sles-version}
:nvidia-license-image-registry: docker.io


//:build-variables-file: /tmp/.build-variables.sh

:rancher-kubectl-url: https://www.suse.com/c/rancher_blog/how-to-manage-kubernetes-with-kubectl/  
:rke2-cluster-access-url: https://docs.rke2.io/cluster_access
:podman-login-url: https://docs.podman.io/en/latest/markdown/podman-login.1.html



:title: Creating and Deploying {nvidia-org} GPU Enabled Containers
:subtitle: Create {nvidia-org} GPU-enabled containers with {sles-full} and deploy them on {rke2-full}
:usecase: enable GPU acceleration for containerized workloads
:executive_summary: This guide describes a process for building {nvidia-drv} enabled container images with {sle-bci-full} and deploying these containers with the {nvidia-op} to {rke2-full} clusters.
:description: Create and deploy {nvidia-org} GPU-enabled containers with {sles-full} and {rke2-full}
:description-short: GPU-enabled containers with SUSE
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// contributor
// specify information about authors, editors, and others here,
// then update docinfo file as appropriate
// -
:author1_firstname: Alex
:author1_surname: Arnoldy
:author1_jobtitle: Global Alliance Solutions Architect
:author1_orgname: SUSE
:contrib1_firstname: Rhys
:contrib1_surname: Oxenham
:contrib1_jobtitle: Senior Director of Field PM & Engineering - Edge
:contrib1_orgname: SUSE
//:author2_firstname: first (given) name
//:author2_surname: surname
//:author2_jobtitle: job title
//:author2_orgname: organization affiliation
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// miscellaneous
// define any additional variables here for use within the document
// -

:git-url: https://git-scm.com/
:oci-url: https://opencontainers.org/

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


= {title}: {subtitle}



== Introduction

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a brief statement (1-4 sentences) of the purpose of the guide.
// This is could be the same as the executive summary.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Graphical Processor Units (GPUs) provide unique capabilities for accelerating a broad spectrum of workloads.
These workloads are increasingly being deployed in containers on Kubernetes clusters.
In this guide, you learn how to:

. create OCI-compliant container images with {sle-bci-full} that contain the {nvidia-drv}

. deploy a GPU-enabled container image on {rke2-full} with the {nvidia-op}

{sle-bci-full} ({sle-bci}) are a trusted choice for organizations with stringent security requirements, providing unparalleled security certifications (including Common Criteria, FIPS, and EAL) and enhanced supportability even when operating heterogeneous software stacks.


== Scope

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Specify what this guide covers in no more than 2 sentences.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

This guide covers the following:

* building an OCI-compliant container image on {sle-bci-full} that includes the {nvidia-drv}

* validating the GPU-enabled container image

* publishing the image to a container image registry

* deploying the container image with the {nvidia-op} Helm chart to a {rke2} cluster

* verifying the container deployment


[IMPORTANT]
====
This guide assumes that you are using Data Center class {nvidia-org} GPUs. Integrating consumer grade GPUs is outside the scope of this document.
====


=== Audience

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify for whom this document is intended, perhaps with:
//   - topics of interests (e.g., machine learning, security, etc.)
//   - job roles (e.g., developer, administrator, platform architect, etc.)
//   - required skills
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

This guide is intended for high technology professionals (including Kubernetes administrators, proficient DevOps practitioners, and application developers) seeking to unlock the full potential of their GPU-accelerated containerized applications.

To be successful, you need a foundational understanding of Podman or Docker, Kubernetes, and {nvidia-org} GPU technologies.


=== Acknowledgements

This guide builds on the work by {contrib1_firstname} {contrib1_surname}, {contrib1_orgname} {contrib1_jobtitle}.


== Prerequisites

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify minimum requirements (prerequisites) the reader
// would need in order to follow the steps of this guide.
// - Link to existing resources whenever possible.
// - Keep this section brief but elaborate as needed.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Before you embark on the procedures outlined in this guide, ensure that your environment is ready with the resources listed here.

* *Build host*
//
+
Your build host is the computer system (physical or virtual) on which you build your OCI-compliant container.
+
[NOTE]
====
The build host does not require access to an {nvidia-org} GPU.
====
+
Your build host should have the following:
+
--
. Operating system
//
+
This guide is developed on a build host running {sles-full} {sles-version} {sles-sp-full}.
//
+
It is strongly recommended that your build host run the same operating system and version as the {sle-bci-full} you will use for your container image.
You can find available images at the {sle-bci-registry}[SUSE Container Images Registry].
+
[IMPORTANT]
====
In this guide, you leverage the {sle-bci} Go development image, as this base image already includes the Go development tools.
Be sure to note the available Go version, as this is needed during the build process.
Go {golang-version} is used in this guide, but updated images are made available frequently.
====

. Containers, Desktop Applications, and Development Tools modules
//
+
Enable these {sles-full} {sles-docs-modules}[modules]:
+
[source, console, subs="attributes+"]
----
VER={sles-version}
SP={sles-sp}
sudo SUSEConnect -p sle-module-containers/$\{VER\}.$\{SP\}/x86_64
sudo SUSEConnect -p sle-module-desktop-applications/$\{VER\}.$\{SP\}/x86_64
sudo SUSEConnect -p sle-module-development-tools/$\{VER\}.$\{SP\}/x86_64
----
+
[NOTE]
====
Be sure to update the `VER` and `SP` variables with the {sles} version and service pack that you are using.
====

. Podman
//
+
{suse-container-docs}#container-podman[Podman] is the Pod Manager Tool that is a daemonless container engine for managing {oci-url}[Open Container Initiative (OCI)] containers.
//
+
Install Podman:
+
[source, console]
----
sudo zypper install podman
----

. Git
//
{git-url}[Git] is a free and open source, distributed version control system that simplifies access to and management of source code.
//
+
Install core Git tools:
+
[source, console]
----
sudo zypper install git-core
----

. Access to SUSE Container Images
//
+
Verify that your build host can access and download base container images from the {sle-bci-registry}[SUSE container registry].

--

* Kubernetes cluster
//
+
Your Kubernetes cluster must have worker nodes that are equipped and configured with {nvidia-dc-url}[Data Center class {nvidia-org} GPUs].
+
--
. Enable the Containers Module and {nvidia-org} Compute Module on all worker nodes by logging in to each one and executing the following commands:
+
[source, console, subs="attributes+"]
----
VER={sles-version}
SP={sles-sp}
sudo SUSEConnect -p sle-module-containers/$\{VER\}.$\{SP\}/x86_64
sudo SUSEConnect -p sle-module-NVIDIA-compute/$\{VER\}/x86_64
----
+
[NOTE]
====
Be sure to update the `VER` and `SP` variables with the {sles} version and service pack that you are using.
====

. {sles-full} {nvidia-org} software packages
//
+
Install required {nvidia-org} software packages on each node:
+
[source, console]
----
sudo zypper install \
  kernel-firmware-nvidia \
  libnvidia-container-tools \
  libnvidia-container1 \
  nvidia-container-runtime \
  sle-module-NVIDIA-compute-release
----

. Podman and Git
//
+
Install Podman and Git on at least one of the worker nodes:
+
[source, console]
----
sudo zypper install podman
sudo zypper install git-core
----

. {rke2-full} ({rke2})
//
+
{rke2-docs}[{rke2}] is the security focused Kubernetes distribution developed by SUSE.
+
Ensure your worker nodes are part of an {rke2} cluster.
You can {rke2-docs}/install/quickstart[deploy] {rke2} on worker nodes manually or use {rancher-docs-rke2}[Rancher by SUSE] to spin up and configure the cluster.

--

* Access to a container image registry from the build host and from the {rke2} cluster.
//
+
Using a container registry streamlines deployment of your GPU-enabled container across all Kubernetes worker nodes.
+
[TIP]
====
The container registry does not need to support authentication, but it should be configured with a valid TLS certificate.
====


== Setting up your environment

Certain information is needed multiple times throughout the build process.
This information can be stored in environment variables and recalled as needed, helping you to streamline commands, avoid errors, and maintain consistency.

[IMPORTANT]
====
Run all commands on your build host.
====

. Log into your build host.

. Create the `/tmp/build-variables.sh` file.
+
[source, console]
----
cat <<EOF> /tmp/build-variables.sh
export REGISTRY=""
export SLE_VERSION=""
export SLE_SP=""
export GOLANG_VERSION=""
export DRIVER_VERSION=""
export OPERATOR_VERSION=""
export CUDA_VERSION=""
EOF
----

. Edit the `/tmp/build-variables.sh` file to supply appropriate values for your project.
+
--
REGISTRY:: URL of the registry where the new container image is to be saved

SLE_VERSION:: {sle-bci-full} version

SLE_SP:: {sle-bci-full} service pack number

GOLANG_VERSION: the version of Go installed in your base container image.

DRIVER_VERSION:: {nvidia-drv} version
You can find the latest "Data Center Driver for Linux x64" version for your GPU at {nvidia-drv-url}.

OPERATOR_VERSION:: {nvidia-op} version
You can find the associated {nvidia-op} version at {nvidia-op-url}.

CUDA_VERSION:: the {nvidia-org} CUDA version appropriate for the selected {nvidia-drv}
The CUDA version is listed under "Software Versions" when you find your driver version at {nvidia-drv-url}.

--
+
For example, your `/tmp/build-variables.sh` file should look something like:
+
[listing, bash, subs="attributes+"]
----
export REGISTRY="registry.example.com"
export SLE_VERSION="{sles-version}"
export SLE_SP="{sles-sp}"
export GOLANG_VERSION="{golang-version}
export DRIVER_VERSION="{nvidia-drv-version}"
export OPERATOR_VERSION="{nvidia-op-version}"
export CUDA_VERSION="{nvidia-cuda-version}"
----

. Source your `/tmp/build-variables.sh` file to set the variables in your current session environment.
+
[source, console]
----
source /tmp/build-variables.sh
----

[IMPORTANT]
====
These variables are lost when you disconnect from your terminal session.
To recreate them, simple source the build-variables file again.
====


== Building the container image

. On your build host, make sure you have sourced your `/tmp/build-variables.sh` file to set your environment variables.
//
+
You can verify your variables with:
+
[source, console]
----
echo &&
echo "
REGISTRY=${REGISTRY}
SLE_VERSION=${SLE_VERSION}
SLE_SP=${SLE_SP}
DRIVER_VERSION=${DRIVER_VERSION}
CUDA_VERSION=${CUDA_VERSION}" && echo
----

. Clone the {nvidia-org} driver GitLab repository and change to the `driver/{nvidia-gitlab-drv-dir}` directory.
+
[source, console, subs="attributes+"]
----
git clone {nvidia-drv-gitlab-url} && cd driver/{nvidia-gitlab-drv-dir}
----

. Locate the file, `Dockerfile`, and update it to reflect your build requirements.

.. Make a backup of `Dockerfile` before modifying the original.
+
[source, console]
----
cp Dockerfile /tmp/Dockerfile.orig
----

.. Update the golang build container image to version `{golang-version}`.
+
[console, subs="attributes+"]
----
sed -i "/^FROM/ s/golang\:1\.../golang\:{golang-version}/" Dockerfile
----

.. Update the base container image to the {sles} {sle-bci-version} BCI:
//
+
[source, console, subs="attributes+"]
----
sed -i '/^FROM/ s/suse\/sle${sles-version}/bci\/bci-base/' Dockerfile
----

.. Verify that the changes have been made correctly to `Dockerfile`.
//
+
[source, console]
----
diff /tmp/Dockerfile.orig Dockerfile
----


. Build the container image.
+
[IMPORTANT] 
====
When building the container image, you may be prompted for the registry that contains the `nvidia/cuda` image.
If so, select the image located in {nvidia-license-image-registry}.
====

.. Execute the `podman build` command, passing necessary arguments.
+
[source, console]
----
sudo podman build -t \
${REGISTRY}/nvidia-sle${SLE_VERSION}sp${SLE_SP}-${DRIVER_VERSION}:${DRIVER_VERSION} \
  --build-arg SLES_VERSION="${SLE_VERSION}.${SLE_SP}" \
  --build-arg DRIVER_ARCH="x86_64" \
  --build-arg DRIVER_VERSION="${DRIVER_VERSION}" \
  --build-arg CUDA_VERSION="${CUDA_VERSION}" \
  --build-arg PRIVATE_KEY=empty  \
.
----

.. Watch the build output for errors, warnings, and failures.
You can safely ignore errors and warnings that don't stop the build process.

.. Verify that the build process finishes successfully.
//
+
You should see a message like:
+
[listing]
====
COMMIT registry.susealliances.com/nvidia-sle15sp5-535.104.05
--> cf976870489
Successfully tagged registry.susealliances.com/nvidia-sle15sp5-535.104.05:latest
cf9768704892c4b8b9e37a4ef591472e121b81949519204811dcc37d2be9d16c
====

. Remove intermediate container images created during the build process.
+
[source, console]
----
for X in $(sudo podman images | awk '/none/ {print$3}'); do sudo podman rmi ${X}; done
----

. Push the newly built image to the container registry.
+
[IMPORTANT] 
====
If the target container registry requires authentication, use the `podman login` command to successfully authenticate before continuing.
See {podman-login-url} for more information.
====
+
[source, console]
----
## Tag the image with the format that Helm will need when deploying on Kubernetes
sudo podman tag ${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION} ${REGISTRY}/driver:${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} &&
## Push the image (with both tags) to the container registry
sudo podman push ${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION} &&
sudo podman push ${REGISTRY}/driver:${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION}
----
//
+
.. Verify the image is saved in the registry, and remotely available:
//
+
[NOTE] 
====
If any of the following variables are not set correctly, press `CTRL+C` and return to <<Setting up your environment>> in this process before continuing.
====
//
+
[source, console]
----
sudo podman search --list-tags ${REGISTRY}/driver:${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION}
----
//
+
. Validate the container image
//
+
[NOTE] 
====
This step is optional and requires running the newly created {nvidia-drv} container locally with `Podman`, outside of the Kubernetes context. 
This can be done on a {sle-bci-version} host configured with the same kind of {nvidia-org} GPU the container was created for, or on a Kubernetes worker node that is configured with an {nvidia-org} GPU.
====
//
+
.. Open a command line session to the host or Kubernetes worker node on which you will test the container image.
//
+
.. Create the `/run/nvidia` directory, if it does not yet exist:
//
+
[source, console]
----
sudo mkdir -p /run/nvidia
----
//
+
.. Run the {nvidia-drv} container locally:
//
+
[NOTE] 
====
If any of the following variables are not set correctly, press `CTRL+C` and return to <<Setting up your environment>> in this process before continuing.
====
//
+
[console]
----
## Validate the variables before using them in the subsequent command
echo && \
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && \
  echo && \
  read -n1 -p "Press CTRL+C to cancel, otherwise press ENTER" BAILOUT

## Run the container image 
sudo podman run -d \
  --name driver.${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} \
  --privileged \
  --pid=host \
  -v /run/nvidia:/run/nvidia:shared \
  -v /var/log:/var/log \
  --restart=unless-stopped \
  ${REGISTRY}/driver:${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION}
----
//
+
.. Verify the container is running:
//
+
[source, console]
----
sudo podman ps -a
----
//
+
... The container's `STATUS` field should show that it is "Up" and the amount of time it has been up should increment with repeated runs of the command.
//
+
.. Monitor the deployment of the {nvidia-drv}:
//
+
[NOTE] 
====
If any of the following variables are not set correctly, press `CTRL+C` and return to <<Setting up your environment>> in this process before continuing.
====
//
+
[source, console]
----
## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press CTRL+C now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Review the standard output of the running container
sudo podman logs -f driver.${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} 
----
//
+
.. The deployment process is complete when the following message is shown:
//
+
[listing]
====
Mounting {nvidia-org} driver rootfs...
Done, now waiting for signal
====
//
+
.. Press `CTRL+C` to close the log viewing session
//
+
.. Ensure the {nvidia-org} kernel modules have been loaded:
//
+
[source, console]
----
sudo lsmod | grep nvidia
----
//
+
... You should see modules such as `nvidia`, `nvidia_modeset`, and `nvidia_uvm`
//
+
.. Verify the `nvidia-smi` utility can communicate withe the GPU:
//
+
[NOTE] 
====
If any of the following variables are not set correctly, press `CTRL+C` and return to <<Setting up your environment>> in this process before continuing.
====
//
+
[source, console]
----
## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press CTRL+C now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Verify the nvidia-smi utility can communicate with the GPU
sudo podman exec -it driver.${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} nvidia-smi
----
//
+
.. When ready to move forward, stop and remove the Podman container:
//
+
[NOTE] 
====
If any of the following variables are not set correctly, press `CTRL+C` and return to <<Setting up your environment>> in this process before continuing.
====
//
+
[source, console]
----
## Validate the variables before using them in the subsequent command
echo &&
echo "
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press CTRL+C now if these variables are NOT correct, otherwise press Enter" BAILOUT && 

## Stop and remove the container instance
sudo podman stop driver.${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} &&
sudo podman rm driver.${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} 
----
//
+
.. Before continuing to the Kubernetes deployment procedure, ensure the {nvidia-org} kernel modules are not loaded on any of the {nvidia-org} GPU equipped Kubernetes worker nodes:
//
+
[source, console]
----
sudo lsmod | grep nvidia
----
//
+
... You should receive no output.
//+
[NOTE]
====
If you see any modules containing the name `nvidia`, use the command `sudo modprobe -r <module name>` to unload them. 
If any modules fail to unload, reboot the node.
====


== Deploy to a Kubernetes cluster


[NOTE] 
====
The preferred method for installing the {nvidia-op} is with the Helm Kubernetes package manager.
====

[IMPORTANT] 
====
The following steps must be run from a Linux system that has the kubectl and Helm (version 3) utilities, as well as the KUBECONFIG file for the target Kubernetes cluster available to it. 
See these documents for more information: {rancher-kubectl-url} and {rke2-cluster-access-url}.

In addition, if the container build host is a different system than the one being used to perform the Helm install, the /tmp/.build-variables.sh file will need to be created on the second system. 
Return to <<Setting up your environnment>> in the proceeding procedure before continuing.
//Return to <<step_1, Step 1>> in the proceeding procedure before continuing.
====

. Add the {nvidia-org} helm software repository:
//
+
[console, subs="attributes+"]
----
helm repo add {nvidia-helm-url}
helm repo update
----
//
+
. Deploy the {nvidia-op} with Helm:
//
+
[NOTE]
====
If any of the following variables are not set correctly, press `CTRL+C` and return to <<Setting up your environment>> in the proceeding procedure before continuing.
====
//
+
[source, console]
----
## Verify the selected cluster before deploying
echo &&
echo "Cluster name: $(kubectl config current-context)" && 
echo "" && 
kubectl get nodes -o wide && 
echo "" && 
read -n1 -p "Is this the target Kubernetes cluster for the Helm chart? (y/n) " YESNO && 
echo "" && 
[ ${YESNO} != y ] && { echo "Exiting."; echo ""; exit; } || echo "" && 

## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
OPERATOR_VERSION=${OPERATOR_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press CTRL+C now if these variables are NOT correct, otherwise press Enter" BAILOUT && 

## Deploy the Helm chart
helm install -n gpu-operator \
  --generate-name \
  --wait \
  --create-namespace \
  --version=${OPERATOR_VERSION} \
    nvidia/gpu-operator \
  --set driver.repository=${REGISTRY} \
  --set driver.version=${DRIVER_VERSION} \
  --set operator.defaultRuntime=containerd \
  --set toolkit.env[0].name=CONTAINERD_CONFIG \
  --set toolkit.env[0].value=/var/lib/rancher/rke2/agent/etc/containerd/config.toml \
  --set toolkit.env[1].name=CONTAINERD_SOCKET \
  --set toolkit.env[1].value=/run/k3s/containerd/containerd.sock \
  --set toolkit.env[2].name=CONTAINERD_RUNTIME_CLASS \
  --set toolkit.env[2].value=nvidia \
  --set toolkit.env[3].name=CONTAINERD_SET_AS_DEFAULT \
  --set-string toolkit.env[3].value=true
----
//
+
. Verify the {nvidia-op}, {nvidia-drv} and associated elements have been deployed correctly with the command: 
//
+
[source, console]
----
kubectl get pods -n gpu-operator
----
//
+
.. The output should be similar to the following:
//
+
[source, console]
----
NAME                                                          READY   STATUS      RESTARTS   AGE

gpu-feature-discovery-crrsq                                   1/1     Running     0          60s

gpu-operator-7fb75556c7-x8spj                                 1/1     Running     0          5m13s

gpu-operator-node-feature-discovery-master-58d884d5cc-w7q7b   1/1     Running     0          5m13s

gpu-operator-node-feature-discovery-worker-6rht2              1/1     Running     0          5m13s

gpu-operator-node-feature-discovery-worker-9r8js              1/1     Running     0          5m13s

nvidia-container-toolkit-daemonset-lhgqf                      1/1     Running     0          4m53s

nvidia-cuda-validator-rhvbb                                   0/1     Completed   0          54s

nvidia-dcgm-5jqzg                                             1/1     Running     0          60s

nvidia-dcgm-exporter-h964h                                    1/1     Running     0          60s

nvidia-device-plugin-daemonset-d9ntc                          1/1     Running     0          60s

nvidia-device-plugin-validator-cm2fd                          0/1     Completed   0          48s

nvidia-driver-daemonset-5xj6g                                 1/1     Running     0          4m53s

nvidia-mig-manager-89z9b                                      1/1     Running     0          4m53s

nvidia-operator-validator-bwx99                               1/1     Running     0          58s
----



== Validation

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Illustrate functionality with a demonstration.
// Begin with a description or outline of the demonstration.
// Provide clear steps (in ordered lists) for the reader to follow.
// Typical demonstration flow is:
// 1. Prepare the environment for the demonstration.
//    This should be minimal, such as downloading some data to use.
//    If this requires more than a couple steps, consider putting it
//    in a subsection.
// 2. Perform the demonstration.
//    Be careful not to overuse screenshots.
// 3. Verify.
//    This may be interwoven into performing the demonstration.
//
// As with Installation, leverage ordered lists, code blocks,
// admonitions, and screenshots.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

[NOTE]
====
The {nvidia-op} Helm chart provides two pods that validate the state of the installed software.
====

. Validate the state of the {nvidia-op} software:
//
+
[source, console]
----
kubectl logs -n gpu-operator -l app=nvidia-operator-validator
----
//
+
.. The output should be similar to:
//
+
[listing]
====
Defaulted container "nvidia-operator-validator" out of: nvidia-operator-validator, driver-validation (init), toolkit-validation (init), cuda-validation (init), plugin-validation (init)

*all validations are successful*
====
//
+
. Validate the state of the {nvidia-cuda} software:
//
+
[source, console]
----
kubectl logs -n gpu-operator -l app=nvidia-cuda-validator
----
//
+
.. The output should be similar to the following:
//
+
[listing]
====
Defaulted container "nvidia-cuda-validator" out of: nvidia-cuda-validator, cuda-validation (init)

*cuda workload validation is successful*
====
//
+
. To validate that the {nvidia-drv} is communicating with the GPU, you can run this command to view the statics of the Kubernetes workers that are configured with GPUs:
//
+
[source, console]
----
kubectl exec -it \
"$(for EACH in \
$(kubectl get pods -n gpu-operator \
-l app=nvidia-driver-daemonset \
-o jsonpath={.items..metadata.name}); \
do echo ${EACH}; done)" \
-n gpu-operator \
nvidia-smi
----
//
+
[NOTE]
====
This command can also be used to verify which application processes are running on the {nvidia-org} GPUs, and how many resources are being consumed.
====

== Summary

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Summarize:
// - Motivation (1 sentence)
// - What was covered (1-2 sentences)
// - Next steps (unordered list of 2-4 further learning resources)
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// In this guide, you learned ...

This guide has effectively steered the creation of OCI compliant container images leveraging the {sle-bci-full} and incorporating the {nvidia-drv}.
Furthermore, it has provided coherent instructions for validating the functionality of the container image and the seamless deployment of the image within a Kubernetes cluster, specifically {rke2}.

This integrated solution is aimed at affording containerized applications GPU-acceleration, while avoiding the need to manage additional software on each GPU equipped node.
The strategic choice of SUSE's {sle-bci-full} as the foundation for this integration underscores an organization's commitment to security, certifications, and supportability.

Organizations with exacting security requirements depend on {sles-full}'s numerous certifications such as Common Criteria, FIPS, and EAL.
Additionally, SUSE's commitment to providing robust support for heterogeneous software stacks guarantees customer's the freedom to design their IT landscape to suit their unique business challenges. 

A pivotal point to underscore is the indispensability of a Kubernetes cluster, preferably {rke2}, that provides full {nvidia-org} GPU support and the requisite {nvidia-op} to fully harness GPU resources when deploying GPU intensive workloads.



// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

// vim: set syntax=asciidoc:

//end
