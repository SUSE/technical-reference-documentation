:docinfo:
include::./common_docinfo_vars.adoc[]

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// General comments
// Keep in mind that this is a "getting started" guide and the
//   audience that you are trying to reach.
// Leverage ASCIIDoc features to make this document readable and usable:
//   - Text highlights (follow SUSE style guides)
//   - Admonitions (i.e., NOTE, TIP, IMPORTANT, CAUTION, WARNING)
//   - Code blocks
//   - Lists (ordered and unordered, as appropriate)
//   - Links (to other resources)
//   - Images
//     - Place image files under the ./media directory tree
//       (e.g., ./media/src/svg, ./media/src/png)
//     - Format preference: svg > png > jpg
//     - Consolidate images wherever possible
//       (i.e., don't use two images when one conveys the message)
//   - Use sections and subsections to organize and group related
//     steps.
// 
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Document attributes and variables
//
// NOTES:
// 1. Update variables below and adjust docbook file accordingly.
// 2. Comment out any variables/attributes not used.
// 3. Follow the pattern to include additional variables.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// organization - do NOT modify
// -
:trd: Technical Reference Documentation
:type: Getting Started
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// document
// -
:title: Nvidia GPU Driver on SLE BCI
:subtitle: Creating and deploying the SLE15 Base Container Image with the Nvidia GPU Driver on RKE2 Kubernetes

:product1: SLE BCI
:product1_full: SUSE Linux Enterprise Base Container Image
:product1_version: SLE15 SP5
:product1_url: registry.suse.com/bci/bci-base-15sp5/index.html
:product2: RKE2
:product2_full: Rancher Kubernetes Engine 2
:product2_url: www.suse.com/products/rancher-kubernetes-engine/

:usecase: Nvidia GPU Driver based on SLE BCI

:sle15sp_version: SP5
:bci-base_version: 15.5
:golang_version: 12.1.1
:gpu-driver_version: 535.104.05
:gpu-operator_version: v23.6.1

// :executive_summary: A brief statement of what this document provides (e.g., This document provides a brief introduction to implementing {usecase} with {product2_full} and {product1_full}.)
:executive_summary: This document describes the process of creating an {usecase} and deploying it as part of the Nvidia Kubernetes Operator on the Rancher Kubernetes Engine 2 distribution of Kubernetes
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// contributor
// specify information about authors, editors, and others here,
// then update docinfo file as appropriate
// -
:author1_firstname: Alex
:author1_surname: Arnoldy
:author1_jobtitle: Embedded Solutions Architect
:author1_orgname: SUSE Alliance Architects
//:author2_firstname: first (given) name
//:author2_surname: surname
//:author2_jobtitle: job title
//:author2_orgname: organization affiliation
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// miscellaneous
// define any additional variables here for use within the document
// -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


= {title}: {subtitle}



== Introduction

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a brief statement (1-4 sentences) of the purpose of the guide.
// This is could be the same as the executive summary.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

This Getting Started guide provides comprehensive instructions for the creation of an OCI compliant container image containing an Nvidia GPU driver, and which is based on the SUSE Linux Enterprise 15 Base Container Image. The primary objective is to seamlessly integrate the Nvidia GPU Operator, simplifying GPU management and support within Kubernetes clusters for GPU-intensive workloads. The choice of SUSE's SLE 15 Base Container Image is motivated by the unparalleled security certifications and enhanced supportability it offers, particularly when operating heterogeneous software stacks.

=== Motivation

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a motivation for the document in 2-4 sentences to identify:
//   - what the document is about
//   - why it may be of interest to the reader (e.g., a use case)
//   - what products are being highlighted
// Include an approved SUSE | Partner logo lock-up if possible
// Include any additional, relevant details
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

The integration of the Nvidia GPU operator with the SUSE Linux Enterprise 15 Base Container Image provides a secure, certified, and supportable foundation for GPU-accelerated workloads. SUSE's SLE 15 Base Container Image is distinguished by a wide range of security certifications, including Common Criteria, FIPS, and EAL, making it a trusted choice for organizations with stringent security requirements.

You can confidently run diverse and complex software stacks without affecting the supportability of the base container image that is providing the Nvidia GPU operator. This ensures better stability and reliability for GPU-driven Kubernetes applications.

=== Scope

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Specify what this guide covers in no more than 2 sentences.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

//This guide will help you take the first steps to ...

This guide is dedicated to helping you create an OCI compliant container images that incorporate an appropriate Nvidia GPU driver within the SUSE Linux Enterprise 15 Base Container Image. Additionally, it provides instructions for deploying these container images within a Kubernetes cluster, specifically RKE2.

SUSE always recommends you use the most current Service Pack of SUSE Linux Enterprise that is available.

It is assumed that you are using Data Center class Nvidia GPU(s). Integrating consumer grade GPUs is beyond the scope of this document.

=== Audience

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify for whom this document is intended, perhaps with:
//   - topics of interests (e.g., machine learning, security, etc.)
//   - job roles (e.g., developer, administrator, platform architect, etc.)
//   - required skills
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// This document is intended for ...

This guide is intened for an audience comprising Kubernetes administrators, proficient DevOps practitioners, and application developers. It assumes a foundational understanding of Podman and/or Docker, Kubernetes, and Nvidia GPU technologies. This guide should be suitable for most high technology professionals seeking to unlock the full potential of their GPU accellerated containerized applications.

== Prerequisites

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify minimum requirements (prerequisites) the reader
// would need in order to follow the steps of this guide.
// - Link to existing resources whenever possible.
// - Keep this section brief but elaborate as needed.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

.Before embarking on the procedures outlined in this guide, you should ensure the following prerequisites have been met

* You have access to a SUSE Linux Enterprise 15 build host. It is highly recommended to ensure the build host is the same Service Pack (e.g. {product1_version}) version as the {product1} image to be used. NOTE: The build host does not require access to an Nvidia GPU.
* The SLE Containers Module has been enabled on the build host, and the SLE Containers Module plus the NVIDIA Compute Module have been enabled on all Kubernetes worker nodes.
* Podman is installed on the build host. If needed, you can install Podman from the SLE Containers module.
* You have access to an OCI image registry that is available to the build host AND the target Kubernetes cluster. This will allow the GPU driver to be easily deployed across all Kubernetes worker nodes that have Nvidia GPUs as part of the Nvidia Kubernetes Operator Helm chart.
* You have access to a Kubernetes cluster that is equipped and correctly configured with Data Center class Nvidia GPUs. These instructions leverage SUSE's security focused Kubernetes distribution, RKE2. 
* You possess a basic familiarity with Podman and/or Docker, Kubernetes, and Nvidia GPU concepts.


== Technical overview

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a technical overview of the solution.
// - Identify components.
// - Describe how the components fit together.
//   Leverage diagrams as appropriate, including (but not limited to):
//   - component architecture
//   - data flow diagram
//   - workflow diagram
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// Resources:
// https://gitlab.com/rdoxenham/driver/-/tree/main/sle15
// https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html#suse15
// https://docs.asciidoctor.org/asciidoc/latest/verbatim/source-blocks/
// https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#vmware-vsphere-with-tanzu
// https://docs.nvidia.com/grid/12.0/index.html



This section offers a detailed explanation of the steps required to create OCI compliant container images with the Nvidia GPU Operator inside the SUSE Linux Enterprise 15 SP5 Base Container Image.

Step 1[[step_1]]: Set these variables that will be consumed throughout this procedure:

// NOTE:  Find the latest "Data Center Driver for Linux x64" version for your GPU at https://www.nvidia.com/download/index.aspx. Find the associated GPU Operator version at https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html

NOTE:  Find the latest "Data Center Driver for Linux x64" version for your GPU at https://www.nvidia.com/download/index.aspx. Find the associated GPU Operator version at https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html

[console, bash]
----
## Set the variables that will be used throughout this procedure
echo &&
read -p "Enter \"local\" (withoout quotes) or provide the name of the registry where the new image will be saved: " REGISTRY && 
read -p "Enter the number for the SLE 15 service pack to be used for the container image (i.e. 5 for SP5): " SLE15_SP_VERSION && 
read -p "Provide the Nvidia GPU driver version (e.g. 535.104.05): " DRIVER_VERSION && 
read -p "Provide the Nvidia GPU Operator version (e.g. v23.6.1): " OPERATOR_VERSION && 
----

Step 2: Enable the SLE Containers software Module on the build host:

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing

----
sudo SUSEConnect -p sle-module-containers/15.${SLE15_SP_VERSION}/x86_64
----

Step 3: Clone the Nvidia GitLab repository and change to the `driver/sle15` directory:

----
git clone https://gitlab.com/nvidia/container-images/driver && cd driver/sle15
----

Step 4: Update the Dockerfile in this directory:

. Make a copy of the Dockerfile to use, if the modifications go badly:
//
+
[console, bash]
----
cp Dockerfile /tmp/Dockerfile.orig
----

. Update the golang build container image to version `1.18`:
//
+
[console, bash]
----
sed -i '/^FROM/ s/golang\:1\.../golang\:1.18/' Dockerfile
----

. Update the Dockerfile's base container image to the {product1_version} BCI:
//
+
[console, bash]
----
sed -i '/^FROM/ s/suse\/sle15/bci\/bci-base/' Dockerfile
----

Step 5: Build the SLE15 container image with the Nvidia GPU operator installed:

NOTE: When using a SLE15 build host that has a different Service Pack version that the {product1} container image to be used, the SUSE software repositories for the host will be used. This means that packages installed into the container image will match the Service Pack version of the host, not the {product1} container image. Ensure build host Service Pack matches the {product1} container image to avoid this situation.


CAUTION: While the follow steps allow for building the image in a "local" registry, it is highly recommended that this option only be used when building on a SLE15 host that has a Data Center class GPU. Building to a "local" registry will prevent you from deploying the new container image to a Kubernetes cluster.


* Build the container:

IMPORTANT: When building the container image, you may be prompted for the registry that contains the `nvidia/cuda` image. Select the image from docker.io.

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing

```
## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}
DRIVER_TYPE=${DRIVER_TYPE}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && 

## Build the Nvidia GPU driver container
sudo podman build -t \
${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION} \
  --build-arg SLES_VERSION="15.${SLE15_SP_VERSION}" \   
  --build-arg DRIVER_TYPE="${DRIVER_TYPE}" \     
  --build-arg DRIVER_ARCH="x86_64" \     
  --build-arg DRIVER_VERSION="${DRIVER_VERSION}" \     
  --build-arg CUDA_VERSION="12.1.1" \   
  --build-arg PRIVATE_KEY=empty  \
.
```

* Watch the build output for errors, warnings, and failures. You can safely ignore errors and warnings that don't stop the build process.

* The build process should finish with a message saying that the final image was committed and tagged. For example:


====
 COMMIT registry.susealliances.com/nvidia-sle15sp5-535.104.05
--> cf976870489
Successfully tagged registry.susealliances.com/nvidia-sle15sp5-535.104.05:latest
cf9768704892c4b8b9e37a4ef591472e121b81949519204811dcc37d2be9d16c
====

Step 6: Remove the intermediate build container image that was created as part of the build process (and any other leftover artifacts):

----
for EACH in $(sudo podman images | awk '/none/ {print$3}'); do sudo podman rmi ${EACH}; done
----

Step 7: If the specified build registry was not set to "local", push the newly build image to the container registry:

IMPORTANT: If the target container registry requires authentication, use the `sudo podman login` command to successfully authenticate before continuing. See https://docs.podman.io/en/latest/markdown/podman-login.1.html for more information.

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing

----
## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && 

## Tag the image with the format that Helm will need when deploying on Kubernetes
sudo podman tag ${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION} ${REGISTRY}/driver:${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION} &&
## Push the image (with both tags) to the container registry
sudo podman push ${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION} &&
sudo podman push ${REGISTRY}/driver:${DRIVER_VERSION}-sles15.${SLE15_SP_VERSION}
----

* Verify the image is saved in the registry, and remotely available:

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing

----
sudo podman search --list-tags ${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}
----

Step 8: Validate the container image

NOTE: This step is optional and requires running the newly created GPU driver container locally with `Podman`, outside of the Kubernetes context. This can be done on a {product1_version} host configured with the same kind of Nvidia GPU the container was created for, or on a Kuberentes worker node that is configured with an Nvidia GPU.

* Open a command line session to the host or Kubernetes worker node on which you will test the container image.

* Create the `/run/nvidia` directory, in case it does not yet exist:

----
sudo mkdir -p /run/nvidia
----

* Run the GPU operator container locally:

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing

----
## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Run the container image 
sudo podman run -d \
  --name nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION} \
  --privileged \
  --pid=host \
  -v /run/nvidia:/run/nvidia:shared \
  -v /var/log:/var/log \
  --restart=unless-stopped \
${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION}
----

* Verify the container is running:

----
sudo podman ps -a
----

** The container's `STATUS` field should show that it is "Up" and the amount of time it has been up should increment with repeated runs of the command.


* Monitor the deployment of the Nvidia GPU operator:

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing

----
## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Review the standard output of the running container
sudo podman logs -f nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}
----

* The deployment process is complete when the following message is shown:

----
Mounting NVIDIA driver rootfs...
Done, now waiting for signal
----

* Press `Ctrl+c` to close the log viewing session

* Ensure the Nvidia kernel modules have been loaded:

----
sudo lsmod | grep nvidia
----

** You should see modules such as `nvidia`, `nvidia_modeset`, and `nvidia_uvm`


* Verify the `nvidia-smi` utility can communicate withe the GPU:

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing

----
## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Verify the nvidia-smi utitlity can communicate with the GPU
sudo podman exec -it nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION} nvidia-smi
----



* When ready to move forward, stop and remove the Podman container:

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing

----
## Validate the variables before using them in the subsequent command
echo &&
echo "
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && 

## Stop and remove the container instance
sudo podman stop nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION} &&
sudo podman rm nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION} 
----

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Detail the steps of the installation procedure.
// The reader should be able to copy and paste commands to
// a local environment or follow along locally with screenshots.
// Include one or more verification steps to validate installation.
//
// Leverage:
// - Ordered lists
// - Code blocks
// - Screenshots
// - Admonitions
//
// If multiple installation methods are to be detailed, then
// - Create a summary list here
// - Detail each method in its own subsection.
//
// NOTE: For solutions involving SUSE Rancher, it is preferred
//       to detail two installation methods:
//       - Through the Rancher Apps Catalog with appropriate
//         screenshots and SUSE branding.
//       - A more manual approach (e.g., on the command-line).
//
// Complex configuration procedures may be broken out into one or more
// Configuration sections.
// These may be subsections of Installation or separate sections at
// the same level as Installation.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

== Deployment to a Kubernetes (RKE2) cluster


* Enable the SLE Containers software Module on each Kubernetes GPU worker:

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this beginning of this procedure before continuing

----
sudo SUSEConnect -p sle-module-containers/15.${SLE15_SP_VERSION}/x86_64
sudo SUSEConnect -p sle-module-NVIDIA-compute/15/x86_64
----

* Install the following packages:

----
sudo zypper in kernel-firmware-nvidia libnvidia-container-tools libnvidia-container1 nvidia-container-runtime sle-module-NVIDIA-compute sle-module-NVIDIA-compute-release
----

NOTE: The preferred method for installing the Nvidia GPU Operator is with the Helm Kuberenetes package manager.

IMPORTANT: The following commands must be run from a Linux system that has the kubectl and helm (version 3) commands, as well as the KUBECONFIG file for the target Kubernetes cluster available to it. See these documents for more information: https://www.suse.com/c/rancher_blog/how-to-manage-kubernetes-with-kubectl/  https://docs.rke2.io/cluster_access 
* Add the Nvidia helm software repository:

----
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia && 
helm repo update
----

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to <<step_1, Step 1>> in this process before continuing

* Deploy the GPU Operator with Helm:

----
## Verify the selected cluster before deploying
echo &&
echo "Cluster name: $(kubectl config current-context)" && 
echo "" && 
kubectl get nodes -o wide && 
echo "" && 
read -n1 -p "Is this the correct Kuberentes cluster to install the Nvidia GPU operator? (y/n) " YESNO && 
echo "" && 
[ ${YESNO} != y ] && { echo "Exiting."; echo ""; exit; } || echo "" && 

## Validate the variables before using them in the subsequent command
echo &&
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
OPERATOR_VERSION=${OPERATOR_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && 

## Deploy the Nvidia GPU Operator with supporting software and drivers
helm install -n gpu-operator \
  --generate-name \
  --wait \
  --create-namespace \
  --version=${OPERATOR_VERSION} \
    nvidia/gpu-operator \
  --set driver.repository=${REGISTRY} \
  --set driver.version=${DRIVER_VERSION} \
  --set operator.defaultRuntime=containerd \
  --set toolkit.env[0].name=CONTAINERD_CONFIG \
  --set toolkit.env[0].value=/var/lib/rancher/rke2/agent/etc/containerd/config.toml \
  --set toolkit.env[1].name=CONTAINERD_SOCKET \
  --set toolkit.env[1].value=/run/k3s/containerd/containerd.sock \
  --set toolkit.env[2].name=CONTAINERD_RUNTIME_CLASS \
  --set toolkit.env[2].value=nvidia \
  --set toolkit.env[3].name=CONTAINERD_SET_AS_DEFAULT \
  --set-string toolkit.env[3].value=true
----

* Verify the Nvidia GPU Operator, driver and associated elements have been deployed correctly with the command: 

----
kubectl get pods -n gpu-operator
----

** The output should resemble:

----
NAME                                                          READY   STATUS      RESTARTS   AGE
gpu-feature-discovery-crrsq                                   1/1     Running     0          60s
gpu-operator-7fb75556c7-x8spj                                 1/1     Running     0          5m13s
gpu-operator-node-feature-discovery-master-58d884d5cc-w7q7b   1/1     Running     0          5m13s
gpu-operator-node-feature-discovery-worker-6rht2              1/1     Running     0          5m13s
gpu-operator-node-feature-discovery-worker-9r8js              1/1     Running     0          5m13s
nvidia-container-toolkit-daemonset-lhgqf                      1/1     Running     0          4m53s
nvidia-cuda-validator-rhvbb                                   0/1     Completed   0          54s
nvidia-dcgm-5jqzg                                             1/1     Running     0          60s
nvidia-dcgm-exporter-h964h                                    1/1     Running     0          60s
nvidia-device-plugin-daemonset-d9ntc                          1/1     Running     0          60s
nvidia-device-plugin-validator-cm2fd                          0/1     Completed   0          48s
nvidia-driver-daemonset-5xj6g                                 1/1     Running     0          4m53s
nvidia-mig-manager-89z9b                                      1/1     Running     0          4m53s
nvidia-operator-validator-bwx99                               1/1     Running     0          58s
----



== Validation

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Illustrate functionality with a demonstration.
// Begin with a description or outline of the demonstration.
// Provide clear steps (in ordered lists) for the reader to follow.
// Typical demonstration flow is:
// 1. Prepare the environment for the demonstration.
//    This should be minimal, such as downloading some data to use.
//    If this requires more than a couple steps, consider putting it
//    in a subsection.
// 2. Perform the demonstration.
//    Be careful not to overuse screenshots.
// 3. Verify.
//    This may be interwoven into performing the demonstration.
//
// As with Installation, leverage ordered lists, code blocks,
// admonitions, and screenshots.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

* To validate that the Nvidia GPU driver is communicating with the GPU, you can run this command to view the statics of the Kubernetes workers that are configured with GPUs:

[console, bash]
----
kubectl exec -it \
"$(for EACH in \
$(kubectl get pods -n gpu-operator \
-l app=nvidia-driver-daemonset \
-o jsonpath={.items..metadata.name}); \
do echo ${EACH}; done)" \
-n gpu-operator \
nvidia-smi
----

[NOTE]
====
This command can also be used to verify which application processes are running on the Nvidia GPUs, and how many resources are being consumed.
====

== Summary

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Summarize:
// - Motivation (1 sentence)
// - What was covered (1-2 sentences)
// - Next steps (unordered list of 2-4 further learning resources)
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// In this guide, you learned ...

This guide has effectively steered the creation of OCI compliant container images rooted in the SUSE Linux Enterprise 15 SP5 Base Container Image, incorporating the Nvidia GPU operator. Furthermore, it has provided coherent instructions for the seamless deployment of these images within a Kubernetes cluster, specifically RKE2. The ensuing utilization of this integrated solution is aimed at affording containerized applications GPU-acceleration, thus enhancing computational performance and efficiency.

The strategic choice of SUSE's SLE 15 SP5 Base Container Image as the foundation for this integration underscores the commitment to security, certifications, and supportability. Organizations with exacting security requirements will find solace in the numerous certifications such as Common Criteria, FIPS, and EAL. Additionally, the enhanced support for heterogeneous software stacks guarantees the reliability and stability of GPU-accelerated applications running on this foundation.

This holistic approach to GPU management within containerized landscapes is poised to augment the capabilities of developers and system administrators alike, fostering innovation and operational excellence. For further insights and updates pertaining to the Nvidia GPU operator and its multifaceted functionalities, kindly refer to the official Nvidia documentation and the designated repository mentioned within this guide.

A pivotal point to underscore is the indispensability of a Kubernetes cluster, preferably RKE2, that is provisioned with Nvidia GPU support and the requisite Nvidia drivers to fully harness GPU resources when deploying containers integrated with the aforementioned OCI compliant container images.

For an exhaustive comprehension of advanced configurations and nuanced details, the consultative resources made available by SUSE, Docker, Nvidia, and RKE2 are strongly recommended.




// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
