:docinfo:
include::./common_docinfo_vars.adoc[]

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// General comments
// Keep in mind that this is a "getting started" guide and the
//   audience that you are trying to reach.
// Leverage ASCIIDoc features to make this document readable and usable:
//   - Text highlights (follow SUSE style guides)
//   - Admonitions (i.e., NOTE, TIP, IMPORTANT, CAUTION, WARNING)
//   - Code blocks
//   - Lists (ordered and unordered, as appropriate)
//   - Links (to other resources)
//   - Images
//     - Place image files under the ./media directory tree
//       (e.g., ./media/src/svg, ./media/src/png)
//     - Format preference: svg > png > jpg
//     - Consolidate images wherever possible
//       (i.e., don't use two images when one conveys the message)
//   - Use sections and subsections to organize and group related
//     steps.
// 
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Document attributes and variables
//
// NOTES:
// 1. Update variables below and adjust docbook file accordingly.
// 2. Comment out any variables/attributes not used.
// 3. Follow the pattern to include additional variables.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// organization - do NOT modify
// -
:trd: Technical Reference Documentation
:type: Getting Started
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// document
// -
:title: Nvidia GPU Operator on BCI
:subtitle: Deploying the Nvidia GPU Kubernetes Operator on SLE15 Base Container Image

:product1: BCI
:product1_full: SUSE Linux Enterprise Base Container Image
:product1_version: SLE15 SP5
:product1_url: registry.suse.com/bci/bci-base-15sp5/index.html
:product2: RKE2
:product2_full: Rancher Kubernetes Engine 2
:product2_url: www.suse.com/products/rancher-kubernetes-engine/

:usecase: Nvidia GPU Operator on SLE BCI

:executive_summary: A brief statement of what this document provides (e.g., This document provides a brief introduction to implementing {usecase} with {product2_full} and {product1_full}.)
:executive_summary: This document describes the process of creating an Nvidia GPU Kubernetes operator container image based on SUSE Linux Enterprise Base Container Image
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// contributor
// specify information about authors, editors, and others here,
// then update docinfo file as appropriate
// -
:author1_firstname: Alex
:author1_surname: Arnoldy
:author1_jobtitle: Embedded Solutions Architect
:author1_orgname: SUSE Alliance Architects
//:author2_firstname: first (given) name
//:author2_surname: surname
//:author2_jobtitle: job title
//:author2_orgname: organization affiliation
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// miscellaneous
// define any additional variables here for use within the document
// -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


= {title}: {subtitle}



== Introduction

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a brief statement (1-4 sentences) of the purpose of the guide.
// This is could be the same as the executive summary.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

This Getting Started guide provides comprehensive instructions for the creation of an OCI compliant container image containing the Nvidia GPU Kubernetes operator, and which is based on the SUSE Linux Enterprise 15 Base Container Image. The primary objective is to seamlessly integrate the Nvidia GPU operator, simplifying GPU management and support within Kubernetes clusters for GPU-intensive workloads. The choice of SUSE's SLE 15 Base Container Image is motivated by the unparalleled security certifications and enhanced supportability it offers, particularly when operating heterogeneous software stacks.

=== Motivation

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a motivation for the document in 2-4 sentences to identify:
//   - what the document is about
//   - why it may be of interest to the reader (e.g., a use case)
//   - what products are being highlighted
// Include an approved SUSE | Partner logo lock-up if possible
// Include any additional, relevant details
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

The integration of the Nvidia GPU operator with the SUSE Linux Enterprise 15 Base Container Image provides a secure, certified, and supportable foundation for GPU-accelerated workloads. SUSE's SLE 15 Base Container Image is distinguished by a myriad of security certifications, including Common Criteria, FIPS, and EAL, making it a trusted choice for organizations with stringent security requirements.

Furthermore, users can confidently run diverse and complex software stacks without affecting the supportability of the base container image that is providing the Nvidia GPU operator. This ensures better stability and reliability for GPU-driven Kubernetes applications.

=== Scope

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Specify what this guide covers in no more than 2 sentences.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

//This guide will help you take the first steps to ...

This guide is dedicated to the process of creating OCI compliant container images that incorporate the Nvidia GPU operator within the SUSE Linux Enterprise 15 Base Container Image. Additionally, it provides instructions for deploying these container images within a Kubernetes cluster, specifically RKE2.

SUSE always recommends using the most current Service Pack of SUSE Linux Enterprise that is available.

It is assumed that Data Center class  Nvidia GPU(s) in use. Integrating consumer grade GPUs is beyond the scope of this document.

=== Audience

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify for whom this document is intended, perhaps with:
//   - topics of interests (e.g., machine learning, security, etc.)
//   - job roles (e.g., developer, administrator, platform architect, etc.)
//   - required skills
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// This document is intended for ...

This guide caters to an audience comprising Kubernetes administrators, proficient DevOps practitioners, and application developers. It assumes a foundational understanding of Podman and/or Docker, Kubernetes, and Nvidia GPU technologies. This this guide should be suitable for most high technology professionals seeking to unlock the full potential of their GPU accellerated containerized applications.

== Prerequisites

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify minimum requirements (prerequisites) the reader
// would need in order to follow the steps of this guide.
// - Link to existing resources whenever possible.
// - Keep this section brief but elaborate as needed.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Before embarking on the procedures outlined in this guide, it is imperative to ensure the fulfillment of the following prerequisites:

   1. You have access to a SUSE Linux Enterprise 15 build host. It is highly recommended to ensure the build host is the same Service Pack version as the SLE BCI image to be used. NOTE: The build host does not have to have access to an Nvidia GPU.
   1. Podman and sudo applications have been installed on the build host.
   1. You have access to an OCI image registry that is available to the build host and target Kubernetes cluster. This will allow the GPU operator to be easily deployed across all Kubernetes worker nodes that have Nvidia GPUs.
   1. You have access to a Kubernetes cluster that is equipped and correctly configured to use Data Center class Nvidia GPUs. These instructions leverage SUSE's security focused Kubernetes distribution, RKE2. 
   1. You possess a basic familiarity with Podman and/or Docker, Kubernetes, and Nvidia GPU concepts.


== Technical overview

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a technical overview of the solution.
// - Identify components.
// - Describe how the components fit together.
//   Leverage diagrams as appropriate, including (but not limited to):
//   - component architecture
//   - data flow diagram
//   - workflow diagram
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

////

Resources:
https://gitlab.com/rdoxenham/driver/-/tree/main/sle15
https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html#suse15
https://docs.asciidoctor.org/asciidoc/latest/verbatim/source-blocks/
https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#vmware-vsphere-with-tanzu
https://docs.nvidia.com/grid/12.0/index.html

.Rough notes on process:

* Clone the Nvidia repository:

----
git clone https://gitlab.com/nvidia/container-images/driver && cd driver/sle15
----

* Can find the latest Data Center Driver for Linux x64 version at https://www.nvidia.com/en-us/drivers/unix/, or https://www.nvidia.com/download/index.aspx
** Copy the driver version number

* Update the Dockerfile: 
** Update `ARG CUDA_VERSION` to `12.1.1`
** Update `FROM registry.suse.com/bci/golang` to version `1.18`

NOTE: When using a SLE15 build host that has a different Service Pack version that the SLE BCI container image to be used, the SUSE software repositories for the host will be used. This means that packages that are installed into the container image will match the Service Pack version of the host, not the BCI container image. Ensure build host Service Pack matches the BCI container image to avoid this situation.

* Set these variables:

----
REGISTRY=""		# E.g. REGISTRY="local" or REGISTRY="registry.susealliances.com"
SLE15_SP_VERSION=""	# I.e. For SLE15 SP5, set to SP_VERSION="5"
DRIVER_VERSION=""	# E.g. DRIVER_VERSION="535.104.05"
DRIVER_TYPE=""		# I.e. "vgpu" or "passthrough"
----

* Build the container:

----
sudo podman build -t \
${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION} \
  --build-arg SLES_VERSION="15.${SLE15_SP_VERSION}" \   
  --build-arg DRIVER_TYPE="${DRIVER_TYPE}" \     
  --build-arg DRIVER_ARCH="x86_64" \     
  --build-arg DRIVER_VERSION="${DRIVER_VERSION}" \     
  --build-arg CUDA_VERSION="12.1.1" \   
  --build-arg PRIVATE_KEY=empty  \
.
----

* Remove the intermediate build container image that was created as part of the build process (and any other leftover artifacts):

----
for EACH in $(sudo podman images | awk '/none/ {print$3}'); do sudo podman rmi ${EACH}; done
----

////


This section offers a detailed explanation of the steps required to create OCI compliant container images with the Nvidia GPU Operator inside the SUSE Linux Enterprise 15 SP5 Base Container Image.

Step 1: Set these variables that will be consumed throughout this procedure:

NOTE:  Find the latest "Data Center Driver for Linux x64" version for your GPU at https://www.nvidia.com/download/index.aspx. Find the associated GPU Operator version at https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html

----
## Set the variables that will be used throughout this procedure
echo 
read -p "Enter \"local\" (withoout quotes) or provide the name of the registry where the new image will be saved: " REGISTRY && \
read -p "Enter the number for the SLE 15 service pack to be used for the container image (i.e. 5 for SP5): " SLE15_SP_VERSION && \
read -p "Provide the Nvidia GPU driver version (e.g. 535.104.05): " DRIVER_VERSION && \
read -p "Provide the Nvidia GPU Operator version (e.g. v23.6.1): " OPERATOR_VERSION && \
read -p "Enter vgpu or passthrough for the type of Nvidia driver: " DRIVER_TYPE 
----

Step 2: Clone the Nvidia GitLab repository and change to the `driver/sle15` directory:

----
git clone https://gitlab.com/nvidia/container-images/driver && cd driver/sle15
----

Step 3: Update the Dockerfile in that directory:

* Update `ARG CUDA_VERSION` to version `12.1.1`
* Update `FROM registry.suse.com/bci/golang` to version `1.18`

Step 4: (Optional) To utilize the vGPU capabilities of your datacenter grade GPU, you'll need to download the latest "vGPU Driver Catalog" (filename vgpuDriverCatalog.yaml) and the latest "Linux KVM" package (filename "NVIDIA-GRID-Linux-KVM-*.zip").  

NOTE: Additional information can be found at https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/install-gpu-operator-vgpu.html

* To obtain these two files, sign in to your https://ui.licensing.nvidia.com/:"NVIDIA Licensing Portal" account
** Navigate to "Software Downloads" and download the files

* Unzip the NVIDIA GRID driver zip file: 

----
unzip /path/to/downloads/NVIDIA-GRID-Linux-KVM-525.105.14-525.105.17-528.89.zip
----

* Before coping in the new files, ensure only the "README.md" file exists in the existing `drivers/` subdirectory:

----
ls -la drivers/
----

* Copy the guest GRID driver file to the "drivers/" subdirectory. The following example assumes driver version "525.105.17":

----
cp -p Guest_Drivers/NVIDIA-Linux-x86_64-525.105.17-grid.run drivers/
----

* Copy the vGPU Driver Catalog file into the "drivers/" subdirectory:

----
cp -p /path/to/downloads/vgpuDriverCatalog.yaml drivers/
----

Step 5: Build the SLE15 container image with the Nvidia GPU operator installed:

NOTE: When using a SLE15 build host that has a different Service Pack version that the SLE BCI container image to be used, the SUSE software repositories for the host will be used. This means that packages installed into the container image will match the Service Pack version of the host, not the BCI container image. Ensure build host Service Pack matches the BCI container image to avoid this situation.


CAUTION: While the follow steps allow for building the image in a "local" registry, it is highly recommended that this option only be used when building on a SLE15 host that has a Data Center class GPU.


* Build the container:

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to Step 1 in this process before continuing

----
## Validate the variables before using them in the subsequent command
echo
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}
DRIVER_TYPE=${DRIVER_TYPE}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Build the Nvidia GPU driver container
sudo podman build -t \
${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION} \
  --build-arg SLES_VERSION="15.${SLE15_SP_VERSION}" \   
  --build-arg DRIVER_TYPE="${DRIVER_TYPE}" \     
  --build-arg DRIVER_ARCH="x86_64" \     
  --build-arg DRIVER_VERSION="${DRIVER_VERSION}" \     
  --build-arg CUDA_VERSION="12.1.1" \   
  --build-arg PRIVATE_KEY=empty  \
.
----

* Watch the build output for errors, warnings, and failures. Errors and warnings that don't stop the build process can often be safely ignored.

* The build process should finish with a message saying that the final image was committed and tagged. For example:

----
 COMMIT registry.susealliances.com/nvidia-sle15sp5-535.104.05
--> cf976870489
Successfully tagged registry.susealliances.com/nvidia-sle15sp5-535.104.05:latest
cf9768704892c4b8b9e37a4ef591472e121b81949519204811dcc37d2be9d16c
----

Step 6: Remove the intermediate build container image that was created as part of the build process (and any other leftover artifacts):

----
for EACH in $(sudo podman images | awk '/none/ {print$3}'); do sudo podman rmi ${EACH}; done
----

Step 7: If the specified build registry was not set to "local", push the newly build image to the container registry:

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to Step 1 in this process before continuing

----
## Validate the variables before using them in the subsequent command
echo
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Tag the image with the format that Helm will need when deploying on Kubernetes
sudo podman tag ${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION} ${REGISTRY}/driver:${DRIVER_VERSION} \
## Push both the image (with both tags) to the container registry
sudo podman push ${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION} \
sudo podman push ${REGISTRY}/driver:${DRIVER_VERSION}
----

* Verify the image is saved in the registry, and remotely available:

----
sudo podman search --list-tags ${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}
----

Step 8: Validate the container image

NOTE: This step is optional and requires running the GPU operator container locally with `Podman`, outside of the Kubernetes context. This can be done on a SLE15 node configured with an Nvidia GPU, or a Kuberentes worker node that is configured with an Nvidia GPU

* Open a command line session to a node in the Kubernetes cluster that is equipped with, and correctly configured to use, a Data Center class Nvidia GPU

* Create the `/run/nvidia` directory, in case it does not yet exist:

----
sudo mkdir -p /run/nvidia
----

* Run the GPU operator container locally:

NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to Step 1 in this process before continuing

----
## Validate the variables before using them in the subsequent command
echo
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Run the container image 
sudo podman run -d \
  --name nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION} \
  --privileged \
  --pid=host \
  -v /run/nvidia:/run/nvidia:shared \
  -v /var/log:/var/log \
  --restart=unless-stopped \
${REGISTRY}/nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}:${DRIVER_VERSION}
----

* Verify the container is running:

----
sudo podman ps -a
----

** The container's `STATUS` field should show that it is "Up" and the amount of time it has been up should increment with repeated runs of the command.


* Monitor the deployment of the Nvidia GPU operator:

----
## Validate the variables before using them in the subsequent command
echo
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Review the standard output of the running container
sudo podman logs -f nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION}
----

* The deployment process is complete when the following message is shown:

----
Mounting NVIDIA driver rootfs...
Done, now waiting for signal
----

* Press `Ctrl+c` to close the log viewing session

* Ensure the Nvidia kernel modules have been loaded:

----
sudo lsmod | grep nvidia
----

** You should see modules such as `nvidia`, `nvidia_modeset`, and `nvidia_uvm`


* Verify the `nvidia-smi` utility can communicate withe the GPU:

----
## Validate the variables before using them in the subsequent command
echo
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Verify the nvidia-smi utitlity can communicate with the GPU
sudo podman exec -it nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION} nvidia-smi
----

* When ready to move forward, stop and remove the Podman container:

----
## Validate the variables before using them in the subsequent command
echo
echo "
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \
sudo podman stop nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION} 
sudo podman rm nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION} 
----

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Detail the steps of the installation procedure.
// The reader should be able to copy and paste commands to
// a local environment or follow along locally with screenshots.
// Include one or more verification steps to validate installation.
//
// Leverage:
// - Ordered lists
// - Code blocks
// - Screenshots
// - Admonitions
//
// If multiple installation methods are to be detailed, then
// - Create a summary list here
// - Detail each method in its own subsection.
//
// NOTE: For solutions involving SUSE Rancher, it is preferred
//       to detail two installation methods:
//       - Through the Rancher Apps Catalog with appropriate
//         screenshots and SUSE branding.
//       - A more manual approach (e.g., on the command-line).
//
// Complex configuration procedures may be broken out into one or more
// Configuration sections.
// These may be subsections of Installation or separate sections at
// the same level as Installation.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

== Deployment to a Kubernetes (RKE2) cluster

Each node on the RKE2 cluster needs to have the containerd runtime environment updated to recognize and run GPU workloads

* Find this secition in the `/var/lib/rancher/rke2/agent/etc/containerd/config.toml` file:

----
[plugins."io.containerd.grpc.v1.cri".containerd]
  snapshotter = "overlayfs"
  disable_snapshot_annotations = true
----

** Add these lines directly below those entries:

----
  default_runtime_name = "nvidia"

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia]
    privileged_without_host_devices = false
    runtime_engine = ""
    runtime_root = ""
    runtime_type = "io.containerd.runc.v2"
    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia.options]
      BinaryName = "/usr/bin/nvidia-container-runtime"
----

* Restart RKE2 on each node, one at a time, with this command:

----
sudo systemctl restart rke2-server.service
sudo systemctl status rke2-server.service
----


The preferred method for installing the Nvidia GPU Operator is with the Helm Kuberenetes package manager.


NOTE: If any of the following variables are not set correctly, press `Ctrl+c` and return to Step 1 in this process before continuing

* Deploy the GPU Operator with Helm:

----
## Verify the selected cluster before deploying
echo
echo "Cluster name: $(kubectl config current-context)" && 
echo "" && 
kubectl get nodes -o wide && 
echo "" && 
read -n1 -p "Is this the correct Kuberentes cluster to install the Nvidia GPU operator? (y/n) " YESNO && 
echo "" && 
[ ${YESNO} != y ] && { echo "Exiting."; echo ""; exit; } || echo "" && 

## Validate the variables before using them in the subsequent command
echo
echo "
REGISTRY=${REGISTRY}
SLE15_SP_VERSION=${SLE15_SP_VERSION}
DRIVER_VERSION=${DRIVER_VERSION}" && echo && read -n1 -p "Press Ctrl+c now if these variables are NOT correct, otherwise press Enter" BAILOUT && \

## Deploy the Nvidia GPU Operator with supporting software and drivers
helm install --wait --generate-name \
  -n gpu-operator --create-namespace \
  nvidia/gpu-operator \
  --set driver.repository=${REGISTRY} \
  --set operator.defaultRuntime=containerd
  --set toolkit.env[0].name=CONTAINERD_CONFIG \
  --set toolkit.env[0].value=/var/lib/rancher/rke2/agent/etc/containerd/config.toml \
  --set toolkit.env[1].name=CONTAINERD_SOCKET \
  --set toolkit.env[1].value=/run/k3s/containerd/containerd.sock \
  --set toolkit.env[2].name=CONTAINERD_RUNTIME_CLASS \
  --set toolkit.env[2].value=nvidia \
  --set toolkit.env[3].name=CONTAINERD_SET_AS_DEFAULT \
  --set-string toolkit.env[3].value=true
----

* Verify the Nvidia GPU Operator, driver and associated elements have been deployed correctly with the command: 

----
kubectl get pods -n gpu-operator
----

** The output should resemble:

----
NAME                                                          READY   STATUS      RESTARTS   AGE
gpu-feature-discovery-crrsq                                   1/1     Running     0          60s
gpu-operator-7fb75556c7-x8spj                                 1/1     Running     0          5m13s
gpu-operator-node-feature-discovery-master-58d884d5cc-w7q7b   1/1     Running     0          5m13s
gpu-operator-node-feature-discovery-worker-6rht2              1/1     Running     0          5m13s
gpu-operator-node-feature-discovery-worker-9r8js              1/1     Running     0          5m13s
nvidia-container-toolkit-daemonset-lhgqf                      1/1     Running     0          4m53s
nvidia-cuda-validator-rhvbb                                   0/1     Completed   0          54s
nvidia-dcgm-5jqzg                                             1/1     Running     0          60s
nvidia-dcgm-exporter-h964h                                    1/1     Running     0          60s
nvidia-device-plugin-daemonset-d9ntc                          1/1     Running     0          60s
nvidia-device-plugin-validator-cm2fd                          0/1     Completed   0          48s
nvidia-driver-daemonset-5xj6g                                 1/1     Running     0          4m53s
nvidia-mig-manager-89z9b                                      1/1     Running     0          4m53s
nvidia-operator-validator-bwx99                               1/1     Running     0          58s
----


////

  --set operator.image=nvidia-sle15sp${SLE15_SP_VERSION}-${DRIVER_VERSION} \
  --set driver.version=${DRIVER_VERSION}


Afer building and validating the SLE15 Nvidia GPU operator container image, you can now deploy it to a Kubernetes cluster with worker nodes configured with Data Center class Nvidia GPUs. Note that the Kubernetes worker nodes must be confgiured with the essential Nvidia drivers for the appropriate GPU(s).

* Deploy 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-gpu-app
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: my-gpu-app
    spec:
      containers:
      - name: my-gpu-container
        image: sle15-nvidia-operator
        resources:
          limits:
            nvidia.com/gpu: 1  # Adjust GPU resource allocation as necessitated

Deploy the articulated Deployment YAML to the Kubernetes cluster via the following command:

bash

kubectl apply -f my-gpu-app-deployment.yaml

Ascertain the operational status of the deployment by issuing the command:

kubectl get pods

The resulting list should feature the 'my-gpu-app' pod in the 'Running' state.

Need to view logs of the pod

////



== Validation

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Illustrate functionality with a demonstration.
// Begin with a description or outline of the demonstration.
// Provide clear steps (in ordered lists) for the reader to follow.
// Typical demonstration flow is:
// 1. Prepare the environment for the demonstration.
//    This should be minimal, such as downloading some data to use.
//    If this requires more than a couple steps, consider putting it
//    in a subsection.
// 2. Perform the demonstration.
//    Be careful not to overuse screenshots.
// 3. Verify.
//    This may be interwoven into performing the demonstration.
//
// As with Installation, leverage ordered lists, code blocks,
// admonitions, and screenshots.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

ATTENTION: Need to review the prior work I did for GPU integration, likely with Supermicro

1. Gain access to the active pod to conduct a comprehensive validation of GPU functionality:


1. kubectl exec -it my-gpu-app-<pod-id> -- /bin/bash

1. Within the pod's context, employ Nvidia tools such as 'nvidia-smi' to meticulously scrutinize GPU-related metrics and execute tasks necessitating GPU capabilities.

1. Execute comprehensive tests of GPU-intensive workloads or applications within the pod to substantiate the unimpeded access and optimal utilization of GPU resources.



== Summary

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Summarize:
// - Motivation (1 sentence)
// - What was covered (1-2 sentences)
// - Next steps (unordered list of 2-4 further learning resources)
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// In this guide, you learned ...

This guide has effectively steered the creation of OCI compliant container images rooted in the SUSE Linux Enterprise 15 SP5 Base Container Image, incorporating the Nvidia GPU operator. Furthermore, it has provided coherent instructions for the seamless deployment of these images within a Kubernetes cluster, specifically RKE2. The ensuing utilization of this integrated solution is aimed at affording containerized applications GPU-acceleration, thus enhancing computational performance and efficiency.

The strategic choice of SUSE's SLE 15 SP5 Base Container Image as the foundation for this integration underscores the commitment to security, certifications, and supportability. Organizations with exacting security requirements will find solace in the numerous certifications such as Common Criteria, FIPS, and EAL. Additionally, the enhanced support for heterogeneous software stacks guarantees the reliability and stability of GPU-accelerated applications running on this foundation.

This holistic approach to GPU management within containerized landscapes is poised to augment the capabilities of developers and system administrators alike, fostering innovation and operational excellence. For further insights and updates pertaining to the Nvidia GPU operator and its multifaceted functionalities, kindly refer to the official Nvidia documentation and the designated repository mentioned within this guide.

A pivotal point to underscore is the indispensability of a Kubernetes cluster, preferably RKE2, that is provisioned with Nvidia GPU support and the requisite Nvidia drivers to fully harness GPU resources when deploying containers integrated with the aforementioned OCI compliant container images.

For an exhaustive comprehension of advanced configurations and nuanced details, the consultative resources made available by SUSE, Docker, Nvidia, and RKE2 are strongly recommended.




// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
