
The figure below illustrates the high-level architecture of the {pn_Rancher} installation that manages multiple downstream Kubernetes clusters: 

image::ArchitectureOverview-{portfolioName}.png[title="Architecture Overview - {pn_Rancher}", scaledwidth=80%]

Authentication Proxy::
A user is authenticated via {pn_Rancher} and then, if authorized, can access both the {pn_Rancher} environment and the downstream clusters and workloads.

API Server::
This provides the programmatic interface backend for a user, utilizing command-line interactions with {pn_Rancher} and the managed clusters.

Data Store::
The purpose of this service is to capture the configuration and state of {pn_Rancher} and the managed clusters to aid in backup and recovery processes.

Cluster Controller::
Interacting with a cluster agent on the downstream cluster, the cluster controller allows the communication path for users and services to leverage for workloads and cluster management.

Once setup, users can interact with {pn_Rancher} through the web-based user interface (UI), the command-line interface ( CLI ), and programatically through the application programming interface ( API ). Depending upon the assigned roles, group membership and privileges, a user could:

* manage all clusters, users, roles, projects
* deploy new clusters, import other clusters, or remove existing ones
* manage workloads across respective or labelled clusters
* simply view clusters or workloads, or just benefit from what is running

ifdef::RC,RI[]
For the best performance and security, the recommended deployment is a dedicated Kubernetes cluster for the {pn_Rancher} management server. Running user workloads on this cluster is not advised. After deploying {pn_Rancher}, one can then create or import clusters for orchestrated workloads.
endif::RC,RI[]

ifdef::GS[]

IMPORTANT: Regardless of the deployment target, {pn_Rancher} should always run on a node or cluster that is separate from the downstream clusters that it manages. Running user workloads on this {pn_Rancher} cluster or nodes is not advised.

To aid in planning, training or assessing functionality like in a <<G_PoC>> deployment, {pn_Rancher} can be installed on a single node running a Linux operating system as described later in this document.

ifdef::BP[]
TIP: To improve <<G_Availability>>, the {pn_Rancher} backup operator can then be used to {pn_Rancher_MigrateURL}[migrate] from the single node to a <<G_Production>> installation on a multi-node, high-availability Kubernetes cluster.
endif::BP[]
endif::GS[]

