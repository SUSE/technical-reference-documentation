
ifdef::IHV-Cisco-C240-SD[]

The base, starting configuration can reside all within a single {an_Cisco} {an_CiscoUCS}. Based upon the relatively small resource requirements for a
ifdef::focusRancher[{pn_Rancher}] 
ifdef::focusK3s[{pn_K3s}]
deployment, a viable approach is to deploy as a virtual machine (VM) on the target nodes, on top of an existing hypervisor, like KVM.

//-
Preparation(s)::
For a physical host, like {modelCiscoUCS-C240SDURL}[{modelCiscoUCS-C240SD}] used in the deployment:
. If using {an_Cisco} {an_CiscoUCS} Manager
**  Log into the Cisco UCS Manager
*** Select the Equipment tab
*** In the navigation pane expand Rack-Mounts and then Servers
*** Right-click the server and select KVM console
*** In the right pane, click the KVM Console
**** Click the link to launch the KVM console
**** Select the Virtual Media tab and activate Virtual Devices found in Virtual Media tab
**** Click the Virtual Media tab to select CD/DVD
**** Select Map Drive in the Virtual Disk Management window and browse to respective operating system media, open and use the image for a system boot.

//-
Deployment Process::
On the respective compute module node, determine if a hypervisor is already available for the solution's virtual machines.

. If this will be the first use of this node, an option is to deploy a KVM hypervisor, based upon {pn_SLES} by following the {pn_SLES_VirtDocURL}[Virtualization Guide].
** Given the simplicity of the deployment, the operating system and hypervisor can be installed with the {pn_SLES} ISO media and the {an_Cisco} {an_Cisco_BMC} virtual media and virtual console methodology.
. Then for the solution VM, utilize the hypervisor user interface to allocate the necessary CPU, memory, disk and networking as noted in the {pn_Rancher} {pn_Rancher_HWReqURL}[hardware requirements].

//-
Deployment Consideration(s)::
To further optimize deployment factors, leverage the following practices:

// ifdef::FCTR+Automation[]
// * <<G_Automation>>
// ** FixMe - To simplify multiple compute module setups and configurations, leverage the {vn_HPE} {vn_HPE_ComposerTech} {vn_HPE_OVTerraformURL}[SDK for Terraform Provider].
// endif::FCTR+Automation[]
ifdef::FCTR+Availability[]
* <<G_Availability>>
** While the initial deployment only requires a single VM, as noted in later deployment sections, having multiple VMs provides resiliency to accomplish high availability. To reduce single points of failure, it would be beneficial to have the multi-VM deployments spread across multiple hypervisor nodes. So consideration of consistent hypervisor and compute module configurations, with the needed resources for the {pn_Rancher} VMs will yield a robust, reliable production implementation.
endif::FCTR+Availability[]

endif::IHV-Cisco-C240-SD[]
