
ifdef::focusRancher[]

=== {pn_Rancher}

// leverage multiple prep sections
ifdef::layerSLEMicro,layerSLES[include::./SA-RA-Deployment-OS-prep.adoc[]]

Deployment Process::
While logged into the node, as root or with sudo privileges, install {pn_Rancher}:
+
ifdef::GS[]
. Run the following installation command
+
----
sudo docker run \
	--detach \
	--restart=unless-stopped \
	--publish 80:80 --publish 443:443 \
	--privileged rancher/rancher
----
+
ifdef::BP[]
TIP: This process deploys an auto-generated, self-signed security certificate for the {pn_Rancher} service. Other <<G_Security>> certificate authority options are described in the "Install/Upgrade {portfolioName} on a Kubernetes Cluster" in the {pn_Rancher} {pn_Rancher_DocURL}[product documentation].
endif::BP[]
+
. Once the previous command completes, from a client system connect via a web browser to the {pn_Rancher} node via *_IPAddress_* or *_HostName_* and accept the service certificate (if deemed valid).
** Enter a new admin password
+
IMPORTANT: On the second configuration page, also ensure the "Server URL" is set to the *_IPAddress_* or *_HostName_* of this deployed {pn_Rancher} node.
endif::GS[]
ifdef::RC,RI[]
. Create the Helm Chart custom resource for cert-manager:
* Set the following variable with the desired version of cert-manager
+
----
CERT_MANAGER_VERSION=""
----
+
NOTE: At this time, the most current, supported version of cert-manager is v1.0.4
+
// ** e.g., `CERT_MANAGER_VERSION="v1.0.4"`
* Create the cert-manager Helm Chart custom resource manifest
+
----
cat <<EOF> cert-manager-helm-crd.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: cert-manager
  namespace: kube-system
spec:
  chart: cert-manager
  targetNamespace: cert-manager
  version: ${CERT_MANAGER_VERSION}
  repo: https://charts.jetstack.io
EOF
----
+
* Create the cert-manager CRDs and apply the Helm Chart resource manifest: 
+
----
kubectl create namespace cert-manager

kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/${CERT_MANAGER_VERSION}/cert-manager.crds.yaml 

sudo mv cert-manager-helm-crd.yaml /var/lib/rancher/k3s/server/manifests/
----
+
** Monitor the progress of the installation: `watch -c "kubectl get deployments -A"`
*** The deployment is complete when all deployments (cert-manager, cert-manager-cainjector, cert-manager-webhook) show at least "1" as "AVAILABLE"
*** Use Ctrl+c to exit the watch loop after all pods are running
+
ifdef::layerK3s[]
. Create the Helm Chart custom resource for {pn_Rancher}:
* Set the following variable to the hostname of the {pn_Rancher} server instance
+
----
HOSTNAME=""
----
+
// ** e.g., `HOSTNAME="suse-rancher.sandbox.local"`
// +
NOTE: This hostname should be resolvable to an IP address of the {pn_K3s} host, or a load balancer/proxy server that supports this installation of {pn_Rancher}.
+
* Create the {pn_Rancher} Helm Chart custom resource manifest
+
----
cat <<EOF> suse-rancher-helm-crd.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: rancher
  namespace: kube-system
spec:
  chart: rancher
  targetNamespace: cattle-system
  repo: https://releases.rancher.com/server-charts/stable
  set:
    hostname: ${HOSTNAME}
EOF
----
+
* Apply the Helm Chart resource manifest: 
+
----
kubectl create namespace cattle-system
sudo mv suse-rancher-helm-crd.yaml /var/lib/rancher/k3s/server/manifests/
----
+
** Monitor the progress of the installation: `watch -c "kubectl get pods -n cattle-system"`
*** The installation is complete when all pods have a status of "Completed" or a status of "Running" with the number of "READY" pods being "1/1", "2/2", etc.
*** Use Ctrl+c to exit the watch loop after all pods are running
+
endif::layerK3s[]

ifdef::layerRKE1[]

NOTE: Installing Rancher requires a client system (i.e. admin workstation) that has been configured with Helm version 3 and kubectl, as well as access to the kubeconfig file for the RKE cluster. 

.Perform the following steps to install {pn_Rancher}:
* Export the variable KUBECONFIG to the absolute pathname of the kube_config_cluster.yml file. I.e. `export KUBECONFIG=~/rke-cluster/kube_config_cluster.yml`

NOTE: The easiest way of installing {pn_Rancher} is with self-signed certificates. See https://rancher.com/docs/rancher/v2.x/en/installation/install-rancher-on-k8s/#6-install-rancher-with-helm-and-your-chosen-certificate-option for other options.

. Create the Helm Chart custom resource for cert-manager:

* Install the cert-manager custom resource definitions:
----
kubectl create namespace cert-manager

kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.4/cert-manager.crds.yaml
----

.Install cert-manager:

* Set the following variable with the desired version of cert-manager
+
----
CERT_MANAGER_VERSION=""
----
+
NOTE: At the time of writing, the most current, supported version of cert-manager is v1.0.4
+
// ** e.g., `CERT_MANAGER_VERSION="v1.0.4"`
* Add the jetstack repo and install the cert-manager Helm chart
----
helm repo add jetstack https://charts.jetstack.io

helm repo update

helm install \
  cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --version ${CERT_MANAGER_VERSION}
----

.Install {pn_Rancher}  

* Set the following variable to the FQDN of the {pn_Rancher} server instance, or load balancer that is serving {pn_Rancher}

----
RANCHER_HOSTNAME=""
----

* Add the {pn_Rancher} Helm repository and install the {pn_Rancher} Helm chart: 
----
kubectl create namespace cattle-system

helm repo add rancher-stable https://releases.rancher.com/server-charts/stable

helm repo update

helm install \
  rancher rancher-stable/rancher \
  --namespace cattle-system \
  --set hostname=${RANCHER_HOSTNAME}
----

endif::layerRKE1[]

. (Optional) Create an SSH tunnel to access {pn_Rancher}: 
+
NOTE: This optional step is useful in cases where NAT routers and/or firewalls prevent the client web browser from reaching the exposed {pn_Rancher} server IP address and/or port. This step requires that a Linux host is accessible through SSH from the client system and that the Linux host can reach the exposed {pn_Rancher} service. The {pn_Rancher} hostname should be resolvable to the appropriate IP address by the local workstation.
+
* Create an SSH tunnel through the Linux host to the IP address of the {pn_Rancher} server on the NodePort, as noted in Step 3:
+
----
ssh -N -D 8080 user@Linux-host
----
+
* On the local workstation web browser, change the SOCKS Host settings to "127.0.0.1" and port "8080"
+
NOTE: This will route all traffic from this web browser through the remote Linux host. Be sure to close the tunnel and revert the SOCKS Host settings when you're done.
+
. Connect to the {pn_Rancher} web UI and configure {pn_Rancher}:
* On the client system, use a web browser to connect to the {pn_Rancher} service
** e.g., `https://suse-rancher.sandbox.local`
* Provide a new Admin password
+
IMPORTANT: On the second configuration page, ensure the "Rancher Server URL" is set to the hostname specified when creating the {pn_Rancher} HelmChart custom resource and the port is 443.
+
** e.g., `suse-rancher.sandbox.local:443`

//-
Deployment Consideration(s)::
To further optimize deployment factors, leverage the following practices
ifdef::FCTR+Availability[]
* <<G_Availability>>
** In instances where a load balancer is used to access a {pn_K3s} cluster, deploying two additional {pn_K3s} cluster nodes, for a total of three, will automatically make {pn_Rancher} highly available.
endif::FCTR+Availability[]
ifdef::FCTR+Security[]
* <<G_Security>>
** The basic deployment steps described above are for deploying {pn_Rancher} with automatically generated, self-signed security certificates. Other options are to have {pn_Rancher} create public certificates via Let's Encrypt associated with with a publicly resolvable hostname for the {pn_Rancher} server, or to provide preconfigured, private certificates. See {pn_Rancher} https://rancher.com/docs/rancher/v2.x/en/installation/install-rancher-on-k8s/#3-choose-your-ssl-configuration[product documentation] for more information.
endif::FCTR+Security[]
ifdef::FCTR+Integrity[]
* <<G_Integrity>>
** This deployment of {pn_Rancher} uses the {pn_K3s} etcd key/value store to persist its data and configuration, which offers several advantages. With a multi-node cluster and this resiliency through replication, having to provide highly-available storage isn't needed. In addition, backing up the {pn_K3s} etcd store protects the cluster as well as the installation of {pn_Rancher} and permits restoration of a given state.
endif::FCTR+Integrity[]

endif::RC,RI[]

// Next Steps::
After this successful deployment of the {pn_Rancher} solution, review the {pn_Rancher_DocURL}[product documentation] for details on how downstream Kubernetes clusters can be:

* deployed ( refer to sub-section "Setting up Kubernetes Clusters in {portfolioName}" ) or
* imported ( refer to sub-section "Importing Existing Clusters" ), then 
* managed ( refer to sub-section "Cluster Administration" ) and
* accessed ( refer to sub-section "Cluster Access" ) to address orchestration of workload, maintaining security and many more functions are readily available.

endif::focusRancher[]

