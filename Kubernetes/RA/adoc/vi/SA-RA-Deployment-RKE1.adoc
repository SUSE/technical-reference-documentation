

ifdef::focusRKE1,layerRKE1[]
=== {pn_RKE1}

// leverage multiple prep sections
ifdef::layerSLEMicro,layerSLES[include::./SA-RA-Deployment-OS-prep.adoc[]]
ifdef::focusRancher[]
+
. Identify the appropriate, supported version of the {pn_RKE1} binary (e.g. vX.Y.Z), by reviewing the "{portfolioName} Support Matrix" on the {pn_Rancher_SupURL}[Support and Maintenance Terms of Service] web page. 
endif::focusRancher[]
ifndef::focusRKE1[]
+
. Identify the appropriate, desired version of the {pn_RKE1} binary (e.g. vX.Y.Z), by reviewing the "Releases" on the {pn_K3s_Download}[Download] web page. 
endif::focusRKE1[]

//-
Deployment Process::
The primary steps for deploying this {pn_RKE1} Kubernetes
// ifdef::focusRKE1[]
// ifdef::layerRKE1[]
are:
// To meet the {pn_Rancher} prerequisites and requirements on supported Kubernetes instances,
// ifdef::layerRKE1[{pn_RKE1_ProductPage}[{pn_RKE1}]]
// can be utilized, and as desired later scaled out to a production cluster.
//+
+
NOTE: Installing {pn_RKE1} requires a client system (i.e. admin workstation) that has been configured with kubectl.

. Download the {pn_RKE1} binary according to the instructions on product {pn_RKE1_DocURL}[documentation] page, then follow the directions on that page, but with the following exceptions:
//. Download the {pn_RKE1} binary according to the instructions on this webpage: https://rancher.com/docs/rke/latest/en/installation/. Follow the directions on that page, but with the following exceptions:
. Create the cluster.yml file with the command `rke config`
+
NOTE: See product documentation for https://rancher.com/docs/rke/latest/en/example-yamls/[example-yamls] and https://rancher.com/docs/rke/latest/en/config-options/[config-options] for detailed examples and descriptions of the cluster.yml parameters.
+
** It is recommended to create a unique SSH key for this {pn_RKE1} cluster with the command `ssh-keygen`
*** Provide the path to that key for the option "Cluster Level SSH Private Key Path"
** The option "Number of Hosts" refers to the number of hosts to configure at this time 
*** Additional hosts can be added very easily after {pn_RKE1} cluster creation 
*** For this implementation it is recommended to configure one or three hosts
** Give all hosts the roles of "Control Plane", "Worker", and "etcd"
** Answer "n" for the option "Enable PodSecurityPolicy"
. Update the cluster.yml file before continuing with the step "Deploying Kubernetes with RKE"
. If a load balancer has been deployed for the {pn_RKE1} control-plane nodes, update the cluster.yml file before deploying {pn_RKE1} to include the IP address or FQDN of the load balancer. The appropriate location is under authentication.sans. For example:
+
----
authentication:
  strategy: x509
  sans: ["rancher.susealliances.com"]
----
+
ifdef::layerSLEMicro[]
. Update the cluster.yml file to work with the {pn_SLEMicro} read-only fileystem as shown below:
+
** Update the network.options block:
+
----
network:
  plugin: canal
  options: {
   canal_flex_volume_plugin_dir: /opt/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds,
   flannel_backend_type: vxlan 
  }
----
+
** Update the service.kube-controller.extra_args blocks:
+
----
kube-controller:
    image: ""
    extra_args: {flex-volume-plugin-dir: /opt/kubernetes/kubelet-plugins/volume/exec/}
----
endif::layerSLEMicro[]
+
. Verify password-less SSH is available from the admin workstation to each of the cluster hosts as the user specified in the cluster.yml file
. When ready, run `rke up` to create the RKE cluster 
. After the `rke up` command completes, the RKE cluster will continue the Kubernetes installation process
** Monitor the progress of the installation:
*** Export the variable KUBECONFIG to the absolute pathname of the kube_config_cluster.yml file. I.e. `export KUBECONFIG=~/rke-cluster/kube_config_cluster.yml`
*** Run the command: `watch -c "kubectl get deployments -A"`
**** The cluster deployment is complete when elements of all the deployments show at least "1" as "AVAILABLE"
**** Use Ctrl+c to exit the watch loop after all deployment pods are running
+
ifdef::BP[]
TIP: To address <<G_Availability>> and possible <<G_Scaling>> to a multiple node cluster, etcd is enabled instead of using the default SQLite datastore.
endif::BP[]

ifdef::BP[]
//-
Deployment Consideration(s)::
To further optimize deployment factors, leverage the following practices:
ifdef::FCTR+Availability[]
* <<G_Availability>>
** A full high-availability {pn_RKE1} cluster is recommended for production workloads. For this use case, two additional hosts should be added; for a total of three. All three hosts will perform the roles of control-plane, etcd, and worker.
+
. Deploy the same operating system on the new compute platform nodes, and prepare them in the same way as the first node
. Update the cluster.yml file to include the addional node
+
*** Using a text editor, copy the information for the first node (found under the "nodes:" section)
**** The node information usually starts with "- address:" and ends with the start of another node entry, or the beginning of the "services: " section, i.e.
+
----
- address: 172.16.240.71
  port: "22"
  internal_address: ""
  role:
  - controlplane
  - worker
  - etcd

. . .

  labels: {}
  taints: []
----
+
*** Paste the information into the same section, once for each additional host
*** Update the pasted information, as appropriate, for each additional host
+
. When the cluster.yml file is updated with the information specific to each node, run the command `rke up`
*** Run the command: `watch -c "kubectl get deployments -A"`
**** The cluster deployment is complete when elements of all the deployments show at least "1" as "AVAILABLE"
**** Use Ctrl+c to exit the watch loop after all deployment pods are running
+
ifdef::focusRancher[]
In this configuration the {pn_RKE1} nodes are acting as control-plane, etcd, and worker nodes. This is perfect for the {pn_Rancher} server cluster as it does not require additional nodes to maintain a highly available {pn_Rancher} server application.
+
endif::focusRancher[]

endif::FCTR+Availability[]
endif::BP[]

ifdef::focusRKE1[]
// Next Steps::
After this successful deployment of the {pn_RKE1} solution, review the {pn_RKE1_DocURL}[product documentation] for details on how to directly utilize this Kubernetes cluster. Furthermore, by reviewing the {pn_Rancher} {pn_Rancher_DocURL}[product documentation] this solution can also be:

* imported ( refer to sub-section "Importing Existing Clusters" ), then
* managed ( refer to sub-section "Cluster Administration" ) and
* accessed ( refer to sub-section "Cluster Access" ) to address orchestration of workloads, maintaining security and many more functions are readily available.
endif::focusRKE1[]

endif::focusRKE1,layerRKE1[]

