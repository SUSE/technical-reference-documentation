
ifdef::iK3s,iRKE1,iRKE2[]

ifdef::layerRKE1[]
=== {pn_RKE1}
endif::layerRKE1[]

ifdef::layerRKE2[]
=== {pn_RKE2}
endif::layerRKE2[]

ifdef::focusK3s,layerK3s[]
=== {pn_K3s}

ifdef::focusRancher[]
While it is highly recommended that Kubernetes workloads are isolated from the Kubernetes control-plane and data-plane, this design will maintain all functions, including the {pn_Rancher}, on this node. The {pn_Rancher} workload is a known quantity and no other workloads will be run on this Kubernetes cluster. For this reason the {pn_Rancher} deployment is more closely aligned with appliance-based approach.

//-
Preparation(s)::
To meet the {pn_Rancher} prerequisites and requirements on supported Kubernetes instances,
ifdef::layerK3s[{pn_K3s_ProductPage}[{pn_K3s}]]
can be utilized, and as desired later scaled out to a production cluster.
+
. Identify the appropriate, supported version of the {pn_K3s} binary, by reviewing the "{portfolioName} Support Matrix" on the {pn_Rancher_SupURL}[Support and Maintenance Terms of Service] web page.
. Log into the node's operating system, either as root or as a user with sudo privileges.
endif::focusRancher[]

While it is highly recommended that Kubernetes workloads are isolated from the Kubernetes control-plane and data-plane, this design will maintain all functions, including the {pn_Rancher}, on this node. The {pn_Rancher} workload is a known quantity and no other workloads will be run on this Kubernetes cluster. For this reason the {pn_Rancher} deployment is more closely aligned with appliance-based approach.

//-
Preparation(s)::
{pn_K3s} provides the certified Kubernetes functionality, can be deployed on a single node and as desired later scaled out to a production cluster.
+
. Identify the appropriate, released version of the {pn_K3s} binary, by reviewing the {pn_K3s_Download}[download] web page.
. Log into the node's operating system, either as root or as a user with sudo privileges.

//-
Deployment Process::
The primary steps for deploying this {pn_K3s} Kubernetes layer are:

. Set the following variable with the noted version of {pn_K3s}, as found during the preparation steps.
+
----
K3s_VERSION=""
----
+
// ** e.g., `K3s_VERSION="v1.20.4+k3s1"`
// +
. Install the version of {pn_K3s} with embedded etcd enabled:
+
----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=${K3s_VERSION} INSTALL_K3S_EXEC='server --cluster-init --write-kubeconfig-mode=644' sh -s -
----
+
ifdef::BP[]
TIP: To address possible scaling to a multiple node cluster, etcd is enabled instead of using the default sqlite datastore.
+
endif::BP[]
** Monitor the progress of the installation: `watch -c "kubectl get deployments -A"`
*** The deployment is complete when all deployments elements (coredns, local-path-provisioner, metrics-server, and traefik) show at least "1" as "AVAILABLE"
*** Then use Ctrl+c to exit the watch loop after all deployment pods are running

ifdef::BP[]
//-
Deployment Consideration(s)::
To further optimize deployment factors, leverage the following practices:
ifdef::FCTR+Availability[]
* <<G_Availability>>
** While a single {pn_K3s} node works perfectly fine, a full high-availability {pn_K3s} cluster is recommended for production workloads. The etcd key/value store (aka database) requires an odd number of nodes be allocated to the {pn_K3s} plane (aka master nodes). In this case, two additional control-plane nodes will be added; for a total of three.
+
. Deploy the same operating system on the new compute platform nodes, then log into the new nodes as root or as a user with sudo privileges.
. Execute the following sets of commands on each of the remaining control-plane nodes:
+
----
FIRST_SERVER_IP=""      # Private IP preferred, if available
NODE_TOKEN=""           # From the /var/lib/rancher/k3s/server/node-token file on the first server
K3s_VERSION=""          # Match the first of the first server
----
+
----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=${K3s_VERSION} K3S_URL=https://${FIRST_SERVER_IP}:6443 K3S_TOKEN=${NODE_TOKEN} K3S_KUBECONFIG_MODE="644" INSTALL_K3S_EXEC='server' sh -
----
+
ifdef::focusRancher[]
By default, the {pn_K3s} server nodes are available to run non-control-plane workloads. In this case, the {pn_K3s} default behavior is perfect for the {pn_Rancher} server cluster as it doesn't require additional agent (aka worker) nodes to maintain a highly available {pn_Rancher} server application.
+
endif::focusRancher[]
NOTE: This can be changed to the normal Kubernetes default by adding a taint to each server node. See the official Kubernetes documentation for more information on how to do that.
+
. (Optional) In cases where agent nodes are desired, execute the following sets of commands on each of the agent nodes to add it to the {pn_K3s} cluster:
+
----
FIRST_SERVER_IP=""      # Private IP preferred, if available
NODE_TOKEN=""           # From the /var/lib/rancher/k3s/server/node-token file on the first server
K3s_VERSION=""          # Match the first of the first server
----
+
----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=${K3s_VERSION} K3S_URL=https://${FIRST_SERVER_IP}:6443 K3S_TOKEN=${NODE_TOKEN} K3S_KUBECONFIG_MODE="644" sh -
----
endif::FCTR+Availability[]
endif::BP[]

endif::focusK3s,layerK3s[]

endif::iK3s,iRKE1,iRKE2[]

