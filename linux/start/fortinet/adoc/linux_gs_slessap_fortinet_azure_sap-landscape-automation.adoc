:docinfo:
include::../../../../common/adoc/common_docinfo_vars.adoc[]

// the ifdef's make it possible to only change the DC file for generating the right document
ifdef::Azure[]
:cloud: Azure
endif::[]

// Variables & Attributes
:title: Using SUSE and Fortinet automation to deploy a secured SAP landscape on Azure Cloud Platform : Getting Started
:productname: SUSE Linux Enterprise Server for SAP Applications
:author1: Peter Schinagl, Sr. Technical Architect, SUSE
//:author2: First Last, title, Partner
:revdate: March 01, 2022
:revnumber: 0.1
//:toc2:
//:toc-title: {title}
//:toclevels: 4

:partner: Fortinet

:sles: SUSE Linux Enterprise Server
:sles4sap: {sles} for SAP applications
:hana_version: SAP HANA 2.0 SPS05
:hana_archive_version: 51054623

= {title}

== Introduction

=== Motivation

// Why this would be of interest
// Challenges
// Benefits
What is Project Mayerhofen?

Project Mayerhofen is a secure automated SAP deployment on Azure, based on the SUSE initiated Github project https://github.com/SUSE/ha-sap-terraform-deployments[Automated SAP/HA Deployments in Public/Private Clouds with Terraform].


As organizations upgrade their existing systems and migrate to S/4 HANA, there is the opportunity to migrate to Azure for more flexibility and scalability on demand. However, this also shifts the attack surface for the SAP system as they are no longer in their own datacenter.

A very useful approach to start with in the cloud is Infrastructure-as-Code (IaS), as all steps of deployments can be automated from begin on.

Combining the SUSE SAP automation with a automated Fortinet Security Fabric provisioning results in an architecture which provides added security, optimized connectivity and faster rollout of an SAP landscape in Azure.

Some key values of the solution are:

. Simplify SAP Cloud deployments on Azure
. Increase flexibility and scalability on demand
. Improve builds and time to production
. Uses industry standards for automated deployment and configuration
. Combine SUSE SAP automation with an automated Fortinet Security Fabric provisioning
. Provides security, optimized connectivity and faster rollout of SAP landscapes

Adding the Fortinet deployment, it is the same workflow as with the normal SUSE SAP automation project, you only need to enable “with fortinet” switch to benefit from the additional installation of the Fortinet Fabric.


=== Scope

// This guide will help you take the first steps to
This document will walk you through a simple deployment of a HA SAP Landscape using the SUSE Automation Project for SAP Solutions Project and Fortinet's secure fabric on {cloud}. The project could be quite extensively customized, show here is a simple easy to start-with example.

The first part of the project uses Terraform to build the {cloud} infrastructure, where the Fortinet part is deployed in a HUB and the SAP Landscape on SUSE in a SPOKE.
The HUB virtual network is the connection point to on-premises networks and a central location for services used by SPOKE virtual networks. The SPOKE virtual networks are peered with the HUB and can be used to isolate workloads in their own virtual network.

The second part of the project uses Salt to deploy and configure the operating system ({sles4sap}), SAP software (SAP HANA and SAP Netweaver), and if chosen a SUSE Linux Enterprise High Availability (HA) cluster for the SAP applications.

If extensive configuration and customization are required, please refer to the project documentation at https://github.com/SUSE/ha-sap-terraform-deployments and the Solution Architecture Document available in the SUSE Partner Program.

For simplicity, this guide uses the Azure Cloud Shell to perform the deployment, as it provides easy access to most of the required tooling. It is possible to easily use a local Linux or macOS computer, but some commands may need modification or omission.

=== Audience

// This document is intended for

Everybody who want to build a secure SAP Landscape in Azure.

== Technical overview

// Description
// Architecture diagram

The architecture for the deployment is similar to the one below:

ifeval::[ "{cloud}" == "Azure" ]
image::Project_Mayerhofer_general_v1.4.svg[title=Azure Automation Architecture,scaledwidth=99%]
endif::[]

The project will perform the following actions:

* Deploy an Azure VPN Gateway as entry point to the Azure Hub/Spoke network
* Deploy an Azure Hub network
* Deploy FortiGate and FortiADC HA pairs in the HUB
* Deploy a Spoke-Network for the SAP Landscape
* Deploying instances SAP HANA Instance (in a HA scenario)
* Deploy SAP Netweaver/S4HANA Instances (in a HA scenario)
* Configuring the operating system for SAP workload
If a HA scenario is chosen, additionally
* Configuring SAP HANA System Replication (HSR)
* Configure SAP Netweaver ENQ Replication
* Configuring SUSE Linux Enterprise High Availability cluster components

// Minimum requirements (prerequisites) for this guide
// * Requirement 1 https://url[url]
// * Requirement 2 https://url[url]
The minimum requirements to get started are:

* an Azure account, to be able to deploy the solution
* an SAP account, to get the needed SAP media for the solution you want to deploy

=== The Fortinet Solutions

==== FortiGate
FortiGate Next Generation Firewall (NGFW) enable security-driven networking and consolidate industry-leading security capabilities such as intrusion prevention system (IPS), web filtering, secure sockets layer (SSL) inspection, and automated threat protection.
This deployment contains an HA Cluster of two FortiGate instances which provides segmentation within the HUB network and provides segmentation and security inspection for communication to and from the SAP SPOKE network.
The FortiGate Firewall Cluster can be accessed via HTTPs or via SSH on their public IP and credentials output by the terraform deployment.

Further Information on the configuration can be found within the the following Guides:

. http://docs.fortinet.com/document/fortigate-public-cloud/7.0.0/azure-administration-guide/128029/about-fortigate-vm-for-azure[FortiGate Documentation for Azure]
. https://docs.fortinet.com/document/fortigate/7.0.4/administration-guide/954635/getting-started[FortiGate Administration Guide]
. https://www.fortinet.com/sap[Fortinet Solutions for SAP]

==== FortiADC
FortiADC is an advanced Application Delivery Controller (ADC) that ensures application availability, application security, and
application optimization. FortiADC offers advanced security features (WAF, DDoS, and AV) and application connectors
for easy deployment and full visibility to your networks and applications.

FortiADC provides a dedicated Connector specific for SAP to allow the same load balancing capabilities like SAP Web Dispatcher with the addition of a Web Application Firewall (WAF).
This deployment contains a Cluster of two FortiADC Instances.
The FortiADC Cluster can be accessed via HTTPs or via SSH on their public IP and credentials output by the terraform deployment.

Further Information on the configuration can be found within the the following Guides:

. https://docs.fortinet.com/document/fortiadc/6.2.2/handbook/105358/introduction[FortiADC Administration Guide]
. https://docs.fortinet.com/document/fortiadc/6.2.2/handbook/382594/sap-connector[FortiADC SAP Connector Configuration]
. https://www.fortinet.com/sap[Fortinet Solutions for SAP]


== Installation


//=== Install from the SUSE Rancher Apps & Marketplace
//Use an ordered list for steps.
//Link elements (with '+') to maintain consistent indentation.
//E.g.:
//. From the top-left Rancher menu select __Apps & Marketplace__.
//From here, you will be taken to the __Charts__ page.
//Notice __underscores__ are emphasis to indicate a specific
//UI element.
//+
//image::kubernetes_gs_suseprod_partner-partnerprod_image_description.png[scaledwidth="85%", align="center"]

//. Step 2.

//. Step 3.

//=== Install using generic/manual steps

//Provide steps or link to existing document.
//[source, bash]
//---
//echo "Use code blocks for command-line steps."
//---

The overall process looks like

image::Flow-SAPAutomation2.png[title=Workflow of the Automation,scaledwidth=85%,align="center"]

INFO: The project uses PAYG images by default, so you do not require any subscriptions or license files to provide a simple easy start. But you can use BYOS/BYOL too. The needed changes are mentioned in the tfvars example files.

=== Configuring the Cloud Shell

ifeval::[ "{cloud}" == "Azure" ]
Start an Azure Cloud Shell simply from the top menu within Azure Portal. It is the small icon with the commandline prompt.

When Cloud Shell is started the first time you can select between "Bash" and "Powershell". Please use "Bash"

The Cloud Shell is a managed service by Microsoft, and comes with the most popular command-line tools and language support you need.
The Cloud Shell also securely authenticates automatically for instant access to your resources through the Azure CLI or Azure PowerShell cmdlets.
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
Start an AWS Cloud Shell from the AWS Services menu.

When the shell has launched, the next step is to configure the CLI and provide API access keys to allow the creation of AWS resources and infrastructure.
The API Keys are created from the AWS console. For more details, refer to
https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html

The quickest way to configure the AWS CLI is by running the command:

----
aws configure
----

This is described in the documentation linked above.

The command generates a file in `$HOME/.aws/credentials` which is referenced later.

IMPORTANT: The user specified in this step needs certain AWS permissions to ensure the deployment is successful.
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
IMPORTANT: The GCP user specified in the below-mentioned step needs certain GCP Project permissions to ensure the deployment is successful.
For simplification, the GCP user used in this guide has the _Project Owner_ IAM role.

The following procedures show the minimum steps required to prepare the GCP infrastructure to host the SAP HANA environment:

. Create a new GCP Project to host the SAP HANA environment.
. Enable the GCP Compute Engine API.
. Using the newly created GCP project console, start a GCP Cloud Shell.
. Using the GCP Cloud Shell, create a new GCP Key for the default GCP Service Account. The key will be used by Terraform to access the GCP infrastructure.
+
TIP: For more details about creating GCP Service Account key, refer to https://cloud.google.com/iam/docs/creating-managing-service-account-keys#iam-service-account-keys-create-gcloud
+
NOTE: For simplification, the default GCP Service Account will be used in this guide.
endif::[]

=== Ensuring Terraform is installed

ifeval::[ "{cloud}" == "Azure" ]
Terraform is already deployed as part of the Azure Cloud Shell. The following command output shows the Terraform version used at the time of creating this guide:
----
$ terraform -v
Terraform v1.0.0
on linux_amd64
----

The Project could run with higher versions too, but you may get some warnings.
endif::[]

ifeval::[ "{cloud}" == "AWS" ]

Terraform is not currently deployed as part of the AWS Cloud Shell, in this step Terraform is downloaded and installed to the `~/bin` directory.
Update the command below with the latest version of Terraform as needed.
As Terraform is updated on a regular basis, it may be necessary to update after installation, or specify a different version to download.

From the `~` directory, run:

----
mkdir ~/bin
cd ~/bin
wget https://releases.hashicorp.com/terraform/0.14.7/terraform_0.14.7_linux_amd64.zip
unzip terraform_0.14.7_linux_amd64.zip
cd ~
----

Check Terraform is working by running:

----
terraform --version
----

endif::[]

ifeval::[ "{cloud}" == "GCP" ]
Terraform is already deployed as part of the GCP Cloud Shell. The following command output shows the Terraform version used at the time of creating this guide:

----
$ terraform -v
Terraform v1.0.0
on linux_amd64
----
endif::[]


=== Preparing the SAP media

With the correct entitlement, SAP media can be downloaded from the SAP Web site at https://support.sap.com/en/my-support/software-downloads.html. The SAP Media needs to be made available so it can be accessed during the deployment.

The SUSE Automation for SAP Applications project allows for three methods for presenting the SAP media:

. SAR file and SAPCAR executable (SAP HANA Database only)
. Multipart exe/RAR files
. Extracted media

The different formats have some benefits and drawbacks:

. The compressed archives (SAR and RAR) provide a simple copy to the cloud but a longer install time due to extracting it during the process
. The uncompressed/extracted media are the fastest install, but more files are copied to the cloud share, which also takes time in forehand as preparation.

ifeval::[ "{cloud}" != "GCP" ]
We use here in the example the compressed archives for the install (exe/RAR) as its the easiest to download and upload to the cloud share.

This guide uses the most recent SAP HANA media, {hana_version}. The SAP HANA media file name downloaded at the time of creating this guide is `{hana_archive_version}`.
Follow the SAP instructions to download the SAP HANA media.
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
In this example, we use the extracted archives for the installation as it is the fastest deployment way.
endif::[]

[NOTE]
====
It depends on the used way to download the SAP media. If multiple compressed files are downloads, the official SAP extract tool `SAPCAR` tool must be used to extract the SAP media.

Extracted SAP Media will contain a lot of extracted files. Depending on your network speed, it can consume a lot of time to upload the extracted SAP media files. The CSP's infrastructure provide normally a good network speed, but if you have problems a way would be to create a workstation machine in the cloud to download/upload the SAP media.

Also have a look how far your share for the SAP media is from the region where you install the machines - it should be best the same region.
====

[TIP]
====
It's a good practice to have the SAP Media versioned on the cloud share in order to build a library for automatic installs and (re)deployments.
So think about you SAP media structure.

As an *example*, here how a full SAP Application media tree (in a compressed format) for a S/4HANA version 1809 install would look like:
----
<FS>/s4hana1809
       ├SWPM_CD
       │  ├SWPM20SP07_5-80003424.SAR
       │  └SAPCAR_721-20010450.EXE
       │
       ├EXP_CD
       │  ├S4CORE104_INST_EXPORT_1.zip
       │  ├S4CORE104_INST_EXPORT_2.zip
       │  └...
       ├DBCLIENT_CD
       │  └IMDB_CLIENT20_005_111-80002082.SAR
       ├BASKET_CD
       │   ├SAPHOSTAGENT24_24-20009394.SAR
       │   ├igshelper_4-10010245.sar
       │   ├igsexe_1-80001746.sar
       │   ├SAPEXEDB_400-80000698.SAR
       │   └SAPEXE_400-80000699.SAR
       └HANA
          ├51053061_part1.exe
          ├51053061_part2.rar
          ├51053061_part3.rar
          └51053061_part4.rar


 HANA       : contains the HANA Database install
 BASKET_ CD : contains SAP kernel, patch + more like hostagent.
 DBCLIENT_CD: contains the package corresponding to DB CLIENT, e.g HANA
 EXP_CD     : contains the package corresponding to EXPORT files
 SWPM_CD    : must contain the .exe file corresponding to SAPCAR and the
              .sar file corresponding to SWPM.
              The file suffix must be .exe and .sar.
----
====


In the next steps we show as example a simple HANA install download.

ifeval::[ "{cloud}" == "Azure" ]
For Azure, an Azure File Share is used to host the SAP media.

Using the Azure Portal or the Azure cli perform the following actions:

* Create a storage account
* Create a folder within the Storage Account, for example "mysapmedia"
* Upload the SAP media files to the Storage Account

You will need later the Storage Account name and one key as password for the terraform run.

image::TRD_SLES-SAP-HA-automation-quickstart-cloud-Azure-Storage.png[width=470]

endif::[]

ifeval::[ "{cloud}" == "AWS" ]
For AWS, an S3 bucket is used.

Using the AWS Console, perform the following actions:

* Create an S3 bucket. (The example shows a bucket called mysapmedia, but a unique name should be used.)
* Create a folder within the bucket.
* Upload the SAP media to the folder in the S3 bucket.

image::TRD_SLES-SAP-HA-automation-quickstart-cloud-s3-bucket.png[width=470]

endif::[]

ifeval::[ "{cloud}" == "GCP" ]
A GCP Cloud Storage bucket is used to host the SAP HANA extracted media. Using the GCP Console, perform the following actions:

* Create a new GCP bucket. (The example shows a GCP Cloud Storage bucket called `mysapmedia`, but a unique name should be used.)
* Upload the SAP HANA media extracted directory to the GCP Cloud Storage bucket. The following figure shows the uploaded SAP HANA media extracted directory:
+
image::trd_sles-sap-ha-automation-quickstart-cloud-gcp-bucket.png[title=SAP HANA GCP Storage Bucket, scaledwidth=99%]
endif::[]



=== Downloading and configuring the Automation code

The SUSE and Fortinet SAP Automation code is published in GitHub.

ifeval::[ "{cloud}" == "GCP"]
The following commands will:

. Create a new GCP Cloud Shell directory to host the SUSE SAP automation code
. Change directory to the newly created directory

----
$ mkdir suse-sap-automation
$ cd suse-sap-automation
----
endif::[]


The following command will clone the project to the Cloud Shell ready for configuration.
// fix the URL and Branch
// $ git clone --depth 1 --branch 7.2.0 https://github.com/SUSE/Project-Mayerhofen.git
// other way would be use the tagged version and download the zip file
----
$ git clone --depth 1 https://github.com/SUSE/Project-Mayerhofen.git
----
The --depth 1 make sure that you only get the latest commits and not the whole history for the project

ifeval::[ "{cloud}" == "GCP" ]
Next, move the generated GCP Service Account Key to the SUSE SAP Automation GCP directory:
----
$ cd ~
$ cp <GCP Service Account Key> suse-sap-automation/ha-sap-terraform-deployments/gcp
----
endif::[]

NOTE: If the following SSH keys already exist, the next step can be skipped.

Then, generate SSH key pairs to allow for accessing the SAP HANA instances:
----
#optional if ssh-keys already exist
$ cd ~
$ ssh-keygen -q -t rsa -N '' -f  ~/.ssh/id_rsa
----

==== Configuring the deployment options and modifying the Terraform variables

The files that need to be configured are contained in a subdirectory of the project. Use that as the working directory:

ifeval::[ "{cloud}" == "Azure" ]
The project itself has code for several cloud service providers and virtual infrastructures, but the Fortinet addition is at the moment only available for Azure. Please get in contact with us if you would like to see it for others too.

----
cd ~/ha-sap-terraform-deployments/azure
----
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
----
cd ~/ha-sap-terraform-deployments/aws
----
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
----
$ cd ~/suse-sap-automation/ha-sap-terraform-deployments/gcp
----
endif::[]


A Terraform example template is provided. For a demo environment consisting of a simple SAP Scenario, only a handful of parameters will need to be changed, as most of the possible values come with a useful default as a good and safe starting point.


Copy the Terraform example file to `terraform.tfvars`:
----
$ cp terraform.tfvars.example terraform.tfvars
----


Edit the `terraform.tfvars` file and modify it as explained below.
As there are many option possible, we will describe only one path which will setup the Fortinet appliances and a HANA HSR and a NW HA.
If you are duplicating the lines before modification, ensure the original is commented out, or the deployment will fail.

ifeval::[ "{cloud}" == "Azure" ]
// nothing needed for AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]

With this parameter, Terraform will use the AWS credentials file created above. It is the simplest way to provide credentials for the deployment.
----
aws_credentials = "~/.aws/credentials"
----

If not used, ensure the following lines are commented out or the deployment will fail.

----
#aws_access_key_id = my-access-key-id
#aws_secret_access_key = my-secret-access-key
----
endif::[]


ifeval::[ "{cloud}" == "GCP" ]
First, choose the GCP Project ID for the deployment:
----
# GCP project id
project = "<PROJECT ID>"
----

Then, choose the GCP Service Account Key file path. With the following parameter, Terraform will use the GCP Service Account Key file created above:
----
# Credentials file for GCP
gcp_credentials_file = "<GCP Service Account Key Path and Name>"
----
endif::[]


Choose the region for the deployment, for example:
ifeval::[ "{cloud}" == "Azure" ]
----
# Region where to deploy the configuration
az_region = "westeurope"
----
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
----
# Region where to deploy the configuration
aws_region = "eu-central-1"
----
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
----
# Region where to deploy the configuration
region = "europe-west1"
----
endif::[]

The following parameters select the version of {sles4sap} to deploy:

NOTE: The values shown would also be the defaults used. All defaults point to PAYG images for an easy start. So you only need to enable or change the variables if you want something different.

ifeval::[ "{cloud}" == "Azure" ]
Please set this variable to give the deployment a unique name, which will be shown at most resources as suffix, for example
----
# The name must be unique among different deployments
deployment_name = "mydev123"
----
endif::[]

Provide a name for the OS user for the machines
----
# Admin user for the created machines
admin_user = "cloudadmin"
----

ifeval::[ "{cloud}" == "Azure" ]
If you want to go with the default PAYG image you do NOT need to change anything here. However, if you want to use a BYOS image, you need to make sure that you provide the subscription key and email to register against SUSE Customer Center (SCC). This requires also that you have not blocked internet access in order to reach SCC.
// this is also the default
----
#os_image = "sles-sap-15-sp2:gen2"
----
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
 // this is also the default
For simplicity, the 'os_owner' is set to use PAYG instances from the AWS Marketplace. If an existing SUSE subscription needs to be used, this section can be changed to use BYOS images. Refer to the project documentation.
----
#os_image = "suse-sles-sap-15-sp2"
#os_owner = "679593333241"
----
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
// this is also the default
----
#os_image = "suse-sap-cloud/sles-15-sp2-sap"
----
endif::[]

Next, enter the path for the public and private SSH keys that were generated earlier. Below is an example using the default created SSH keys:
----
# SSH Public key location to configure access to the remote instances
public_key  = "~/.ssh/id_rsa.pub"

# Private SSH Key location
private_key = "~/.ssh/id_rsa"
----

ifeval::[ "{cloud}" == "Azure" ]
As the deployment need a Hub/Spoke network setup, please set the following
----
network_topology = "hub_spoke"
vnet_hub_create = true
bastion_enabled = true
spoke_name = "sap-1"
----

Please have a look at the other provided examples if you would like to use a existing hub/spoke network.
The deployment with Fortinet *requires* the hub/spoke architecture.
endif::[]

To keep the cluster architecture and deployment simple and to provide additional packages needed to deploy, uncomment and set the following parameters:
----
ha_sap_deployment_repo = "https://download.opensuse.org/repositories/network:/ha-clustering:/sap-deployments:/v7/"
----

Then, enable the `pre_deployment` parameter:
----
pre_deployment = true
----

ifeval::[ "{cloud}" == "Azure" ]
The Jumphost server (Bastion Host Server) is enabled by default, and provide the public IP address to the database.
Otherwise the deployed instances will all get a public ip

With the Fortinet additon it is mandatory to have the bastion host enabled.
----
bastion_enabled = true
----
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
// Question to Ab:  Why do you disable the bastion?
The Jumphost server (Bastion Host Server) is enabled by default, and provide the public IP address to the database.
Otherwise the two HANA servers will get a public ip.

Disable the Bastion Host Server creation parameter `bastion_enabled`:
----
bastion_enabled = false
----
endif::[]

Next, set which SAP HANA instance machine type should be selected:
The default is set to some standard types, and you only need to enable and change the variable if you want other sizes.

ifeval::[ "{cloud}" == "Azure" ]
----
#hana_vm_size = "Standard_E8s_v3"
----
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
----
#hana_instancetype = "r3.xlarge"
----

Next set the hostname for the instances, without the domain part
----
name = "hana"
----
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
----
#machine_type = "n1-highmem-32"
----
endif::[]


Modify the following parameter to point to SAP media that was uploaded to the storage location:

ifeval::[ "{cloud}" == "Azure" ]
----
storage_account_name = "YOUR_STORAGE_ACCOUNT_NAME"
storage_account_key = "YOUR_STORAGE_ACCOUNT_KEY"
----

The "hana_inst_master" needs to be set according to your settings of the file share you created before.
e.g.
// all three should be better aligned and build on a fixed structure
----
hana_inst_master = "//YOUR_STORAGE_ACCOUNT_NAME.file.core.windows.net/mysapmedia"

hana_archive_file = "s4hana1809/HANA/{hana_archive_version}.exe"
----
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
----
hana_inst_master = "s3://mysapmedia/s4hana1809/HANA"

hana_archive_file = "{hana_archive_version}.exe"
----

endif::[]

ifeval::[ "{cloud}" == "GCP" ]
[subs="attributes"]
----
hana_inst_master = "mysapmedia/{hana_archive_version}"
----
endif::[]

To create the cluster, we need to set this parameter to true, otherwise only single system is created.
----
# Enable system replication and HA cluster
hana_ha_enabled = true
----

Finally, to ensure a fully automated deployment, it is possible to set passwords within the `terraform.tfvars` file. Uncomment and set the following parameters to your own value:
----
hana_master_password = "SAP_Pass123"
----

NOTE: If the parameters are not set in the `terraform.tfvars` file, they must be entered when running the deployment.

IMPORTANT: All passwords must conform to SAP password policies or the deployment will fail.

Optional: If a monitoring instance should be as part of the deployment, find and uncomment the following:

----
monitoring_enabled = true
----

As we want to deploy a Netweaver scenario, we need a highly available NFS-Share. To build one with Linux we use DRBD and pacemaker
----
# Enable drbd cluster
drbd_enabled = true
----

There is a similar set of variables needed for Netweaver as we had for HANA
----
netweaver_enabled = true
netweaver_app_server_count = 2
netweaver_master_password = "SuSE1234x"
----

The Netweaver product version which should be installed need to be set via product id
Please have a look at the examples to pick the right one for your deployment
----
netweaver_product_id = "NW750.HDB.ABAPHA"
----

Again we need to provide the file share for the Netweaver media, which normally are at the same share
----
netweaver_storage_account_name = "YOUR_STORAGE_ACCOUNT_NAME
netweaver_storage_account_key = "YOUR_STORAGE_ACCOUNT_KEY"
netweaver_storage_account = "//YOUR_STORAGE_ACCOUNT_NAME.file.core.windows.net/path/to/your/nw/installation/master"
----

And where the folders for the SAP installer are, relative to the mountpoint of the Netweaver storage account
----
netweaver_sapcar_exe = "your_sapcar_exe_file_path"
netweaver_swpm_sar = "your_swpm_sar_file_path"
netweaver_sapexe_folder   =  "your_download_basket"
netweaver_additional_dvds = ["your_export_folder", "your_hdbclient_folder"]
----

In order to additionally deploy the Fortinet services, remember we need to choose a HUB-SPOKE network scenario and enable the flag for Fortinet
----
fortinet_enabled=true
----

We need credentials for the FortiGate instances too, please change it to your values
----
fortigate_vm_username    = "azureuser"
fortigate_vm_password    = "SuSE1234x"
----

And for the FortiADC instances

----
fortiadc_vm_username    = "azureuser"
fortiadc_vm_password    = "SuSE1234x"
----

All other values should be set by defaults and use the PAYG instances of Fortinet.

If you want to use BYOL with your Fortinet services, you need to buy and then copy the Fortinet license files into the directory
----
cp *.lic ~/ha-sap-terraform-deployments/azure
----

Here is a example tfvars file for a Fortinet deployment in the Hub, and a HA NW750 install with a HA HANA setup in the spoke:
----
az_region = "westeurope"
deployment_name = "psfn6"
admin_user = "cloudadmin"
public_key  = "/home/<YOURUSER>/.ssh/azure.id_rsa.pub"
private_key = "/home/<YOURUSER>.ssh/azure.id_rsa"
cluster_ssh_pub = "salt://sshkeys/cluster.id_rsa.pub"
cluster_ssh_key = "salt://sshkeys/cluster.id_rsa"
network_topology = "hub_spoke"
vnet_hub_create = true
bastion_enabled = true
spoke_name = "sap-1"
ha_sap_deployment_repo = "https://download.opensuse.org/repositories/network:/ha-clustering:/sap-deployments:/devel/"
pre_deployment = true
hana_count = "2"
storage_account_name = "<YOURSTORAGEACCOUNT>"
storage_account_key = "<YOURACCOUNTKEY>"
hana_inst_master = "//YOURSTORAGEACCOUNT>.file.core.windows.net/sapbits/HANA/51054623"
hana_ha_enabled = true
hana_master_password = "xxxxxxxx"
monitoring_enabled = true
drbd_enabled = true
netweaver_enabled = true
netweaver_app_server_count = 2
netweaver_master_password = "xxxxxxxx"
netweaver_ha_enabled = true
netweaver_cluster_fencing_mechanism = "sbd"
netweaver_product_id = "NW750.HDB.ABAPHA"
netweaver_storage_account_name = "<YOURSTORAGEACCOUNT>"
netweaver_storage_account_key = "<YOURACCOUNTKEY>"
netweaver_storage_account = "<YOURSTORAGEACCOUNT>.file.core.windows.net/sapbits"
netweaver_sapcar_exe = "netweaver/SAPCAR"
netweaver_swpm_sar   = "netweaver/SWPM10SP26_6-20009701.SAR"
netweaver_sapexe_folder = "netweaver/kernel_nw75_sar"
netweaver_additional_dvds = ["netweaver/51050829_3", "HANA/51054623/DATA_UNITS/HDB_CLIENT_LINUX_X86_64"]
fortinet_enabled         = true
fortigate_vm_username    = "azureuser"
fortigate_vm_password    = "xxxxxxxxx"
fortiadc_vm_username    = "azureuser"
fortiadc_vm_password    = "xxxxxxxxx"
----

[NOTE]
====
Here how parts of the Fortinet section looks for using BYOL images:

----
..
fortinet_vm_license_type = "byol"

fortigate_a_license_file = "license_fortigate_a.lic"
fortigate_b_license_file = "license_fortigate_b.lic"
fortigate_os_image = "fortinet:fortinet_fortigate-vm_v5:fortinet_fg-vm:7.0.2"
..
fortinet_enabled         = true
fortigate_vm_username    = "azureuser"
fortigate_vm_password    = "xxxxxxxxx"
fortiadc_vm_username    = "azureuser"
fortiadc_vm_password    = "xxxxxxxxx"
..
fortiadc_a_license_file = "license_fortiadc_a.lic"
fortiadc_b_license_file = "license_fortiadc_b.lic"
fortiadc_os_image = "fortinet:fortinet-fortiadc:fad-vm-byol:6.1.3"
..
----
====



=== Finalizing the automation configuration

ifeval::[ "{cloud}" == "Azure" ]
Ensure that the subscription used to host the SAP HANA HA cluster meets the infrastructure quota requirements. For more info, refer to https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/get-started
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
==== Subscribing to the AWS Marketplace offer

To automatically deploy instances from the AWS Marketplace, ensure to *Subscribe* to the offering.

A link for {sles4sap} 15 SP2 can be found at link:https://aws.amazon.com/marketplace/server/procurement?productId=e9701ac9-43ee-4dda-b944-17c6c231c8db[].

If a different version of {sles4sap} is required, subscribe to the relevant version on the marketplace.


=== Configuring IAM policies

If the deployment is being run from the root user of the AWS account, or if the user specified when configuring the AWS CLI has
Admin privileges in your AWS account, you can skip this step.

If using an IAM user with limited permissions, additional IAM rights may be required as IAM policies are created and attached
during deployment, for example to access and manage EC2 instances, S3 buckets, IAM (to create roles and policies) and EFS storage.

There are two options available to achieve this:

a. Attach the *IAMFullAccess* policy to the user executing the project. However, this is not recommended.
b. The recommended method is to create a new IAM policy and attach it to the desired user.

TIP: Depending on your own IAM rights, you may need to reach out to an AWS administrator for your account to set this up for you.

Create the following policy and attach it to the IAM user running the deployment:

----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "iam:CreateServiceLinkedRole",
                "iam:PassRole",
                "iam:CreateRole",
                "iam:TagRole",
                "iam:GetRole",
                "iam:DeleteRole",
                "iam:GetRolePolicy",
                "iam:PutRolePolicy",
                "iam:DeleteRolePolicy",
                "iam:ListInstanceProfilesForRole",
                "iam:CreateInstanceProfile",
                "iam:GetInstanceProfile",
                "iam:RemoveRoleFromInstanceProfile",
                "iam:DeleteInstanceProfile",
                "iam:AddRoleToInstanceProfile"
            ],
            "Resource": "*"
        }
    ]
}
----
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
Ensure that the GCP Project used to host the SAP HANA HA cluster meets the infrastructure quota requirements set by Google Cloud. For more info, refer to https://cloud.google.com/solutions/sap/docs/sap-hana-planning-guide#quotas
endif::[]


=== Deploying the project

Terraform will create and name resources when running the deployment based on the "workspace" in use.
It is highly recommended to create a unique workspace from which to run the deployment.

----
$ terraform init
$ terraform plan
$ terraform apply
----

TIP: The Cloud Shell has a timeout of around 20 minutes and the shell will close if left
unattended, resulting in a failed deployment.  It is strongly advised to retain focus on the Cloud Shell window to ensure the timeout does not occur as a full deployment could take nearly four hours.


ifeval::[ "{cloud}" == "Azure" ]
If successful, the output will be the public IP addresses for the cluster nodes like below.

----

module.netweaver_node.module.netweaver_provision.null_resource.provision[3] (remote-exec): Total states run:     38
module.netweaver_node.module.netweaver_provision.null_resource.provision[3] (remote-exec): Total run time: 6339.329 s
module.netweaver_node.module.netweaver_provision.null_resource.provision[3] (remote-exec): Tue Jan 25 19:12:24 UTC 2022::vmnetweaver04::[INFO] deployment done

Apply complete! Resources: 227 added, 0 changed, 0 destroyed.

Outputs:

bastion_public_ip = "20.2.3.43"
cluster_nodes_ip = [
  [
    "10.74.1.10",
    "10.74.1.11",
  ],
]
cluster_nodes_name = [
  [
    "vmhana01",
    "vmhana02",
  ],
]
cluster_nodes_public_ip = [
  [],
]
cluster_nodes_public_name = [
  [],
]
drbd_ip = [
  "10.74.1.6",
  "10.74.1.7",
]
drbd_name = [
  "vmdrbd01",
  "vmdrbd02",
]
drbd_public_ip = []
drbd_public_name = []
fortiadc_a_url = "https://20.2.3.44:41443"
fortiadc_b_url = "https://20.2.3.44:51443"
fortigate_a_url = "https://20.2.3.41"
fortigate_b_url = "https://20.2.3.42"
fortigate_url = "https://20.2.3.44"
iscsi_srv_ip = [
  "10.74.1.4",
]
iscsisrv_name = [
  "vmiscsi01",
]
iscsisrv_public_ip = []
iscsisrv_public_name = []
monitoring_ip = ""
monitoring_name = ""
monitoring_public_ip = ""
monitoring_public_name = ""
netweaver_ip = [
  "10.74.1.60",
  "10.74.1.61",
  "10.74.1.62",
  "10.74.1.63",
]
netweaver_name = [
  "vmnetweaver01",
  "vmnetweaver02",
  "vmnetweaver03",
  "vmnetweaver04",
]
netweaver_public_ip = []
netweaver_public_name = []
----
endif::[]

ifeval::[ "{cloud}" == "AWS" ]

If successful, the output will be the public IP addresses for the cluster nodes similar to the output below.

----
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec):               ----------
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec):               method:
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec):                   update
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec):               url:
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec):                   /tmp/cluster.config
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec):
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Summary for local
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): -------------
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Succeeded: 34 (changed=26)
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Failed:     0
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): -------------
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Total states run:     34
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Total run time: 1384.111 s
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Thu Aug 26 11:18:58 UTC 2021::hana02::[INFO] deployment done
module.hana_node.module.hana_provision.null_resource.provision[1]: Creation complete after 43m28s [id=xxxxxxxxxxxxxx]

Apply complete! Resources: 33 added, 0 changed, 0 destroyed.

Outputs:

cluster_nodes_ip = [
  "10.0.1.10",
  "10.0.2.11",
]
cluster_nodes_name = [
  "i-0a553e68e157ed667",
  "i-00887a206e454b0ab",
]
cluster_nodes_public_ip = [
  "18.192.212.32",
  "3.66.190.173",
]
cluster_nodes_public_name = [
  "ec2-18-192-212-32.eu-central-1.compute.amazonaws.com",
  "ec2-3-66-190-173.eu-central-1.compute.amazonaws.com",
]
drbd_ip = []
drbd_name = []
drbd_public_ip = []
drbd_public_name = []
iscsisrv_ip = ""
iscsisrv_name = ""
iscsisrv_public_ip = ""
iscsisrv_public_name = ""
monitoring_ip = ""
monitoring_name = ""
monitoring_public_ip = ""
monitoring_public_name = ""
netweaver_ip = []
netweaver_name = []
netweaver_public_ip = []
netweaver_public_name = []
----

endif::[]

ifeval::[ "{cloud}" == "GCP" ]

If successful, the output lists the public IP addresses for the cluster nodes. This will look similar to the following considering the different Public IP addresses for each deployment:
-----
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Summary for local
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): -------------
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Succeeded: 33 (changed=23)
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Failed:     0
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): -------------
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Total states run:     33
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Total run time: 1028.670 s
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Wed Jun 23 10:14:22 UTC 2021::demo-hana02::[INFO] deployment done
module.hana_node.module.hana_provision.null_resource.provision[1]: Creation complete after 27m24s [id=3463680564647535989]

Apply complete! Resources: 26 added, 0 changed, 0 destroyed.

Outputs:

bastion_public_ip = ""
cluster_nodes_ip = [
  "10.0.0.10",
  "10.0.0.11",
]
cluster_nodes_name = [
  "demo-hana01",
  "demo-hana02",
]
cluster_nodes_public_ip = tolist([
  "34.127.16.75",
  "34.145.94.26",
])
cluster_nodes_public_name = []
drbd_ip = []
drbd_name = []
drbd_public_ip = []
drbd_public_name = []
iscsisrv_ip = ""
iscsisrv_name = ""
iscsisrv_public_ip = ""
iscsisrv_public_name = []
monitoring_ip = ""
monitoring_name = ""
monitoring_public_ip = ""
monitoring_public_name = ""
netweaver_ip = []
netweaver_name = []
netweaver_public_ip = []
netweaver_public_name = []
-----
endif::[]

=== Tearing down

When finished with the deployment, or even if the deployment has failed, ensure that Terraform is used to tear down the environment.

----
$ terraform destroy
----

ifeval::[ "{cloud}" == "Azure" ]
This method will ensure all resource, such as instances, volumes, networks, etc are cleaned up.
You need to delete the following components manually:
* Azure File Store
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
This method will ensure all AWS resource, such as instances, volumes, VPCs, and IAM roles, are cleaned up.
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
This method will ensure all GCP resources, such as instances, disks, VPCs, and roles are cleaned up.
You need to delete the following GCP components manually:

* GCP Cloud Storage bucket
* GCP Project
endif::[]


//== Use case and demonstration

//Describe use case and demonstration.

//[NOTE]
//====
// Use admonitions (i.e., NOTE, TIP, IMPORTANT, CAUTION, WARNING) as needed.
//====


//Use sections to break up major actions, such as:
//=== Prepare the demonstration environment (beyond installation)
//=== Perform action (to illustrate capability)
//=== Examine results (verifying capability)
//Use ordered lists for steps.
//Minimize/consolidate images whenever possible.



== Summary

//Summarize the motivation
//Summarize what was demonstrated
//Hint at other capabilities.
Combining the SUSE SAP automation with a automated Fortinet Security Fabric provisioning results in an architecture which provides added security, optimized connectivity and faster rollout of an SAP landscape in Azure.


== Additional resources

Learn more about the capabilities of {title} with these additional references.
//Use an unordred lists for references

* https://www.fortinet.com/sap[Fortinet Solutions for SAP]
* https://www.suse.com/partners/alliance/sap/[SUSE Solutions for SAP]
* https://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/sap-workload-automation-suse[SAP workload automation using SUSE on Azure]
//* https://url/[reference-title]


++++
<?pdfpagebreak?>
++++

// Standard SUSE Technical Reference Documents includes

:leveloffset: 0

== Legal notice
include::../../../../common/adoc/common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::../../../../common/adoc/common_gfdl1.2_i.adoc[]

//end
