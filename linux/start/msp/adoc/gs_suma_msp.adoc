:docinfo:
include::./common_docinfo_vars.adoc[]

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// General comments
// Keep in mind that this is a "getting started" guide and the
//   audience that you are trying to reach.
// Leverage ASCIIDoc features to make this document readable and usable:
//   - Text highlights (follow SUSE style guides)
//   - Admonitions (i.e., NOTE, TIP, IMPORTANT, CAUTION, WARNING)
//   - Code blocks
//   - Lists (ordered and unordered, as appropriate)
//   - Links (to other resources)
//   - Images
//     - Place image files under the ./media directory tree
//       (e.g., ./media/src/svg, ./media/src/png)
//     - Format preference: svg > png > jpg
//     - Consolidate images wherever possible
//       (i.e., don't use two images when one conveys the message)
//   - Use sections and subsections to organize and group related
//     steps.
// 
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Document attributes and variables
//
// NOTES:
// 1. Update variables below and adjust docbook file accordingly.
// 2. Comment out any variables/attributes not used.
// 3. Follow the pattern to include additional variables.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// organization - do NOT modify
// -
:trd: Technical Reference Documentation
:type: Getting Started
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// document
// -
:title: Title
:subtitle: Subtitle

:product1: SUSE Manager
:product1_full: SUSE Manager
:product1_version: 4.3
:product1_url: Primary SUSE product/platform URL (without the protocol)
:product1_docs: documentation.suse.com/suma/{product1_version}/en/suse-manager
:product2: Secondary product short name
:product2_full: Secondary product full name
:product2_url: Secondary product URL

:usecase: A few words to identify high level use case (e.g., Database-as-a-Service, edge analytics in healthcare, Kubernetes-native object storage)

:executive_summary: A brief statement of what this document provides (e.g., This document provides a brief introduction to implementing {usecase} with {product2_full} and {product1_full}.)
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// contributor
// specify information about authors, editors, and others here,
// then update docinfo file as appropriate
// -
:author1_firstname: first (given) name
:author1_surname: surname
:author1_jobtitle: job title
:author1_orgname: organization affiliation
//:author2_firstname: first (given) name
//:author2_surname: surname
//:author2_jobtitle: job title
//:author2_orgname: organization affiliation
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// miscellaneous
// define any additional variables here for use within the document
// -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


= {title}: {subtitle}


== Introduction

The Linux footprint of enterprise IT landscapes continues to grow.
This growth can be particularly problematic for a managed services provider (MSP), who may need to manage systems lifecycles for many clients.
You need a management tool that can scale with your business needs.

{product1_full} {product1_version} is an enterprise, open source management solution for software-defined infrastructure.
{product1} is designed to help enterprises reduce complexity and regain control of IT assets with a single tool.
It provides systems asset management, systems provisioning, and automated software management to help you keep your systems up-to-date and secure.

{product1} easily scales to satisfy your technical and business requirements without compromising on operational performance.
Seamlessly manage a broad spectrum of Linux systems on a variety of hardware architectures and hypervisors as well as container, IoT, and cloud platforms.
Unify system lifecycle management across your entire IT landscape.


{product1} delivers flexibility and peace of mind that you can grow and manage your IT landscape efficiently and effectively.


=== Scope

This document explains an architecture that enables service providers to leverage {product1} to offer systems lifecycle management as a service to their clients.
This document describes the use of a central server to consolidate management operations while maintaining the advantages of independent servers for each customer.


=== Audience

This document is intended for IT professionals at managed services providers, who are responsible for delivering systems lifecycle mangement for their customers.
You should have a basic familiarity with {product1} and with typical procedures for provisioning and patching Linux systems to get the most out of this document.


== Solution Design

Managing a large and growing IT footprint can be a challenge for any enterprise.
Managed services providers (MSPs) must do this for multiple clients.
A single {product1} server could be used to manage all customer systems.
But there are key concerns with this approach, such as:

Scaling::
{product1} is designed as a single server that can manage up to 10000 clients.
An MSP with multiple large customers could reach this limit quickly.
To accommodate this, {product1} can be scaled horizontally by adding {product1} servers.

Separation of entitlements::
{product1} uses customer credentials to access software entitlements from upstream providers, such as the https://scc.suse.com/[SUSE Customer Center (SCC)].
It is not a good idea to mix the entitlements of multiple customers in a single environment, as it would be difficult to keep track of which managed systems are entitled to which products.

Access control and user isolation::
In situations where customers would expect access to {product1}, an MSP would need some way to isolate each customerâ€™s users so they can only see and manage their own systems.
The MSP could install a dedicated {product1} server for each client.
This would address isolation, but it is somewhat inefficient because of the need to manage each of these servers independently.

{product1} empowers MSPs and large enterprises to address these concerns through:

* Organizations
* Content lifecycle management
* Hub and peripherals
* Inter-Server Synchronization


=== Organizations

Organizations form the basic building blocks of the solution.
Organizations manage user access and permissions within a single {product1} server.
You can think of an organization as a separate management environment with its own managed systems, users, and software channels.
You create one organization for each customer, enabling you to keep separate the various managed systems, users, and software channels.


=== Content lifecycle management

Content lifecycle management involves creating clones of software channels at particualr points in time and assigning these to different systems.
This is used to create multi-stage lifecycles where patches are passed from vendor channels to a development environment, from there to a test stage, and then to production.
MSPs can create separate lifecycles for each customer.
This ensures that each customer receives only the patches that are appropriate to its respective software lifecycle needs even though the patches may be based on the same upstream channels.


=== Hub and peripherals

https://{product1_docs}/specialized-guides/large-deployments/multi-server.html[{product1} Hub] was introduced to manage very large deployments by allowing horizontal scaling to multiple servers.

image::typical-hub-architecture-overview.svg[Hub and peripheral topology, scaledwidth="85%", align="center"]

In this hub and peripheral topology, one {product1} server acts as the central (or *hub*) server for one or more *peripheral* {product1} servers to which any number of client systems can be registered.
Each peripheral server is an independent {product1} server.
By dedicating a peripheral server for each customer, an MSP can address concerns about separation of customers while still allowing centralized control.

[NOTE]
====
Activation keys, bootstrap, repositories, system groups, and so on need to be createdon each peripheral.
Similarly, any custom Salt states you create must be manually copied to the appropriate peripheral.
Synchronizing these components between hub and peripherals is a manual process.  However, SUSE provides a set of Salt modules, states, and formulas that allow management of organizations, users, system groups, activation keys, and other settings on peripheral servers.
====


=== Inter-Server Synchronization

With multiple {product1} servers in a hub and peripheral architecture, you need to ensure that content and permissions stay aligned.
https://{product1_docs}/administration/iss.html[Inter-Server Synchronization (ISS)] allows you to connect two or more {product1} servers and keep them up-to-date.
Since only the hub needs access to an external source (such as SCC), a side benefit is that peripheral servers do not require Internet access.


== Deployment

{product1} can be deployed onto bare metal or virtual machines, running on-premises or in cloud environments.



=== Prerequisites

When sizing servers for {product1}, the most important variables are CPU, RAM, and disk storage.
Review https://{product1_docs}/installation-and-upgrade/hardware-requirements.html[{product1} Hardware requirements] for recommendations.

For this guide, the hub and peripheral servers will have very few clients, but import and export operations involve a lot of SQL transactions and, potentially, significant storage for the database (located in `/var/lib/pgsql`).
The recommended configuration for both hub and peripheral servers is:

* CPU: 8 CPU cores
* RAM: 64 GB
* Storage: 100 GB
 
A way to transfer data between the hub and peripherals is also needed.
Ideally, this could be through a shared (NFS) filesystem, but you can also just copy the data across the network (such as with `rsync`). 


=== Creating the hub server

The hub server can be installed following the standard {product1} method as documented in the https://{product1_docs}/quickstart/quickstart-install-suma-server.html[quickstart guide].

[NOTE]
====
Commands shown here should be issued as the root user.
====

To create a new hub server, you need to install and enable the Hub XMLRPC API service.

. On the hubÂ server, install the `hub-xmlrpc-api` package.
//
+
The package is available in the {product1} repositories, so can be installed with a simple command.
+
[source, console]
----
zypper install hub-xmlrpc-api
----

. Set the Hub XMLRPC API service to start automatically at boot time, and start it immediately:
+
[source, console]
----
systemctl enable hub-xmlrpc-api.service
systemctl start hub-xmlrpc-api.service
----
       
. Check that these parameters in the `/etc/hub/hub.conf` configuration file are correct:
+
--

* HUB_API_URL: URL to the Hub Server XMLRPC API endpoint.
//
+
Use the default value.

* HUB_CONNECT_TIMEOUT: the maximum number of seconds to wait for a response when connecting to a Server.
//
+
Use the default value in most cases.

* HUB_REQUEST_TIMEOUT: the maximum number of seconds to wait for a response when calling a Server method.
//
+
Use the default value in most cases.

* HUB_CONNECT_USING_SSL: use HTTPS instead of HTTP for communicating with peripheral Servers.
//
+
This is recommended for a secure environment.
To use HTTPS to connect to peripheral servers, you must set the HUB_CONNECT_USING_SSL parameter to `true` and ensure that the SSL certificates for all the peripheral servers are installed on the system where the Hub XMLRPC API service runs.
Do this by copying the RHN-ORG-TRUSTED-SSL-CERT certificate file from each peripheral serverâ€™s `http://<server-url>/pub/` directory to `/etc/pki/trust/anchors/`, then run `update-ca-certificates`.

--

. Restart services to activate any configuration changes you make.
+
[source, console]
----
sudo systemctl restart hub-xmlrpc-api.service
----


=== Onboarding new customers

The first step for onboarding a new customer involves adding the customer's SUSE Customer Center credentials to the hub server.
This allows customer software entitlements can be replicated from SCC to the hub.
To properly synchronize content lifecycle channels later, do not create a separate organization for each new customer.
SUSE Manager merges products coming from all SCC Credentials, which gives a centralized view of all products that are available through the hub. 


=== Install SUSE Manager on a New Peripheral Server

SUSE Manager should be installed on each peripheral in the normal manner. SUSE Services have developed processes to automate this  which is especially useful for situations where there will be a large number of such systems. However, manual installation is also fine.
However it is done, after the basic installation is complete, the normal process for setting up SUSE Manager involves a number of steps including setting up the database, creating the admin user and configuring credentials for the SUSE Customer Center. When setting up a new peripheral server we need to follow these steps with the exception of the SCC connection. Peripherals will not connect to the SCC but rather will obtain content from the hub.

First, we need to setup the database and certificates
. On the SUSE Manager Server, at the command line, use this command to begin setup:

+
[source, console]
----
yast2 susemanager_setup
----

. From the introduction screen select __SUSE Manager Setup__ > __Setup SUSE Manager from scratch__ and click __Next__ to continue.
. Enter an email address to receive status notifications and click Next to continue. SUSE Manager can sometimes send a large volume of notification emails. You can disable email notifications in the Web UI after setup, if you need to. 
. Enter your certificate information and a password. If you intend to use a custom SSL certificate, you need to have set this up first. 
. Click __Next__ to continue.
. From the __SUSE Manager Setup__ > __Database Settings__ screen, enter a database user and password and click Next to continue.
. Click __Next__ to continue.
. Click __Yes__ to run setup when prompted, and wait for it to complete.
. Click __Next__ to continue. Make a note of the address of the SUSE Manager Web UI.
. Click __Finish__ to complete SUSE Manager setup.

Next we need to create the admin user and main organization. It is important that this should be the same on each peripheral as on the hub, 
. In your browser, enter the address provided after completing setup. With this address you open the SUSE Manager Web UI.
. In the Web UI, navigate to the __Create Organization__ > __Organization Name__ field and enter your organization name.
. In the __Create Organization__ > __Desired Login__ and __Create Organization__ > __Desired Password__ fields, enter your username and password.
. Fill in the Account Information fields including an email for system notifications.
. Click __Create Organization__ to finish creating your administration account.
        
You are now presented with the __SUSE Manager Home__ > __Overview page__. After this, you would normally proceed to enter SCC credentials and synchronize the SUSE Linux Enterprise product channels from SUSE Customer Center. However, we will not do that here. Instead, we will proceed to register this server as a peripheral of our hub.

=== Register a Peripheral

Peripheral servers must be registered to the Hub Server as Salt clients. When you register the peripheral servers, you must assign them the appropriate SUSE Manager Server software channel as their base channel.  The best way to do this is to create a specific Activation Key for new peripherals and assign the the appropriate channel as the default base channel for this key.

There are a couple of ways to register new clients, we will look at using the Web UI here.
. In the SUSE Manager Web UI, navigate to __Systems__ > __Bootstrapping__.
. In the __Host__ field, type the fully qualified domain name (FQDN) of the client to be bootstrapped.
. In the __SSH Port__ field, type the SSH port number to use to connect and bootstrap the client. By default, the SSH port is 22.
. In the __User__ field, type the username to log in to the client. By default, the username is root.
. To bootstrap the client with SSH, in the __Authentication__ field, check __SSH Private Key__, and upload the SSHp rivate key to use to log in to the client. If your SSH private key requires a passphrase, type it into the __SSH Private Key Passphrase__ field, or leave it blank for no passphrase.
. To bootstrap the client with a password, in the __Authentication__ field, check __Password__, and type the password to log in to the client.
. In the __Activation Key__ field, select the activation key that is associated with the peripheral servers. 
. By default, the __Disable SSH Strict Key Host Checking__ checkbox is selected. This allows the bootstrap process to automatically accept SSH host keys without requiring you to manually authenticate.
. Check the __Manage System Completely via SSH__ checkbox. If you check this option, the client is configured to use SSH for its connection to the server, and no other connection method is configured.
. Click __Bootstrap__ to begin registration.
When the bootstrap process has completed, your client is listed at __Systems__ > __System List__. The peripheral server can now be managed as a standard salt client of the hub. 

== Synchronizing Content

=== Installing the ISS Package
To use Inter Server Sync, you need to install the inter-server-sync package on source and target servers.

+
[source, console]
----
zypper install inter-server-sync
----
In our case, the source server will be the hub and the target servers will be each new peripheral SUSE Manager server.

=== Building Customer-specific channels with Content Lifecycle Management

=== Exporting and Importing Content
You can now use the inter-server-sync command to export the CLM channels from the hub SUSE Manager to a local file system directory. For example:

inter-server-sync export â€“channel-with-children

Once the epxort is complete, you can now transfer this to the peripheral server eitehr through a common NFS mount or some other method.

On the target server, on the command line execute the ISS import command. The -h option provides detailed help:

    inter-server-sync import -h

=== Registering Clients


== Centralized Management

=== Salt Modules and Formulas

Managemnt of users, system groups, activation keys etc. Salt states are defined on the hub level and can be applied to the peripheral servers. 

Activation keys make reference to channels so you need to import the appropriate content first.

Salt uses execution and state modules to define, apply, and orchestrate configuration of your devices. Uyuni provides a set of modules called Uyuni configuration modules, that you can use to configure both SUSE Manager and Uyuni Servers.

You can use the Uyuni configuration modules directly or using SLS files. They are are especially useful if you have multiple Uyuni Servers, for example in Hub installations, but they are also useful for smaller installations.

You can use Uyuni configuration modules to configure:

* Organizations
* Users
* User permissions
* System groups
* Activation Keys

The Uyuni configuration modules are available in the uyuni-config-modules package. On the SUSE Manager Hub Server, at the command prompt, as root, use this command to install them:

+
[source, console]
----
zypper in uyuni-config-modules
----

This package also installs detailed API descriptions, indications on pillar settings, and examples. When you have installed the package, navigate to /usr/share/doc/packages/uyuni-config-modules/

=== Hub API

The Hub XMLRPC API operates in a similar way to the SUSE Manager API. https://documentation.suse.com/suma/4.3/api/suse-manager/index.html

Make sure the hub-xmlrpc-api service is started:

+
[source, console]
----
systemctl start hub-xmlrpc-api
----

Once it is running, you can connect to the service at port 2830 using any XMLRPC-compliant client libraries. The Hub XMLRPC API exposes the same methods that are available from the serverâ€™s XMLRPC API, with a few differences in parameter and return types. Additionally, the Hub XMLRPC API supports some Hub-specific end points which are not available in the SUSE Manager API.

The Hub XMLRPC API supports three different namespaces:

* The hub namespace is used to target the Hub XMLRPC API Server itself. It supports Hub-specific XMLRPC endpoints which are primarily related to authentication.
* The unicast namespace is used to target a single server registered in the hub. It redirects any call transparently to one specific server and returns any value as if the serverâ€™s XMLRPC API endpoint was used directly.
* The multicast namespace is used to target multiple peripheral servers registered in the hub. It redirects any call transparently to all the specified servers and returns the results in the form of a map.


The Hub XMLRPC API supports three different authentication modes:

* Manual mode (default): API credentials must be explicitly provided for each server.

* Relay mode: the credentials used to authenticate with the Hub are also used to authenticate to each server. You must provide a list of servers to connect to.

* Auto-connect mode: credentials are reused for each server, and any peripheral server you have access to is automatically connected.

=== Reporting

The Hub prepares and provides content for multiple peripheral SUSE Manager Servers. The goal of the reporting feature is to get data from these Servers back and have combined reporting data available on the Hub. The data is made available for external Reporting Tools. 

The main database is a PostgresDB in the SUSE Manager Hub system. It stores all the information collected from all the servers, and aggregates them. Every peripheral Server has its own reporting database where the information is collected for that system. In summary:

* the DB in SUSE Manager Hub stores, collects and eventually aggregates data coming from all the DBs of the peripheral Servers,

* the DB in SUSE Manager Hub stores also its own data from the systems directly connected and managed by the Hub,

* the DB in peripheral SUSE Manager Server stores its own data,

* the reporting tool can be connected either to the Hub or to any SUSE Manager Server.


The reporting database and schema are set up by default using the local PostgreSQL server. The reporting database is a separate database accessible via the network.

Before connecting an external Reporting Tools to the Database, a user with read-only permission should be created. For doing that, it is possible to use uyuni-setup-reportdb-user.


SUSE Manager uses taskomatic jobs to generate the data for the reporting database and to get the data from the peripheral servers to the hub.

To generate and update the data on a peripheral server, the responsible schedule is called update-reporting-default. It is executed by default daily at midnight.

On the SUSE Manager Hub there is a second schedule which is important. To fetch the reporting data from all registered peripheral servers, the task with the name update-reporting-hub-default is executed daily at 1:30 AM.

All times are in the local timezone of the server. If the peripheral servers are in different timezones, it is recommended to align all the schedules. Make sure that all reporting data are gathered at a specific point in time, and update-reporting-hub-default is running when all peripheral servers have actually finished their local jobs.










// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
